{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "882a9018-5319-4a82-87e7-2b0d420475bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from implementations import *\n",
    "from helpers import *\n",
    "from helper_functions import *\n",
    "from optimization import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d56f0e3-9ee7-448f-a9dd-dd349209cc8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading the training dataset\n",
      "training dataset loaded\n"
     ]
    }
   ],
   "source": [
    "# load the training dataset\n",
    "print(\"loading the training dataset\")\n",
    "DATA_TRAIN_PATH = 'train.csv'\n",
    "y_tr, tx_tr, ids_tr = load_csv_data(DATA_TRAIN_PATH)\n",
    "print(\"training dataset loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af90503c-e3fa-4e1b-a222-63ef11b39a80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading the test dataset\n",
      "test dataset loaded\n"
     ]
    }
   ],
   "source": [
    "# load the test dataset\n",
    "print(\"loading the test dataset\")\n",
    "DATA_TEST_PATH = 'test.csv'\n",
    "_, tx_te, ids_te = load_csv_data(DATA_TEST_PATH)\n",
    "print(\"test dataset loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa8da103-df5b-4f1e-b595-9ae0cd91750e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shape of the training y is  (250000,)  and the training tx is  (250000, 30)\n",
      "the shape of the test y is  (568238,)  and the test tx is  (568238, 30)\n"
     ]
    }
   ],
   "source": [
    "# printing the shapes of the arrays\n",
    "print(\"the shape of the training y is \",y_tr.shape,\" and the training tx is \", tx_tr.shape)\n",
    "print(\"the shape of the test y is \",y_te.shape,\" and the test tx is \",tx_te.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6884129-6892-4ec8-bce6-fe61104b9ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing the mean square gd\n",
    "initial_w = np.zeros(tx_tr.shape[1])\n",
    "max_iters = 500\n",
    "gamma = 0.00008\n",
    "\n",
    "#w, loss = mean_squared_error_gd(y_tr, tx_tr, initial_w, max_iters, gamma)\n",
    "\n",
    "#print(\"end of the mean_squared_error_gd with w=\",w,\" and loss=\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3abc58ab-10d3-4a01-b62a-6e3445c65b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end of the least_squares with w= [ 3.42668000e-01  4.47162143e-02 -1.37599075e-01 -3.56739938e-02\n",
      "  1.40897182e-04  4.07508703e-02  6.14829809e-03  4.84925531e-02\n",
      " -2.34341341e-02  6.60004961e-03 -3.14419100e-02  5.04290870e-02\n",
      "  3.69107261e-02  7.75298903e-02 -7.05741375e-04 -9.15065815e-04\n",
      "  2.97850089e-02 -4.44469301e-04  1.29483215e-03  1.51389711e-02\n",
      "  5.02990109e-05  6.53860302e-03  1.87647751e-04 -2.98889851e-04\n",
      " -2.70243153e-03  5.56955690e-04 -5.99751103e-04]  and loss= 0.08515410771135053\n",
      "the accuracy on the train set is  0.743884\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 27 is different from 63)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m teOpti \u001b[38;5;241m=\u001b[39m dataClean_without_splitting_te(tx_te, np\u001b[38;5;241m.\u001b[39mmean(tx_tr, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m), np\u001b[38;5;241m.\u001b[39mstd(tx_tr, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m     15\u001b[0m teOpti_poly \u001b[38;5;241m=\u001b[39m build_poly(teOpti, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m---> 16\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mteOpti_poly\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m OUTPUT_PATH \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mridge regression 1\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     19\u001b[0m create_csv_submission(ids_te, y_pred, OUTPUT_PATH)\n",
      "File \u001b[0;32m~/ml-project-1-ahl/helper_functions.py:81\u001b[0m, in \u001b[0;36mpredict\u001b[0;34m(weights, dataset)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(weights, dataset):\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;124;03m\"\"\"generates predictions given weights and a dataset\"\"\"\u001b[39;00m\n\u001b[0;32m---> 81\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\n\u001b[1;32m     82\u001b[0m     y_pred[np\u001b[38;5;241m.\u001b[39mwhere(y_pred \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m)] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     83\u001b[0m     y_pred[np\u001b[38;5;241m.\u001b[39mwhere(y_pred \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m)] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 27 is different from 63)"
     ]
    }
   ],
   "source": [
    "# test least square\n",
    "txOpti = dataClean_without_splitting(tx_tr)\n",
    "y_tr[np.where(y_tr == -1)] = 0\n",
    "\n",
    "w, loss = least_squares(y_tr, txOpti)\n",
    "\n",
    "label = predict(w, txOpti)\n",
    "y_tr[np.where(y_tr == 0)] = -1\n",
    "acc = calculate_accuracy(y_tr, label)\n",
    "\n",
    "print(\"end of the least_squares with w=\",w,\" and loss=\", loss)\n",
    "print(\"the accuracy on the train set is \", acc)\n",
    "\n",
    "teOpti = dataClean_without_splitting_te(tx_te, np.mean(tx_tr, axis=0), np.std(tx_tr, axis=0))\n",
    "teOpti_poly = build_poly(teOpti, 2)\n",
    "y_pred = predict(w, teOpti_poly)\n",
    "\n",
    "OUTPUT_PATH = 'ridge regression 1'\n",
    "create_csv_submission(ids_te, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1f09a81-21c9-4587-ba44-9bb03ab90776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test mean squared error sgd\n",
    "\n",
    "initial_w = np.zeros(tx_tr.shape[1])\n",
    "max_iters = 100\n",
    "gamma = 0.00001\n",
    "\n",
    "#w, loss = mean_squared_error_sgd(y_tr, tx_tr, initial_w, max_iters, gamma)\n",
    "\n",
    "#print(\"end of the mean_squared_error_sgd with w=\",w,\" and loss=\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34c8ae3d-d431-4939-8e10-d0915a0fc194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of txOpti  (250000, 27)\n",
      "best lambda is  1e-06  and best degree is  2\n",
      "end of the logistic_regression with w= [ 1.24359024e-01  1.24359024e-01  1.06182097e-01 -1.17410729e-01\n",
      "  1.09334422e-02 -5.29598798e-03  1.28444412e-02 -3.79969777e-02\n",
      " -2.81320609e-03 -1.17817143e-02 -2.21571939e-02 -4.32873531e-02\n",
      "  2.99790364e-02  3.86992795e-02  7.08325951e-02 -5.67215086e-04\n",
      " -1.72827291e-04  1.83777410e-02  4.12009593e-04  3.25046822e-04\n",
      " -2.18113723e-02  2.03193007e-04  4.76891778e-02  6.39657181e-05\n",
      " -6.58413313e-05  1.37134831e-02  5.11964680e-04 -3.35142548e-04\n",
      "  1.24359024e-01 -3.60006212e-02  1.43814131e-02 -2.76450826e-02\n",
      "  2.51556831e-03  7.49509617e-03  7.45923226e-03 -4.71295618e-03\n",
      " -2.69572042e-03 -6.97569365e-04  1.76462095e-02  4.06141270e-02\n",
      "  3.05316306e-04 -1.74693466e-02 -1.85779932e-02 -2.80288137e-03\n",
      " -5.97272702e-03 -3.15763153e-02 -2.63192804e-03  1.33749345e-02\n",
      " -1.66378956e-03 -1.08725767e-02  3.00554920e-02 -2.08589044e-03\n",
      " -2.98836375e-03  7.95212009e-03 -3.81506833e-03]  and loss= 0.07529289051889264\n",
      "the accuracy on the train set is  0.793132\n"
     ]
    }
   ],
   "source": [
    "# test ridge regression \n",
    "\n",
    "txOpti = dataClean_without_splitting(tx_tr)\n",
    "print(\"shape of txOpti \", txOpti.shape)\n",
    "\n",
    "lambda_, degree = best_lambda_degree(y_tr, txOpti, 4, lambdas=np.logspace(-6, 0, 30), degrees=np.arange(2,5), seed=1)\n",
    "degree = 2\n",
    "print(\"best lambda is \",lambda_,\" and best degree is \",degree)\n",
    "\n",
    "txOpti_poly = build_poly(txOpti, 2)\n",
    "y_tr[np.where(y_tr == -1)] = 0\n",
    "\n",
    "w, loss = ridge_regression(y_tr, txOpti_poly, 1e-06)\n",
    "\n",
    "label = predict(w, txOpti_poly)\n",
    "y_tr[np.where(y_tr == 0)] = -1\n",
    "acc = calculate_accuracy(y_tr, label)\n",
    "\n",
    "print(\"end of the logistic_regression with w=\",w,\" and loss=\", loss)\n",
    "print(\"the accuracy on the train set is \", acc)\n",
    "\n",
    "#teOpti = dataClean_without_splitting_te(tx_te, np.mean(tx_tr, axis=0), np.std(tx_tr, axis=0))\n",
    "#teOpti_poly = build_poly(teOpti, 2)\n",
    "#y_pred = predict(w, teOpti_poly)\n",
    "\n",
    "#OUTPUT_PATH = 'ridge regression 2'\n",
    "#create_csv_submission(ids_te, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f5c7de-0c97-4975-93cb-71f4916eb96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test logistic regression\n",
    "\n",
    "txOpti, yOpti = dataClean(tx_tr, y_tr)\n",
    "print(\"shape of txOpti \", txOpti[0].shape)\n",
    "print(\"shape of yOpti \", txOpti[0].shape)\n",
    "#initial_w = np.zeros(txOpti.shape[1])\n",
    "# initial_w = w\n",
    "max_iters = 100\n",
    "gamma = 0.5\n",
    "ws = []\n",
    "losses = []\n",
    "y_pred = []\n",
    "accArray = []\n",
    "\n",
    "for i in range(4): \n",
    "    initial_w = np.zeros(txOpti[i].shape[1])\n",
    "    w, loss = logistic_regression(yOpti[i], txOpti[i], initial_w, max_iters, gamma)\n",
    "    label = predict(w, txOpti[i])\n",
    "    acc = calculate_accuracy(yOpti[i], label)\n",
    "    \n",
    "    ws.append(w)\n",
    "    losses.append(loss)\n",
    "    y_pred.append(label)\n",
    "    accArray.append(acc)\n",
    "\n",
    "tot_acc = accArray[0] + accArray[1] + accArray[2] + accArray[3]\n",
    "weights = np.array(list(ws[0])+list(ws[1])+list(ws[2])+list(ws[3]))\n",
    "loss = np.mean(losses)\n",
    "    \n",
    "#teOpti = dataClean(tx_te)\n",
    "#print(\"shape of teOpti \", teOpti.shape)\n",
    "\n",
    "#y_pred = predict(w, teOpti)\n",
    "\n",
    "#OUTPUT_PATH = 'sample-submission LR 2'\n",
    "#create_csv_submission(ids_te, y_pred, OUTPUT_PATH)\n",
    "\n",
    "print(\"end of the logistic_regression with w=\",weights,\" and loss=\", loss)\n",
    "print(\"the accurcy on the train set is \", tot_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1c71eed-4b5e-41af-b0b6-57d490347155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of txOpti  (250000, 27)\n",
      "shape of y  (250000,)\n",
      "Preiteration, the loss=0.8778113302331929, the grad=0.3894684319725095\n",
      "Current iteration=0, the loss=0.8721421959328537, the grad=0.3894684319725095\n",
      "Current iteration=1, the loss=0.8670369603613681, the grad=0.3701154644539232\n",
      "Current iteration=2, the loss=0.8624330902642406, the grad=0.35214966768805916\n",
      "Current iteration=3, the loss=0.8582746751365801, the grad=0.3354951851315108\n",
      "Current iteration=4, the loss=0.854512145707261, the grad=0.32006599837882044\n",
      "Current iteration=5, the loss=0.8511017993836011, the grad=0.3057719598196116\n",
      "Current iteration=6, the loss=0.8480052355811629, the grad=0.29252308132629995\n",
      "Current iteration=7, the loss=0.8451887692290012, the grad=0.28023238085984653\n",
      "Current iteration=8, the loss=0.8426228639612967, the grad=0.26881762217978017\n",
      "Current iteration=9, the loss=0.8402816075804134, the grad=0.2582022520219785\n",
      "Current iteration=10, the loss=0.8381422399183409, the grad=0.24831578300535184\n",
      "Current iteration=11, the loss=0.8361847355450102, the grad=0.23909381137359223\n",
      "Current iteration=12, the loss=0.83439143937696, the grad=0.23047780678366017\n",
      "Current iteration=13, the loss=0.8327467509767207, the grad=0.22241476999375845\n",
      "Current iteration=14, the loss=0.8312368523863034, the grad=0.21485682324347796\n",
      "Current iteration=15, the loss=0.8298494741614195, the grad=0.20776077572342516\n",
      "Current iteration=16, the loss=0.8285736945167574, the grad=0.20108769086497116\n",
      "Current iteration=17, the loss=0.8273997669413575, the grad=0.19480247148698138\n",
      "Current iteration=18, the loss=0.8263189721693501, the grad=0.18887347169713606\n",
      "Current iteration=19, the loss=0.825323490923157, the grad=0.1832721397863017\n",
      "Current iteration=20, the loss=0.8244062943461515, the grad=0.17797269339349692\n",
      "Current iteration=21, the loss=0.8235610494924145, the grad=0.17295182640250015\n",
      "Current iteration=22, the loss=0.8227820376371628, the grad=0.16818844597426935\n",
      "Current iteration=23, the loss=0.8220640835135066, the grad=0.1636634375585189\n",
      "Current iteration=24, the loss=0.8214024938734786, the grad=0.15935945548467975\n",
      "Current iteration=25, the loss=0.820793004019241, the grad=0.15526073668756135\n",
      "Current iteration=26, the loss=0.8202317311597164, the grad=0.15135293519719406\n",
      "Current iteration=27, the loss=0.8197151336241502, the grad=0.14762297516369396\n",
      "Current iteration=28, the loss=0.8192399751122396, the grad=0.14405892036311377\n",
      "Current iteration=29, the loss=0.818803293284949, the grad=0.14064985831794807\n",
      "Current iteration=30, the loss=0.8184023721046861, the grad=0.137385797353233\n",
      "Current iteration=31, the loss=0.8180347174214352, the grad=0.13425757508836986\n",
      "Current iteration=32, the loss=0.8176980353754363, the grad=0.13125677703172853\n",
      "Current iteration=33, the loss=0.8173902132493698, the grad=0.12837566409780973\n",
      "Current iteration=34, the loss=0.8171093024556266, the grad=0.1256071080047318\n",
      "Current iteration=35, the loss=0.8168535033887789, the grad=0.12294453363334108\n",
      "Current iteration=36, the loss=0.8166211519110547, the grad=0.12038186753913768\n",
      "Current iteration=37, the loss=0.8164107072706344, the grad=0.11791349190547956\n",
      "Current iteration=38, the loss=0.8162207412798096, the grad=0.11553420331234664\n",
      "Current iteration=39, the loss=0.8160499286032304, the grad=0.1132391757704538\n",
      "Current iteration=40, the loss=0.8158970380263024, the grad=0.11102392753683174\n",
      "Current iteration=41, the loss=0.8157609245907205, the grad=0.108884291286184\n",
      "Current iteration=42, the loss=0.8156405224986777, the grad=0.1068163872633379\n",
      "Current iteration=43, the loss=0.8155348386997892, the grad=0.10481659908680871\n",
      "Current iteration=44, the loss=0.8154429470855005, the grad=0.10288155191265597\n",
      "Current iteration=45, the loss=0.815363983225056, the grad=0.10100809270212592\n",
      "Current iteration=46, the loss=0.8152971395851117, the grad=0.0991932723666463\n",
      "Current iteration=47, the loss=0.815241661182027, the grad=0.09743432959010412\n",
      "Current iteration=48, the loss=0.8151968416219085, the grad=0.09572867615146848\n",
      "Current iteration=49, the loss=0.815162019488714, the grad=0.09407388359111683\n",
      "Current iteration=50, the loss=0.8151365750453066, the grad=0.09246767108205614\n",
      "Current iteration=51, the loss=0.8151199272163202, the grad=0.09090789438290049\n",
      "Current iteration=52, the loss=0.8151115308252077, the grad=0.08939253576325339\n",
      "Current iteration=53, the loss=0.8151108740608782, the grad=0.0879196948042813\n",
      "Current iteration=54, the loss=0.8151174761520203, the grad=0.08648757998795943\n",
      "Current iteration=55, the loss=0.8151308852295746, the grad=0.08509450099790349\n",
      "Current iteration=56, the loss=0.815150676359871, the grad=0.08373886166303174\n",
      "Current iteration=57, the loss=0.8151764497328075, the grad=0.08241915348265524\n",
      "Current iteration=58, the loss=0.8152078289910369, the grad=0.08113394967810901\n",
      "Current iteration=59, the loss=0.8152444596875853, the grad=0.0798818997217945\n",
      "Current iteration=60, the loss=0.815286007860572, the grad=0.07866172429961514\n",
      "Current iteration=61, the loss=0.8153321587148494, the grad=0.07747221066731519\n",
      "Current iteration=62, the loss=0.8153826154013696, the grad=0.0763122083652578\n",
      "Current iteration=63, the loss=0.8154370978859768, the grad=0.07518062525975523\n",
      "Current iteration=64, the loss=0.8154953419001318, the grad=0.07407642388224692\n",
      "Current iteration=65, the loss=0.8155570979667738, the grad=0.07299861804045857\n",
      "Current iteration=66, the loss=0.8156221304951663, the grad=0.07194626967820342\n",
      "Current iteration=67, the loss=0.8156902169391421, the grad=0.07091848596274547\n",
      "Current iteration=68, the loss=0.8157611470136722, the grad=0.0699144165806616\n",
      "Current iteration=69, the loss=0.8158347219651468, the grad=0.068933251224945\n",
      "Current iteration=70, the loss=0.8159107538911591, the grad=0.06797421725770952\n",
      "Current iteration=71, the loss=0.8159890651059654, the grad=0.06703657753430269\n",
      "Current iteration=72, the loss=0.8160694875481185, the grad=0.06611962837593631\n",
      "Current iteration=73, the loss=0.816151862227088, the grad=0.06522269767911353\n",
      "Current iteration=74, the loss=0.8162360387059356, the grad=0.06434514315117981\n",
      "Current iteration=75, the loss=0.8163218746173747, the grad=0.06348635066227383\n",
      "Current iteration=76, the loss=0.8164092352107659, the grad=0.06264573270480611\n",
      "Current iteration=77, the loss=0.8164979929277925, the grad=0.061822726952362836\n",
      "Current iteration=78, the loss=0.8165880270047583, the grad=0.061016794910627656\n",
      "Current iteration=79, the loss=0.8166792230996084, the grad=0.06022742065354261\n",
      "Current iteration=80, the loss=0.8167714729419238, the grad=0.05945410963849775\n",
      "Current iteration=81, the loss=0.8168646740042904, the grad=0.05869638759485529\n",
      "Current iteration=82, the loss=0.8169587291935597, the grad=0.05795379948058044\n",
      "Current iteration=83, the loss=0.8170535465606321, the grad=0.05722590850217643\n",
      "Current iteration=84, the loss=0.8171490390275126, the grad=0.05651229519350648\n",
      "Current iteration=85, the loss=0.8172451241304645, the grad=0.05581255654943602\n",
      "Current iteration=86, the loss=0.8173417237781985, the grad=0.05512630521054935\n",
      "Current iteration=87, the loss=0.8174387640240903, the grad=0.0544531686954844\n",
      "Current iteration=88, the loss=0.8175361748515166, the grad=0.053792788677697025\n",
      "Current iteration=89, the loss=0.817633889971448, the grad=0.0531448203037071\n",
      "Current iteration=90, the loss=0.817731846631517, the grad=0.0525089315501026\n",
      "Current iteration=91, the loss=0.8178299854358195, the grad=0.05188480261677976\n",
      "Current iteration=92, the loss=0.8179282501747767, the grad=0.05127212535408343\n",
      "Current iteration=93, the loss=0.8180265876644159, the grad=0.05067060272168374\n",
      "Current iteration=94, the loss=0.8181249475944958, the grad=0.050079948277179806\n",
      "Current iteration=95, the loss=0.8182232823849144, the grad=0.04949988569256683\n",
      "Current iteration=96, the loss=0.8183215470499035, the grad=0.048930148296834106\n",
      "Current iteration=97, the loss=0.8184196990695295, the grad=0.048370478643083054\n",
      "Current iteration=98, the loss=0.818517698268059, the grad=0.04782062809866767\n",
      "Current iteration=99, the loss=0.8186155066987837, the grad=0.04728035645696149\n",
      "Current iteration=100, the loss=0.8187130885349113, the grad=0.04674943156945136\n",
      "Current iteration=101, the loss=0.8188104099661738, the grad=0.04622762899694617\n",
      "Current iteration=102, the loss=0.8189074391008103, the grad=0.04571473167876998\n",
      "Current iteration=103, the loss=0.8190041458726199, the grad=0.04521052961888397\n",
      "Current iteration=104, the loss=0.8191005019527834, the grad=0.04471481958795108\n",
      "Current iteration=105, the loss=0.8191964806661878, the grad=0.04422740484042219\n",
      "Current iteration=106, the loss=0.8192920569119945, the grad=0.043748094845781946\n",
      "Current iteration=107, the loss=0.819387207088212, the grad=0.04327670503314767\n",
      "Current iteration=108, the loss=0.8194819090200476, the grad=0.042813056548466524\n",
      "Current iteration=109, the loss=0.8195761418918299, the grad=0.042356976023603554\n",
      "Current iteration=110, the loss=0.8196698861823044, the grad=0.04190829535665782\n",
      "Current iteration=111, the loss=0.8197631236031131, the grad=0.04146685150288474\n",
      "Current iteration=112, the loss=0.8198558370402931, the grad=0.04103248627564169\n",
      "Current iteration=113, the loss=0.8199480104986206, the grad=0.040605046156809266\n",
      "Current iteration=114, the loss=0.8200396290486549, the grad=0.04018438211617409\n",
      "Current iteration=115, the loss=0.820130678776335, the grad=0.03977034943928993\n",
      "Current iteration=116, the loss=0.8202211467349921, the grad=0.03936280756336331\n",
      "Current iteration=117, the loss=0.820311020899657, the grad=0.03896161992073642\n",
      "Current iteration=118, the loss=0.8204002901235307, the grad=0.03856665378956556\n",
      "Current iteration=119, the loss=0.8204889440965204, the grad=0.03817778015131698\n",
      "Current iteration=120, the loss=0.8205769733057198, the grad=0.03779487355472414\n",
      "Current iteration=121, the loss=0.8206643689977444, the grad=0.03741781198587079\n",
      "Current iteration=122, the loss=0.8207511231428206, the grad=0.03704647674408398\n",
      "Current iteration=123, the loss=0.8208372284005441, the grad=0.036680752323339026\n",
      "Current iteration=124, the loss=0.8209226780872191, the grad=0.036320526298895144\n",
      "Current iteration=125, the loss=0.8210074661447048, the grad=0.03596568921889697\n",
      "Current iteration=126, the loss=0.8210915871106852, the grad=0.03561613450069156\n",
      "Current iteration=127, the loss=0.8211750360903033, the grad=0.03527175833162477\n",
      "Current iteration=128, the loss=0.8212578087290814, the grad=0.03493245957409373\n",
      "Current iteration=129, the loss=0.8213399011870719, the grad=0.034598139674644805\n",
      "Current iteration=130, the loss=0.8214213101141766, the grad=0.034268702576917866\n",
      "Current iteration=131, the loss=0.8215020326265755, the grad=0.033944054638248435\n",
      "Current iteration=132, the loss=0.8215820662842171, the grad=0.033624104549749595\n",
      "Current iteration=133, the loss=0.8216614090693122, the grad=0.0333087632597055\n",
      "Current iteration=134, the loss=0.8217400593657892, the grad=0.032997943900116684\n",
      "Current iteration=135, the loss=0.8218180159396657, the grad=0.032691561716246584\n",
      "Current iteration=136, the loss=0.8218952779202874, the grad=0.03238953399902624\n",
      "Current iteration=137, the loss=0.8219718447823979, the grad=0.03209178002018192\n",
      "Current iteration=138, the loss=0.8220477163290029, the grad=0.03179822096995755\n",
      "Current iteration=139, the loss=0.8221228926749854, the grad=0.03150877989731029\n",
      "Current iteration=140, the loss=0.8221973742314433, the grad=0.0312233816524644\n",
      "Current iteration=141, the loss=0.8222711616907142, the grad=0.030941952831713866\n",
      "Current iteration=142, the loss=0.8223442560120511, the grad=0.03066442172437046\n",
      "Current iteration=143, the loss=0.8224166584079322, the grad=0.030390718261759025\n",
      "Current iteration=144, the loss=0.8224883703309599, the grad=0.030120773968166613\n",
      "Current iteration=145, the loss=0.8225593934613327, the grad=0.02985452191365703\n",
      "Current iteration=146, the loss=0.8226297296948661, the grad=0.029591896668666903\n",
      "Current iteration=147, the loss=0.8226993811315272, the grad=0.029332834260303312\n",
      "Current iteration=148, the loss=0.8227683500644717, the grad=0.029077272130267458\n",
      "Current iteration=149, the loss=0.8228366389695535, the grad=0.028825149094331884\n",
      "Current iteration=150, the loss=0.8229042504952899, the grad=0.028576405303303513\n",
      "Current iteration=151, the loss=0.8229711874532619, the grad=0.02833098220540679\n",
      "Current iteration=152, the loss=0.8230374528089256, the grad=0.028088822510025497\n",
      "Current iteration=153, the loss=0.8231030496728277, the grad=0.027849870152744132\n",
      "Current iteration=154, the loss=0.823167981292195, the grad=0.0276140702616333\n",
      "Current iteration=155, the loss=0.8232322510428901, the grad=0.027381369124725326\n",
      "Current iteration=156, the loss=0.8232958624217162, the grad=0.027151714158629818\n",
      "Current iteration=157, the loss=0.82335881903905, the grad=0.026925053878240863\n",
      "Current iteration=158, the loss=0.8234211246118007, the grad=0.026701337867489704\n",
      "Current iteration=159, the loss=0.8234827829566677, the grad=0.02648051675109959\n",
      "Current iteration=160, the loss=0.8235437979836924, the grad=0.02626254216730051\n",
      "Current iteration=161, the loss=0.8236041736900913, the grad=0.026047366741464805\n",
      "Current iteration=162, the loss=0.8236639141543557, the grad=0.025834944060625174\n",
      "Current iteration=163, the loss=0.8237230235306077, the grad=0.02562522864883947\n",
      "Current iteration=164, the loss=0.8237815060432027, the grad=0.025418175943367597\n",
      "Current iteration=165, the loss=0.8238393659815678, the grad=0.02521374227162793\n",
      "Current iteration=166, the loss=0.8238966076952646, the grad=0.025011884828901744\n",
      "Current iteration=167, the loss=0.8239532355892699, the grad=0.024812561656755973\n",
      "Current iteration=168, the loss=0.8240092541194635, the grad=0.02461573162215587\n",
      "Current iteration=169, the loss=0.8240646677883121, the grad=0.02442135439723995\n",
      "Current iteration=170, the loss=0.8241194811407485, the grad=0.024229390439732032\n",
      "Current iteration=171, the loss=0.8241736987602277, the grad=0.02403980097396463\n",
      "Current iteration=172, the loss=0.8242273252649656, the grad=0.02385254797249096\n",
      "Current iteration=173, the loss=0.824280365304336, the grad=0.023667594138262277\n",
      "Current iteration=174, the loss=0.8243328235554382, the grad=0.02348490288734907\n",
      "Current iteration=175, the loss=0.8243847047198113, the grad=0.023304438332185817\n",
      "Current iteration=176, the loss=0.8244360135203028, the grad=0.0231261652653191\n",
      "Current iteration=177, the loss=0.8244867546980742, the grad=0.022950049143640346\n",
      "Current iteration=178, the loss=0.8245369330097466, the grad=0.02277605607308531\n",
      "Current iteration=179, the loss=0.8245865532246746, the grad=0.022604152793782806\n",
      "Current iteration=180, the loss=0.8246356201223469, the grad=0.022434306665636283\n",
      "Current iteration=181, the loss=0.8246841384899072, the grad=0.022266485654322356\n",
      "Current iteration=182, the loss=0.8247321131197899, the grad=0.022100658317691226\n",
      "Current iteration=183, the loss=0.8247795488074664, the grad=0.021936793792554428\n",
      "Current iteration=184, the loss=0.8248264503492988, the grad=0.021774861781846153\n",
      "Current iteration=185, the loss=0.8248728225404954, the grad=0.021614832542144673\n",
      "Current iteration=186, the loss=0.8249186701731612, the grad=0.02145667687154128\n",
      "Current iteration=187, the loss=0.8249639980344473, the grad=0.02130036609784447\n",
      "Current iteration=188, the loss=0.8250088109047873, the grad=0.02114587206710774\n",
      "Current iteration=189, the loss=0.8250531135562184, the grad=0.020993167132469633\n",
      "Current iteration=190, the loss=0.8250969107507904, the grad=0.0208422241432956\n",
      "Current iteration=191, the loss=0.8251402072390498, the grad=0.020693016434610932\n",
      "Current iteration=192, the loss=0.8251830077586034, the grad=0.020545517816815278\n",
      "Current iteration=193, the loss=0.8252253170327518, the grad=0.020399702565669104\n",
      "Current iteration=194, the loss=0.8252671397691977, the grad=0.020255545412542757\n",
      "Current iteration=195, the loss=0.8253084806588182, the grad=0.020113021534919975\n",
      "Current iteration=196, the loss=0.8253493443745022, the grad=0.019972106547146828\n",
      "Current iteration=197, the loss=0.8253897355700517, the grad=0.019832776491418385\n",
      "Current iteration=198, the loss=0.8254296588791414, the grad=0.01969500782899539\n",
      "Current iteration=199, the loss=0.8254691189143376, the grad=0.01955877743164323\n",
      "Current iteration=200, the loss=0.8255081202661663, the grad=0.01942406257328643\n",
      "Current iteration=201, the loss=0.8255466675022414, the grad=0.019290840921871527\n",
      "Current iteration=202, the loss=0.825584765166439, the grad=0.019159090531431805\n",
      "Current iteration=203, the loss=0.8256224177781202, the grad=0.01902878983434764\n",
      "Current iteration=204, the loss=0.8256596298314017, the grad=0.018899917633796188\n",
      "Current iteration=205, the loss=0.8256964057944711, the grad=0.018772453096384693\n",
      "Current iteration=206, the loss=0.8257327501089432, the grad=0.018646375744961693\n",
      "Current iteration=207, the loss=0.8257686671892599, the grad=0.018521665451600715\n",
      "Current iteration=208, the loss=0.8258041614221261, the grad=0.01839830243075118\n",
      "Current iteration=209, the loss=0.8258392371659862, the grad=0.01827626723255157\n",
      "Current iteration=210, the loss=0.8258738987505355, the grad=0.01815554073629993\n",
      "Current iteration=211, the loss=0.8259081504762665, the grad=0.018036104144077053\n",
      "Current iteration=212, the loss=0.8259419966140472, the grad=0.017917938974517895\n",
      "Current iteration=213, the loss=0.8259754414047331, the grad=0.01780102705672677\n",
      "Current iteration=214, the loss=0.8260084890588087, the grad=0.017685350524332344\n",
      "Current iteration=215, the loss=0.8260411437560571, the grad=0.017570891809678062\n",
      "Current iteration=216, the loss=0.8260734096452613, the grad=0.017457633638144628\n",
      "Current iteration=217, the loss=0.8261052908439248, the grad=0.017345559022600165\n",
      "Current iteration=218, the loss=0.8261367914380293, the grad=0.017234651257975065\n",
      "Current iteration=219, the loss=0.826167915481803, the grad=0.017124893915957572\n",
      "Current iteration=220, the loss=0.8261986669975258, the grad=0.017016270839806905\n",
      "Current iteration=221, the loss=0.8262290499753454, the grad=0.016908766139280784\n",
      "Current iteration=222, the loss=0.8262590683731271, the grad=0.016802364185674\n",
      "Current iteration=223, the loss=0.8262887261163087, the grad=0.016697049606965168\n",
      "Current iteration=224, the loss=0.8263180270977918, the grad=0.016592807283068618\n",
      "Current iteration=225, the loss=0.8263469751778382, the grad=0.016489622341188757\n",
      "Current iteration=226, the loss=0.8263755741839919, the grad=0.01638748015127392\n",
      "Current iteration=227, the loss=0.8264038279110171, the grad=0.016286366321567377\n",
      "Current iteration=228, the loss=0.8264317401208496, the grad=0.016186266694252587\n",
      "Current iteration=229, the loss=0.826459314542568, the grad=0.01608716734119063\n",
      "Current iteration=230, the loss=0.8264865548723754, the grad=0.015989054559747053\n",
      "Current iteration=231, the loss=0.8265134647735998, the grad=0.015891914868706034\n",
      "Current iteration=232, the loss=0.8265400478767052, the grad=0.015795735004269673\n",
      "Current iteration=233, the loss=0.8265663077793156, the grad=0.01570050191613997\n",
      "Current iteration=234, the loss=0.8265922480462533, the grad=0.015606202763681748\n",
      "Current iteration=235, the loss=0.8266178722095878, the grad=0.015512824912164208\n",
      "Current iteration=236, the loss=0.8266431837686952, the grad=0.01542035592907934\n",
      "Current iteration=237, the loss=0.8266681861903292, the grad=0.015328783580535141\n",
      "Current iteration=238, the loss=0.8266928829087024, the grad=0.015238095827721908\n",
      "Current iteration=239, the loss=0.8267172773255765, the grad=0.01514828082344977\n",
      "Current iteration=240, the loss=0.8267413728103621, the grad=0.015059326908755777\n",
      "Current iteration=241, the loss=0.8267651727002281, the grad=0.014971222609578705\n",
      "Current iteration=242, the loss=0.8267886803002154, the grad=0.014883956633500155\n",
      "Current iteration=243, the loss=0.8268118988833654, the grad=0.014797517866550347\n",
      "Current iteration=244, the loss=0.8268348316908489, the grad=0.014711895370076796\n",
      "Current iteration=245, the loss=0.8268574819321064, the grad=0.01462707837767483\n",
      "Current iteration=246, the loss=0.8268798527849924, the grad=0.01454305629217814\n",
      "Current iteration=247, the loss=0.826901947395929, the grad=0.014459818682708092\n",
      "Current iteration=248, the loss=0.8269237688800628, the grad=0.014377355281780501\n",
      "Current iteration=249, the loss=0.8269453203214274, the grad=0.014295655982468474\n",
      "Current iteration=250, the loss=0.8269666047731133, the grad=0.014214710835619992\n",
      "Current iteration=251, the loss=0.8269876252574403, the grad=0.014134510047129192\n",
      "Current iteration=252, the loss=0.8270083847661367, the grad=0.01405504397525977\n",
      "Current iteration=253, the loss=0.8270288862605215, the grad=0.013976303128019838\n",
      "Current iteration=254, the loss=0.8270491326716913, the grad=0.013898278160586496\n",
      "Current iteration=255, the loss=0.8270691269007097, the grad=0.013820959872779578\n",
      "Current iteration=256, the loss=0.8270888718188029, the grad=0.013744339206583061\n",
      "Current iteration=257, the loss=0.827108370267556, the grad=0.013668407243713326\n",
      "Current iteration=258, the loss=0.8271276250591146, the grad=0.01359315520323319\n",
      "Current iteration=259, the loss=0.8271466389763864, the grad=0.013518574439210645\n",
      "Current iteration=260, the loss=0.8271654147732493, the grad=0.01344465643842139\n",
      "Current iteration=261, the loss=0.8271839551747587, the grad=0.013371392818094269\n",
      "Current iteration=262, the loss=0.8272022628773584, the grad=0.013298775323698584\n",
      "Current iteration=263, the loss=0.8272203405490941, the grad=0.01322679582677248\n",
      "Current iteration=264, the loss=0.8272381908298269, the grad=0.01315544632279147\n",
      "Current iteration=265, the loss=0.827255816331451, the grad=0.013084718929076252\n",
      "Current iteration=266, the loss=0.8272732196381108, the grad=0.013014605882739146\n",
      "Current iteration=267, the loss=0.8272904033064205, the grad=0.012945099538668028\n",
      "Current iteration=268, the loss=0.8273073698656853, the grad=0.012876192367547303\n",
      "Current iteration=269, the loss=0.8273241218181224, the grad=0.012807876953915005\n",
      "Current iteration=270, the loss=0.8273406616390835, the grad=0.0127401459942552\n",
      "Current iteration=271, the loss=0.8273569917772787, the grad=0.01267299229512516\n",
      "Current iteration=272, the loss=0.8273731146550009, the grad=0.01260640877131646\n",
      "Current iteration=273, the loss=0.8273890326683504, the grad=0.012540388444049259\n",
      "Current iteration=274, the loss=0.8274047481874582, the grad=0.01247492443919933\n",
      "Current iteration=275, the loss=0.8274202635567145, the grad=0.012410009985556891\n",
      "Current iteration=276, the loss=0.8274355810949928, the grad=0.012345638413116762\n",
      "Current iteration=277, the loss=0.8274507030958754, the grad=0.01228180315139925\n",
      "Current iteration=278, the loss=0.8274656318278791, the grad=0.012218497727800964\n",
      "Current iteration=279, the loss=0.8274803695346832, the grad=0.012155715765975192\n",
      "Current iteration=280, the loss=0.8274949184353521, the grad=0.012093450984241142\n",
      "Current iteration=281, the loss=0.8275092807245622, the grad=0.012031697194021404\n",
      "Current iteration=282, the loss=0.8275234585728272, the grad=0.011970448298307268\n",
      "Current iteration=283, the loss=0.8275374541267217, the grad=0.011909698290151144\n",
      "Current iteration=284, the loss=0.8275512695091063, the grad=0.01184944125118574\n",
      "Current iteration=285, the loss=0.8275649068193509, the grad=0.01178967135016934\n",
      "Current iteration=286, the loss=0.8275783681335572, the grad=0.011730382841556771\n",
      "Current iteration=287, the loss=0.8275916555047818, the grad=0.011671570064095482\n",
      "Current iteration=288, the loss=0.8276047709632569, the grad=0.01161322743944632\n",
      "Current iteration=289, the loss=0.8276177165166122, the grad=0.011555349470828489\n",
      "Current iteration=290, the loss=0.8276304941500938, the grad=0.011497930741688258\n",
      "Current iteration=291, the loss=0.8276431058267837, the grad=0.01144096591439093\n",
      "Current iteration=292, the loss=0.8276555534878179, the grad=0.011384449728935677\n",
      "Current iteration=293, the loss=0.827667839052603, the grad=0.011328377001692718\n",
      "Current iteration=294, the loss=0.8276799644190331, the grad=0.01127274262416257\n",
      "Current iteration=295, the loss=0.8276919314637047, the grad=0.0112175415617568\n",
      "Current iteration=296, the loss=0.8277037420421287, the grad=0.011162768852599977\n",
      "Current iteration=297, the loss=0.8277153979889451, the grad=0.011108419606352324\n",
      "Current iteration=298, the loss=0.8277269011181347, the grad=0.011054489003052864\n",
      "Current iteration=299, the loss=0.8277382532232277, the grad=0.011000972291982452\n",
      "Current iteration=300, the loss=0.8277494560775139, the grad=0.010947864790546514\n",
      "Current iteration=301, the loss=0.8277605114342496, the grad=0.010895161883176989\n",
      "Current iteration=302, the loss=0.8277714210268651, the grad=0.010842859020253212\n",
      "Current iteration=303, the loss=0.8277821865691687, the grad=0.010790951717041359\n",
      "Current iteration=304, the loss=0.8277928097555515, the grad=0.010739435552652063\n",
      "Current iteration=305, the loss=0.8278032922611879, the grad=0.010688306169016016\n",
      "Current iteration=306, the loss=0.8278136357422397, the grad=0.010637559269876976\n",
      "Current iteration=307, the loss=0.8278238418360515, the grad=0.010587190619802121\n",
      "Current iteration=308, the loss=0.8278339121613526, the grad=0.010537196043209303\n",
      "Current iteration=309, the loss=0.8278438483184515, the grad=0.010487571423410865\n",
      "Current iteration=310, the loss=0.8278536518894306, the grad=0.010438312701673821\n",
      "Current iteration=311, the loss=0.827863324438343, the grad=0.010389415876296002\n",
      "Current iteration=312, the loss=0.8278728675114, the grad=0.010340877001698003\n",
      "Current iteration=313, the loss=0.8278822826371661, the grad=0.010292692187530456\n",
      "Current iteration=314, the loss=0.8278915713267456, the grad=0.010244857597796548\n",
      "Current iteration=315, the loss=0.8279007350739713, the grad=0.01019736944998945\n",
      "Current iteration=316, the loss=0.82790977535559, the grad=0.01015022401424425\n",
      "Current iteration=317, the loss=0.827918693631447, the grad=0.010103417612504351\n",
      "Current iteration=318, the loss=0.8279274913446714, the grad=0.010056946617701979\n",
      "Current iteration=319, the loss=0.827936169921853, the grad=0.010010807452952408\n",
      "Current iteration=320, the loss=0.8279447307732266, the grad=0.009964996590762\n",
      "Current iteration=321, the loss=0.8279531752928488, the grad=0.009919510552249383\n",
      "Current iteration=322, the loss=0.8279615048587737, the grad=0.009874345906379919\n",
      "Current iteration=323, the loss=0.8279697208332303, the grad=0.009829499269212956\n",
      "Current iteration=324, the loss=0.8279778245627943, the grad=0.00978496730316184\n",
      "Current iteration=325, the loss=0.8279858173785609, the grad=0.009740746716266267\n",
      "Current iteration=326, the loss=0.8279937005963163, the grad=0.009696834261476923\n",
      "Current iteration=327, the loss=0.8280014755167048, the grad=0.009653226735952076\n",
      "Current iteration=328, the loss=0.8280091434253982, the grad=0.009609920980366034\n",
      "Current iteration=329, the loss=0.8280167055932606, the grad=0.009566913878229076\n",
      "Current iteration=330, the loss=0.8280241632765128, the grad=0.009524202355218862\n",
      "Current iteration=331, the loss=0.8280315177168952, the grad=0.009481783378522984\n",
      "Current iteration=332, the loss=0.8280387701418294, the grad=0.009439653956192492\n",
      "Current iteration=333, the loss=0.8280459217645775, the grad=0.009397811136506229\n",
      "Current iteration=334, the loss=0.8280529737844, the grad=0.009356252007345757\n",
      "Current iteration=335, the loss=0.8280599273867136, the grad=0.009314973695580708\n",
      "Current iteration=336, the loss=0.8280667837432448, the grad=0.009273973366464381\n",
      "Current iteration=337, the loss=0.8280735440121844, the grad=0.009233248223039364\n",
      "Current iteration=338, the loss=0.8280802093383397, the grad=0.00919279550555308\n",
      "Current iteration=339, the loss=0.8280867808532841, the grad=0.009152612490883024\n",
      "Current iteration=340, the loss=0.828093259675507, the grad=0.009112696491971519\n",
      "Current iteration=341, the loss=0.8280996469105618, the grad=0.009073044857269901\n",
      "Current iteration=342, the loss=0.8281059436512099, the grad=0.009033654970191853\n",
      "Current iteration=343, the loss=0.8281121509775677, the grad=0.008994524248575895\n",
      "Current iteration=344, the loss=0.828118269957249, the grad=0.008955650144156604\n",
      "Current iteration=345, the loss=0.8281243016455053, the grad=0.008917030142044804\n",
      "Current iteration=346, the loss=0.8281302470853678, the grad=0.008878661760216091\n",
      "Current iteration=347, the loss=0.8281361073077851, the grad=0.008840542549008045\n",
      "Current iteration=348, the loss=0.8281418833317613, the grad=0.008802670090625589\n",
      "Current iteration=349, the loss=0.8281475761644909, the grad=0.008765041998654534\n",
      "Current iteration=350, the loss=0.8281531868014939, the grad=0.008727655917583168\n",
      "Current iteration=351, the loss=0.8281587162267489, the grad=0.008690509522331715\n",
      "Current iteration=352, the loss=0.8281641654128243, the grad=0.008653600517789508\n",
      "Current iteration=353, the loss=0.8281695353210101, the grad=0.00861692663835983\n",
      "Current iteration=354, the loss=0.8281748269014435, the grad=0.008580485647512152\n",
      "Current iteration=355, the loss=0.8281800410932401, the grad=0.008544275337341808\n",
      "Current iteration=356, the loss=0.828185178824618, the grad=0.008508293528136814\n",
      "Current iteration=357, the loss=0.8281902410130226, the grad=0.008472538067951856\n",
      "Current iteration=358, the loss=0.8281952285652514, the grad=0.008437006832189238\n",
      "Current iteration=359, the loss=0.8282001423775744, the grad=0.008401697723186653\n",
      "Current iteration=360, the loss=0.828204983335856, the grad=0.008366608669811738\n",
      "Current iteration=361, the loss=0.8282097523156751, the grad=0.00833173762706326\n",
      "Current iteration=362, the loss=0.8282144501824407, the grad=0.008297082575678802\n",
      "Current iteration=363, the loss=0.8282190777915126, the grad=0.008262641521748838\n",
      "Current iteration=364, the loss=0.8282236359883137, the grad=0.00822841249633713\n",
      "Current iteration=365, the loss=0.8282281256084468, the grad=0.008194393555107314\n",
      "Current iteration=366, the loss=0.8282325474778054, the grad=0.008160582777955535\n",
      "Current iteration=367, the loss=0.8282369024126878, the grad=0.00812697826864911\n",
      "Current iteration=368, the loss=0.8282411912199066, the grad=0.008093578154471027\n",
      "Current iteration=369, the loss=0.8282454146968979, the grad=0.008060380585870244\n",
      "Current iteration=370, the loss=0.8282495736318308, the grad=0.008027383736117657\n",
      "Current iteration=371, the loss=0.8282536688037129, the grad=0.007994585800967671\n",
      "Current iteration=372, the loss=0.8282577009824966, the grad=0.007961984998325233\n",
      "Current iteration=373, the loss=0.8282616709291845, the grad=0.007929579567918272\n",
      "Current iteration=374, the loss=0.828265579395932, the grad=0.007897367770975463\n",
      "Current iteration=375, the loss=0.82826942712615, the grad=0.007865347889909164\n",
      "Current iteration=376, the loss=0.8282732148546058, the grad=0.007833518228003529\n",
      "Current iteration=377, the loss=0.8282769433075241, the grad=0.0078018771091076305\n",
      "Current iteration=378, the loss=0.8282806132026839, the grad=0.007770422877333524\n",
      "Current iteration=379, the loss=0.8282842252495183, the grad=0.007739153896759225\n",
      "Current iteration=380, the loss=0.8282877801492102, the grad=0.007708068551136442\n",
      "Current iteration=381, the loss=0.8282912785947889, the grad=0.007677165243603048\n",
      "Current iteration=382, the loss=0.8282947212712225, the grad=0.007646442396400127\n",
      "Current iteration=383, the loss=0.8282981088555136, the grad=0.007615898450593631\n",
      "Current iteration=384, the loss=0.8283014420167911, the grad=0.007585531865800491\n",
      "Current iteration=385, the loss=0.8283047214163999, the grad=0.007555341119919105\n",
      "Current iteration=386, the loss=0.8283079477079945, the grad=0.007525324708864187\n",
      "Current iteration=387, the loss=0.8283111215376245, the grad=0.007495481146305828\n",
      "Current iteration=388, the loss=0.8283142435438259, the grad=0.007465808963412788\n",
      "Current iteration=389, the loss=0.8283173143577063, the grad=0.007436306708599851\n",
      "Current iteration=390, the loss=0.8283203346030329, the grad=0.007406972947279239\n",
      "Current iteration=391, the loss=0.8283233048963154, the grad=0.007377806261616044\n",
      "Current iteration=392, the loss=0.8283262258468935, the grad=0.007348805250287439\n",
      "Current iteration=393, the loss=0.8283290980570169, the grad=0.007319968528245923\n",
      "Current iteration=394, the loss=0.82833192212193, the grad=0.007291294726486162\n",
      "Current iteration=395, the loss=0.8283346986299525, the grad=0.007262782491815655\n",
      "Current iteration=396, the loss=0.8283374281625592, the grad=0.007234430486628981\n",
      "Current iteration=397, the loss=0.8283401112944616, the grad=0.007206237388685736\n",
      "Current iteration=398, the loss=0.8283427485936837, the grad=0.007178201890891875\n",
      "Current iteration=399, the loss=0.8283453406216417, the grad=0.007150322701084622\n",
      "Current iteration=400, the loss=0.8283478879332208, the grad=0.007122598541820744\n",
      "Current iteration=401, the loss=0.8283503910768503, the grad=0.007095028150168211\n",
      "Current iteration=402, the loss=0.8283528505945786, the grad=0.007067610277501128\n",
      "Current iteration=403, the loss=0.8283552670221486, the grad=0.0070403436892979504\n",
      "Current iteration=404, the loss=0.8283576408890695, the grad=0.007013227164942841\n",
      "Current iteration=405, the loss=0.8283599727186908, the grad=0.006986259497530223\n",
      "Current iteration=406, the loss=0.8283622630282724, the grad=0.006959439493672351\n",
      "Current iteration=407, the loss=0.828364512329057, the grad=0.0069327659733099956\n",
      "Current iteration=408, the loss=0.8283667211263378, the grad=0.0069062377695260435\n",
      "Current iteration=409, the loss=0.8283688899195313, the grad=0.00687985372836206\n",
      "Current iteration=410, the loss=0.8283710192022415, the grad=0.006853612708637748\n",
      "Current iteration=411, the loss=0.8283731094623306, the grad=0.006827513581773233\n",
      "Current iteration=412, the loss=0.8283751611819843, the grad=0.00680155523161414\n",
      "Current iteration=413, the loss=0.8283771748377782, the grad=0.006775736554259399\n",
      "Current iteration=414, the loss=0.8283791509007434, the grad=0.00675005645789177\n",
      "Current iteration=415, the loss=0.8283810898364298, the grad=0.006724513862611036\n",
      "Current iteration=416, the loss=0.8283829921049713, the grad=0.006699107700269753\n",
      "Current iteration=417, the loss=0.8283848581611475, the grad=0.00667383691431162\n",
      "Current iteration=418, the loss=0.8283866884544475, the grad=0.006648700459612348\n",
      "Current iteration=419, the loss=0.8283884834291292, the grad=0.006623697302323007\n",
      "Current iteration=420, the loss=0.8283902435242825, the grad=0.006598826419715846\n",
      "Current iteration=421, the loss=0.8283919691738869, the grad=0.006574086800032464\n",
      "Current iteration=422, the loss=0.8283936608068742, the grad=0.006549477442334373\n",
      "Current iteration=423, the loss=0.8283953188471835, the grad=0.006524997356355903\n",
      "Current iteration=424, the loss=0.828396943713821, the grad=0.00650064556235933\n",
      "Current iteration=425, the loss=0.8283985358209189, the grad=0.0064764210909923235\n",
      "Current iteration=426, the loss=0.8284000955777884, the grad=0.006452322983147537\n",
      "Current iteration=427, the loss=0.8284016233889793, the grad=0.006428350289824406\n",
      "Current iteration=428, the loss=0.8284031196543327, the grad=0.006404502071993098\n",
      "Current iteration=429, the loss=0.8284045847690366, the grad=0.00638077740046051\n",
      "Current iteration=430, the loss=0.8284060191236811, the grad=0.006357175355738399\n",
      "Current iteration=431, the loss=0.8284074231043097, the grad=0.006333695027913479\n",
      "Current iteration=432, the loss=0.8284087970924741, the grad=0.006310335516519585\n",
      "Current iteration=433, the loss=0.8284101414652848, the grad=0.006287095930411736\n",
      "Current iteration=434, the loss=0.8284114565954631, the grad=0.006263975387642183\n",
      "Current iteration=435, the loss=0.8284127428513925, the grad=0.006240973015338346\n",
      "Current iteration=436, the loss=0.8284140005971697, the grad=0.006218087949582607\n",
      "Current iteration=437, the loss=0.8284152301926518, the grad=0.006195319335293955\n",
      "Current iteration=438, the loss=0.8284164319935079, the grad=0.0061726663261114515\n",
      "Current iteration=439, the loss=0.8284176063512672, the grad=0.006150128084279462\n",
      "Current iteration=440, the loss=0.8284187536133664, the grad=0.006127703780534616\n",
      "Current iteration=441, the loss=0.8284198741231971, the grad=0.006105392593994569\n",
      "Current iteration=442, the loss=0.8284209682201537, the grad=0.006083193712048362\n",
      "Current iteration=443, the loss=0.8284220362396792, the grad=0.006061106330248515\n",
      "Current iteration=444, the loss=0.8284230785133104, the grad=0.0060391296522047215\n",
      "Current iteration=445, the loss=0.8284240953687244, the grad=0.006017262889479191\n",
      "Current iteration=446, the loss=0.8284250871297827, the grad=0.005995505261483545\n",
      "Current iteration=447, the loss=0.8284260541165742, the grad=0.005973855995377292\n",
      "Current iteration=448, the loss=0.8284269966454616, the grad=0.00595231432596783\n",
      "Current iteration=449, the loss=0.8284279150291225, the grad=0.00593087949561197\n",
      "Current iteration=450, the loss=0.8284288095765925, the grad=0.005909550754118934\n",
      "Current iteration=451, the loss=0.8284296805933077, the grad=0.005888327358654807\n",
      "Current iteration=452, the loss=0.8284305283811457, the grad=0.0058672085736484495\n",
      "Current iteration=453, the loss=0.8284313532384688, the grad=0.005846193670698809\n",
      "Current iteration=454, the loss=0.8284321554601615, the grad=0.005825281928483616\n",
      "Current iteration=455, the loss=0.8284329353376729, the grad=0.005804472632669478\n",
      "Current iteration=456, the loss=0.8284336931590569, the grad=0.005783765075823285\n",
      "Current iteration=457, the loss=0.8284344292090097, the grad=0.005763158557324941\n",
      "Current iteration=458, the loss=0.8284351437689088, the grad=0.0057426523832814465\n",
      "Current iteration=459, the loss=0.8284358371168542, the grad=0.005722245866442201\n",
      "Current iteration=460, the loss=0.8284365095277022, the grad=0.005701938326115588\n",
      "Current iteration=461, the loss=0.828437161273106, the grad=0.005681729088086834\n",
      "Current iteration=462, the loss=0.8284377926215513, the grad=0.005661617484537043\n",
      "Current iteration=463, the loss=0.8284384038383931, the grad=0.005641602853963437\n",
      "Current iteration=464, the loss=0.8284389951858917, the grad=0.005621684541100826\n",
      "Current iteration=465, the loss=0.8284395669232493, the grad=0.005601861896844165\n",
      "Current iteration=466, the loss=0.8284401193066447, the grad=0.005582134278172324\n",
      "Current iteration=467, the loss=0.8284406525892685, the grad=0.005562501048072926\n",
      "Current iteration=468, the loss=0.8284411670213562, the grad=0.0055429615754683395\n",
      "Current iteration=469, the loss=0.8284416628502246, the grad=0.005523515235142753\n",
      "Current iteration=470, the loss=0.8284421403203038, the grad=0.0055041614076702695\n",
      "Current iteration=471, the loss=0.8284425996731715, the grad=0.0054848994793441136\n",
      "Current iteration=472, the loss=0.8284430411475855, the grad=0.005465728842106883\n",
      "Current iteration=473, the loss=0.828443464979516, the grad=0.005446648893481743\n",
      "Current iteration=474, the loss=0.8284438714021776, the grad=0.00542765903650472\n",
      "Current iteration=475, the loss=0.8284442606460624, the grad=0.005408758679657923\n",
      "Current iteration=476, the loss=0.8284446329389692, the grad=0.005389947236803749\n",
      "Current iteration=477, the loss=0.8284449885060372, the grad=0.005371224127120076\n",
      "Current iteration=478, the loss=0.8284453275697735, the grad=0.00535258877503633\n",
      "Current iteration=479, the loss=0.8284456503500867, the grad=0.005334040610170586\n",
      "Current iteration=480, the loss=0.8284459570643149, the grad=0.005315579067267447\n",
      "Current iteration=481, the loss=0.8284462479272552, the grad=0.005297203586136939\n",
      "Current iteration=482, the loss=0.8284465231511936, the grad=0.005278913611594216\n",
      "Current iteration=483, the loss=0.8284467829459339, the grad=0.005260708593400164\n",
      "Current iteration=484, the loss=0.8284470275188265, the grad=0.005242587986202857\n",
      "Current iteration=485, the loss=0.828447257074796, the grad=0.005224551249479841\n",
      "Current iteration=486, the loss=0.8284474718163684, the grad=0.005206597847481268\n",
      "Current iteration=487, the loss=0.828447671943702, the grad=0.005188727249173822\n",
      "Current iteration=488, the loss=0.8284478576546095, the grad=0.005170938928185463\n",
      "Current iteration=489, the loss=0.8284480291445905, the grad=0.005153232362750948\n",
      "Current iteration=490, the loss=0.828448186606853, the grad=0.0051356070356581435\n",
      "Current iteration=491, the loss=0.8284483302323445, the grad=0.0051180624341950936\n",
      "Current iteration=492, the loss=0.8284484602097736, the grad=0.005100598050097826\n",
      "Current iteration=493, the loss=0.8284485767256389, the grad=0.005083213379498931\n",
      "Current iteration=494, the loss=0.8284486799642524, the grad=0.0050659079228768476\n",
      "Current iteration=495, the loss=0.8284487701077667, the grad=0.005048681185005852\n",
      "Current iteration=496, the loss=0.8284488473361978, the grad=0.005031532674906799\n",
      "Current iteration=497, the loss=0.8284489118274503, the grad=0.005014461905798493\n",
      "Current iteration=498, the loss=0.828448963757342, the grad=0.004997468395049823\n",
      "Current iteration=499, the loss=0.828449003299628, the grad=0.004980551664132482\n",
      "end of the logistic_regression with w= [-0.85202286  0.21171204 -0.75232808 -0.10879023  0.04634508  0.19039949\n",
      "  0.04774379  0.26610564 -0.1360106   0.0251304  -0.2094604   0.27441079\n",
      "  0.19944281  0.36468343 -0.00461655 -0.00642573  0.12755784 -0.00310409\n",
      "  0.00715122  0.02389417 -0.00085557  0.0459461   0.00124537 -0.001023\n",
      " -0.01270603  0.00353079 -0.00218233]  and loss= 0.828449003299628\n",
      "the accuracy on the train set is  0.744156\n"
     ]
    }
   ],
   "source": [
    "# test logistic regression without splitting \n",
    "txOpti = dataClean_without_splitting(tx_tr)\n",
    "print(\"shape of txOpti \", txOpti.shape)\n",
    "print(\"shape of y \", y_tr.shape)\n",
    "initial_w = np.zeros(txOpti.shape[1])\n",
    "max_iters = 500\n",
    "gamma = 0.08\n",
    "\n",
    "y_tr[np.where(y_tr == -1)] = 0\n",
    "w, loss = logistic_regression(y_tr, txOpti, initial_w, max_iters, gamma)\n",
    "\n",
    "label = predict_logistic(w, txOpti)\n",
    "y_tr[np.where(y_tr == 0)] = -1\n",
    "acc = calculate_accuracy(y_tr, label)\n",
    "\n",
    "print(\"end of the logistic_regression with w=\",w,\" and loss=\", loss)\n",
    "print(\"the accuracy on the train set is \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b54129f-2e9e-489f-989e-cffe093f018d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean column , remove column that has only -999 elements. \n",
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.3f}\".format(x)})\n",
    "\n",
    "median = np.median(tx_tr, axis=0) \n",
    "mean = np.mean(tx_tr, axis=0) \n",
    "\n",
    "print(median)\n",
    "print(mean)\n",
    "\n",
    "indx = np.where(median == -999)\n",
    "print(indx)\n",
    "\n",
    "counts = []\n",
    "count = 0\n",
    "\n",
    "for i in indx[0]: \n",
    "    count = np.count_nonzero( tx_tr[:,i] == -999)\n",
    "    print(\"index :\", i)\n",
    "    print(count)\n",
    "    counts.append(count)\n",
    "\n",
    "print(\"counts of each column \", counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cd948ae1-5376-46f3-b929-6d5d233f0bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7,)\n",
      "(250000, 30)\n",
      "(250000, 23)\n"
     ]
    }
   ],
   "source": [
    "median = np.median(tx_tr, axis=0) \n",
    "indx = np.where(median == -999)\n",
    "tx1 = np.delete(tx_tr, indx[0], 1)\n",
    "print(indx[0].shape)\n",
    "print(tx_tr.shape)\n",
    "print(tx1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f062a94a-ff3d-4d4d-88b2-66b0ca8bf65c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of txOpti  (250000, 31)\n",
      "shape of y  (250000,)\n",
      "Current iteration=0, the loss=0.871017428474571, the grad=0.41819012454506566\n",
      "Current iteration=1, the loss=0.8651628645311863, the grad=0.38933256664921284\n",
      "Current iteration=2, the loss=0.8600859630155451, the grad=0.3639497278265676\n",
      "Current iteration=3, the loss=0.8556549531126425, the grad=0.34162653578482893\n",
      "Current iteration=4, the loss=0.8517632775365919, the grad=0.32195374213549893\n",
      "Current iteration=5, the loss=0.8483249701517711, the grad=0.30455370477448707\n",
      "Current iteration=6, the loss=0.8452705743556305, the grad=0.28909189761266657\n",
      "Current iteration=7, the loss=0.8425437445512649, the grad=0.275279568956454\n",
      "Current iteration=8, the loss=0.8400985089434455, the grad=0.26287154280001396\n",
      "Current iteration=9, the loss=0.8378971058030737, the grad=0.2516617450893084\n",
      "Current iteration=10, the loss=0.8359082896852346, the grad=0.24147797359041578\n",
      "Current iteration=11, the loss=0.8341060109280578, the grad=0.23217672845925888\n",
      "Current iteration=12, the loss=0.832468386544388, the grad=0.2236384947472681\n",
      "Current iteration=13, the loss=0.8309768965270048, the grad=0.215763625830618\n",
      "Current iteration=14, the loss=0.8296157538512223, the grad=0.2084688477381484\n",
      "Current iteration=15, the loss=0.8283714082525026, the grad=0.2016843415419054\n",
      "Current iteration=16, the loss=0.8272321532052201, the grad=0.19535133513779152\n",
      "Current iteration=17, the loss=0.8261878127671648, the grad=0.18942012954540358\n",
      "Current iteration=18, the loss=0.8252294904867378, the grad=0.1838484883691092\n",
      "Current iteration=19, the loss=0.8243493667699995, the grad=0.17860032678928206\n",
      "Current iteration=20, the loss=0.823540534284729, the grad=0.17364464546731392\n",
      "Current iteration=21, the loss=0.8227968633858895, the grad=0.16895466358145703\n",
      "Current iteration=22, the loss=0.8221128913718506, the grad=0.16450711319853834\n",
      "Current iteration=23, the loss=0.821483730767934, the grad=0.16028166409549324\n",
      "Current iteration=24, the loss=0.8209049928918917, the grad=0.15626045396019583\n",
      "Current iteration=25, the loss=0.8203727237660503, the grad=0.15242770371143405\n",
      "Current iteration=26, the loss=0.8198833500637802, the grad=0.14876940161096963\n",
      "Current iteration=27, the loss=0.8194336332590629, the grad=0.14527304303144417\n",
      "Current iteration=28, the loss=0.8190206305212169, the grad=0.1419274153190229\n",
      "Current iteration=29, the loss=0.8186416611878269, the grad=0.13872241926075013\n",
      "Current iteration=30, the loss=0.8182942778768226, the grad=0.13564892032866702\n",
      "Current iteration=31, the loss=0.8179762414780017, the grad=0.13269862420493875\n",
      "Current iteration=32, the loss=0.8176854994061082, the grad=0.12986397215943413\n",
      "Current iteration=33, the loss=0.8174201666102544, the grad=0.12713805270610046\n",
      "Current iteration=34, the loss=0.8171785089244277, the grad=0.12451452664956207\n",
      "Current iteration=35, the loss=0.8169589284160105, the grad=0.12198756318277568\n",
      "Current iteration=36, the loss=0.8167599504473976, the grad=0.11955178513757145\n",
      "Current iteration=37, the loss=0.8165802122129439, the grad=0.11720222184432186\n",
      "Current iteration=38, the loss=0.8164184525518126, the grad=0.11493426834217736\n",
      "Current iteration=39, the loss=0.8162735028686835, the grad=0.11274364991116492\n",
      "Current iteration=40, the loss=0.816144279020044, the grad=0.11062639108299506\n",
      "Current iteration=41, the loss=0.8160297740450859, the grad=0.10857878843748338\n",
      "Current iteration=42, the loss=0.8159290516378691, the grad=0.10659738661307849\n",
      "Current iteration=43, the loss=0.8158412402721428, the grad=0.10467895705870266\n",
      "Current iteration=44, the loss=0.8157655279025198, the grad=0.10282047913443718\n",
      "Current iteration=45, the loss=0.8157011571760521, the grad=0.10101912323408829\n",
      "Current iteration=46, the loss=0.815647421097013, the grad=0.09927223565622748\n",
      "Current iteration=47, the loss=0.8156036590950834, the grad=0.09757732499419325\n",
      "Current iteration=48, the loss=0.8155692534534751, the grad=0.09593204985161638\n",
      "Current iteration=49, the loss=0.8155436260588992, the grad=0.09433420771976796\n",
      "Current iteration=50, the loss=0.8155262354399236, the grad=0.0927817248776067\n",
      "Current iteration=51, the loss=0.8155165740642522, the grad=0.09127264719579041\n",
      "Current iteration=52, the loss=0.8155141658688914, the grad=0.0898051317428741\n",
      "Current iteration=53, the loss=0.8155185640001618, the grad=0.08837743910607848\n",
      "Current iteration=54, the loss=0.8155293487431018, the grad=0.0869879263508761\n",
      "Current iteration=55, the loss=0.8155461256220808, the grad=0.08563504055361904\n",
      "Current iteration=56, the loss=0.8155685236564065, the grad=0.08431731284985294\n",
      "Current iteration=57, the loss=0.8155961937564606, the grad=0.08303335294809838\n",
      "Current iteration=58, the loss=0.815628807247411, the grad=0.08178184406495315\n",
      "Current iteration=59, the loss=0.8156660545089013, the grad=0.08056153824255226\n",
      "Current iteration=60, the loss=0.8157076437203052, the grad=0.07937125201387292\n",
      "Current iteration=61, the loss=0.8157532997021811, the grad=0.07820986238519909\n",
      "Current iteration=62, the loss=0.815802762845497, the grad=0.0770763031083739\n",
      "Current iteration=63, the loss=0.8158557881210289, the grad=0.0759695612183403\n",
      "Current iteration=64, the loss=0.8159121441620681, the grad=0.07488867381397867\n",
      "Current iteration=65, the loss=0.8159716124142323, the grad=0.07383272506243942\n",
      "Current iteration=66, the loss=0.8160339863467702, the grad=0.07280084340909541\n",
      "Current iteration=67, the loss=0.8160990707202639, the grad=0.07179219897693559\n",
      "Current iteration=68, the loss=0.8161666809061153, the grad=0.07080600114072212\n",
      "Current iteration=69, the loss=0.8162366422536249, the grad=0.06984149626256625\n",
      "Current iteration=70, the loss=0.81630878950084, the grad=0.0688979655767643\n",
      "Current iteration=71, the loss=0.8163829662257138, the grad=0.0679747232127959\n",
      "Current iteration=72, the loss=0.8164590243343989, the grad=0.06707111434633511\n",
      "Current iteration=73, the loss=0.816536823583801, the grad=0.06618651346897898\n",
      "Current iteration=74, the loss=0.8166162311357524, the grad=0.06532032276816684\n",
      "Current iteration=75, the loss=0.8166971211404029, the grad=0.06447197060945638\n",
      "Current iteration=76, the loss=0.81677937434662, the grad=0.06364091011395173\n",
      "Current iteration=77, the loss=0.8168628777373937, the grad=0.06282661782424775\n",
      "Current iteration=78, the loss=0.8169475241883909, the grad=0.06202859245277187\n",
      "Current iteration=79, the loss=0.8170332121479708, the grad=0.06124635370687812\n",
      "Current iteration=80, the loss=0.8171198453371041, the grad=0.0604794411854745\n",
      "Current iteration=81, the loss=0.817207332467775, the grad=0.05972741334236117\n",
      "Current iteration=82, the loss=0.8172955869785478, the grad=0.058989846511813444\n",
      "Current iteration=83, the loss=0.8173845267860941, the grad=0.058266333992274814\n",
      "Current iteration=84, the loss=0.8174740740515706, the grad=0.057556485184325386\n",
      "Current iteration=85, the loss=0.8175641549608209, the grad=0.05685992477937077\n",
      "Current iteration=86, the loss=0.8176546995174604, the grad=0.05617629199574863\n",
      "Current iteration=87, the loss=0.8177456413479697, the grad=0.055505239859189044\n",
      "Current iteration=88, the loss=0.8178369175179967, the grad=0.0548464345247765\n",
      "Current iteration=89, the loss=0.81792846835912, the grad=0.054199554637765573\n",
      "Current iteration=90, the loss=0.8180202373053874, the grad=0.05356429073078269\n",
      "Current iteration=91, the loss=0.8181121707389944, the grad=0.05294034465511974\n",
      "Current iteration=92, the loss=0.8182042178445147, the grad=0.05232742904398123\n",
      "Current iteration=93, the loss=0.8182963304711307, the grad=0.051725266805691644\n",
      "Current iteration=94, the loss=0.8183884630023665, the grad=0.05113359064500702\n",
      "Current iteration=95, the loss=0.8184805722328475, the grad=0.05055214261079685\n",
      "Current iteration=96, the loss=0.81857261725165, the grad=0.04998067366848092\n",
      "Current iteration=97, the loss=0.8186645593318409, the grad=0.04941894329571077\n",
      "Current iteration=98, the loss=0.818756361825826, the grad=0.04886671909988733\n",
      "Current iteration=99, the loss=0.8188479900661564, the grad=0.048323776456197745\n",
      "Current iteration=100, the loss=0.8189394112714679, the grad=0.04778989816494074\n",
      "Current iteration=101, the loss=0.819030594457249, the grad=0.04726487412698991\n",
      "Current iteration=102, the loss=0.8191215103511558, the grad=0.04674850103631938\n",
      "Current iteration=103, the loss=0.8192121313126022, the grad=0.04624058208858411\n",
      "Current iteration=104, the loss=0.8193024312563906, the grad=0.04574092670481328\n",
      "Current iteration=105, the loss=0.8193923855801387, the grad=0.045249350269333945\n",
      "Current iteration=106, the loss=0.8194819710952985, the grad=0.04476567388109925\n",
      "Current iteration=107, the loss=0.81957116596156, the grad=0.04428972411764574\n",
      "Current iteration=108, the loss=0.8196599496244499, the grad=0.04382133281095548\n",
      "Current iteration=109, the loss=0.8197483027559581, the grad=0.04336033683454215\n",
      "Current iteration=110, the loss=0.8198362071980124, the grad=0.04290657790112297\n",
      "Current iteration=111, the loss=0.8199236459086573, the grad=0.042459902370278226\n",
      "Current iteration=112, the loss=0.8200106029107909, the grad=0.0420201610655367\n",
      "Current iteration=113, the loss=0.8200970632433146, the grad=0.04158720910036025\n",
      "Current iteration=114, the loss=0.8201830129145824, the grad=0.041160905712531705\n",
      "Current iteration=115, the loss=0.8202684388580144, the grad=0.04074111410648209\n",
      "Current iteration=116, the loss=0.8203533288897695, the grad=0.04032770130311992\n",
      "Current iteration=117, the loss=0.8204376716683734, the grad=0.03992053799675221\n",
      "Current iteration=118, the loss=0.8205214566561939, the grad=0.03951949841871105\n",
      "Current iteration=119, the loss=0.8206046740826773, the grad=0.03912446020732362\n",
      "Current iteration=120, the loss=0.8206873149092506, the grad=0.038735304283883595\n",
      "Current iteration=121, the loss=0.8207693707958121, the grad=0.038351914734303064\n",
      "Current iteration=122, the loss=0.8208508340687243, the grad=0.037974178696143085\n",
      "Current iteration=123, the loss=0.820931697690245, the grad=0.03760198625073778\n",
      "Current iteration=124, the loss=0.8210119552293159, the grad=0.03723523032014477\n",
      "Current iteration=125, the loss=0.8210916008336469, the grad=0.036873806568668836\n",
      "Current iteration=126, the loss=0.8211706292030371, the grad=0.03651761330872155\n",
      "Current iteration=127, the loss=0.8212490355638679, the grad=0.03616655141079297\n",
      "Current iteration=128, the loss=0.8213268156447135, the grad=0.03582052421732339\n",
      "Current iteration=129, the loss=0.8214039656530188, the grad=0.035479437460276826\n",
      "Current iteration=130, the loss=0.8214804822527898, the grad=0.03514319918222764\n",
      "Current iteration=131, the loss=0.8215563625432561, the grad=0.034811719660783516\n",
      "Current iteration=132, the loss=0.8216316040384523, the grad=0.03448491133617726\n",
      "Current iteration=133, the loss=0.8217062046476804, the grad=0.03416268874186948\n",
      "Current iteration=134, the loss=0.8217801626568153, the grad=0.03384496843801314\n",
      "Current iteration=135, the loss=0.821853476710408, the grad=0.033531668947638926\n",
      "Current iteration=136, the loss=0.8219261457945581, the grad=0.033222710695428356\n",
      "Current iteration=137, the loss=0.8219981692205126, the grad=0.032918015948949324\n",
      "Current iteration=138, the loss=0.8220695466089684, the grad=0.032617508762234175\n",
      "Current iteration=139, the loss=0.8221402778750375, the grad=0.03232111492158917\n",
      "Current iteration=140, the loss=0.8222103632138508, the grad=0.03202876189352766\n",
      "Current iteration=141, the loss=0.822279803086774, the grad=0.03174037877472739\n",
      "Current iteration=142, the loss=0.8223485982082025, the grad=0.0314558962439163\n",
      "Current iteration=143, the loss=0.8224167495329155, the grad=0.031175246515596614\n",
      "Current iteration=144, the loss=0.8224842582439633, the grad=0.030898363295521925\n",
      "Current iteration=145, the loss=0.8225511257410644, the grad=0.03062518173784668\n",
      "Current iteration=146, the loss=0.8226173536294885, the grad=0.030355638403871024\n",
      "Current iteration=147, the loss=0.8226829437094118, the grad=0.03008967122230882\n",
      "Current iteration=148, the loss=0.8227478979657148, the grad=0.029827219451009846\n",
      "Current iteration=149, the loss=0.8228122185582095, the grad=0.029568223640070963\n",
      "Current iteration=150, the loss=0.8228759078122775, the grad=0.0293126255962743\n",
      "Current iteration=151, the loss=0.8229389682099016, the grad=0.02906036834879384\n",
      "Current iteration=152, the loss=0.8230014023810722, the grad=0.028811396116114777\n",
      "Current iteration=153, the loss=0.823063213095554, the grad=0.02856565427411234\n",
      "Current iteration=154, the loss=0.8231244032550006, the grad=0.028323089325240445\n",
      "Current iteration=155, the loss=0.8231849758853981, the grad=0.028083648868781945\n",
      "Current iteration=156, the loss=0.8232449341298278, the grad=0.02784728157211546\n",
      "Current iteration=157, the loss=0.8233042812415317, the grad=0.027613937142955604\n",
      "Current iteration=158, the loss=0.8233630205772706, the grad=0.02738356630252551\n",
      "Current iteration=159, the loss=0.8234211555909648, the grad=0.027156120759623193\n",
      "Current iteration=160, the loss=0.8234786898275968, the grad=0.026931553185543976\n",
      "Current iteration=161, the loss=0.8235356269173781, the grad=0.026709817189824427\n",
      "Current iteration=162, the loss=0.823591970570161, the grad=0.026490867296773808\n",
      "Current iteration=163, the loss=0.8236477245700847, the grad=0.026274658922761245\n",
      "Current iteration=164, the loss=0.8237028927704529, the grad=0.02606114835422833\n",
      "Current iteration=165, the loss=0.8237574790888277, the grad=0.025850292726397953\n",
      "Current iteration=166, the loss=0.8238114875023318, the grad=0.025642050002651944\n",
      "Current iteration=167, the loss=0.8238649220431545, the grad=0.025436378954551\n",
      "Current iteration=168, the loss=0.8239177867942484, the grad=0.025233239142472174\n",
      "Current iteration=169, the loss=0.8239700858852124, the grad=0.025032590896839575\n",
      "Current iteration=170, the loss=0.8240218234883526, the grad=0.02483439529992577\n",
      "Current iteration=171, the loss=0.8240730038149137, the grad=0.024638614168201876\n",
      "Current iteration=172, the loss=0.8241236311114737, the grad=0.024445210035215922\n",
      "Current iteration=173, the loss=0.8241737096564968, the grad=0.024254146134979022\n",
      "Current iteration=174, the loss=0.8242232437570379, the grad=0.02406538638584105\n",
      "Current iteration=175, the loss=0.8242722377455913, the grad=0.023878895374837308\n",
      "Current iteration=176, the loss=0.8243206959770788, the grad=0.02369463834248897\n",
      "Current iteration=177, the loss=0.8243686228259728, the grad=0.023512581168040798\n",
      "Current iteration=178, the loss=0.8244160226835455, the grad=0.02333269035512025\n",
      "Current iteration=179, the loss=0.8244628999552457, the grad=0.023154933017802998\n",
      "Current iteration=180, the loss=0.8245092590581896, the grad=0.022979276867070037\n",
      "Current iteration=181, the loss=0.8245551044187697, the grad=0.022805690197643034\n",
      "Current iteration=182, the loss=0.824600440470369, the grad=0.022634141875184078\n",
      "Current iteration=183, the loss=0.8246452716511837, the grad=0.022464601323847633\n",
      "Current iteration=184, the loss=0.8246896024021461, the grad=0.022297038514172262\n",
      "Current iteration=185, the loss=0.8247334371649424, the grad=0.022131423951300463\n",
      "Current iteration=186, the loss=0.8247767803801266, the grad=0.021967728663515592\n",
      "Current iteration=187, the loss=0.8248196364853206, the grad=0.0218059241910849\n",
      "Current iteration=188, the loss=0.8248620099135049, the grad=0.021645982575398855\n",
      "Current iteration=189, the loss=0.8249039050913871, the grad=0.021487876348396297\n",
      "Current iteration=190, the loss=0.8249453264378512, the grad=0.021331578522266605\n",
      "Current iteration=191, the loss=0.824986278362485, the grad=0.02117706257941925\n",
      "Current iteration=192, the loss=0.8250267652641776, the grad=0.0210243024627124\n",
      "Current iteration=193, the loss=0.8250667915297903, the grad=0.020873272565932004\n",
      "Current iteration=194, the loss=0.8251063615328914, the grad=0.02072394772451347\n",
      "Current iteration=195, the loss=0.8251454796325586, the grad=0.02057630320649812\n",
      "Current iteration=196, the loss=0.8251841501722427, the grad=0.020430314703716878\n",
      "Current iteration=197, the loss=0.8252223774786911, the grad=0.020285958323194388\n",
      "Current iteration=198, the loss=0.8252601658609279, the grad=0.020143210578766474\n",
      "Current iteration=199, the loss=0.8252975196092908, the grad=0.020002048382904185\n",
      "Current iteration=200, the loss=0.8253344429945174, the grad=0.019862449038738586\n",
      "Current iteration=201, the loss=0.825370940266888, the grad=0.01972439023227952\n",
      "Current iteration=202, the loss=0.825407015655411, the grad=0.019587850024823154\n",
      "Current iteration=203, the loss=0.8254426733670583, the grad=0.019452806845541893\n",
      "Current iteration=204, the loss=0.8254779175860466, the grad=0.01931923948425205\n",
      "Current iteration=205, the loss=0.8255127524731584, the grad=0.01918712708435316\n",
      "Current iteration=206, the loss=0.8255471821651073, the grad=0.01905644913593455\n",
      "Current iteration=207, the loss=0.8255812107739409, the grad=0.01892718546904391\n",
      "Current iteration=208, the loss=0.8256148423864831, the grad=0.018799316247113222\n",
      "Current iteration=209, the loss=0.8256480810638126, the grad=0.018672821960537656\n",
      "Current iteration=210, the loss=0.8256809308407764, the grad=0.018547683420402836\n",
      "Current iteration=211, the loss=0.8257133957255355, the grad=0.018423881752356357\n",
      "Current iteration=212, the loss=0.8257454796991451, the grad=0.018301398390619585\n",
      "Current iteration=213, the loss=0.8257771867151643, the grad=0.018180215072135533\n",
      "Current iteration=214, the loss=0.8258085206992951, the grad=0.018060313830849207\n",
      "Current iteration=215, the loss=0.8258394855490506, the grad=0.017941676992116802\n",
      "Current iteration=216, the loss=0.8258700851334497, the grad=0.01782428716723997\n",
      "Current iteration=217, the loss=0.8259003232927385, the grad=0.017708127248121933\n",
      "Current iteration=218, the loss=0.8259302038381356, the grad=0.01759318040204205\n",
      "Current iteration=219, the loss=0.8259597305516013, the grad=0.01747943006654564\n",
      "Current iteration=220, the loss=0.8259889071856329, the grad=0.01736685994444594\n",
      "Current iteration=221, the loss=0.8260177374630753, the grad=0.01725545399893529\n",
      "Current iteration=222, the loss=0.8260462250769597, the grad=0.017145196448802535\n",
      "Current iteration=223, the loss=0.8260743736903565, the grad=0.01703607176375386\n",
      "Current iteration=224, the loss=0.8261021869362521, the grad=0.016928064659834468\n",
      "Current iteration=225, the loss=0.8261296684174392, the grad=0.016821160094948287\n",
      "Current iteration=226, the loss=0.8261568217064307, the grad=0.016715343264473354\n",
      "Current iteration=227, the loss=0.8261836503453827, the grad=0.016610599596970175\n",
      "Current iteration=228, the loss=0.8262101578460411, the grad=0.01650691474998094\n",
      "Current iteration=229, the loss=0.826236347689698, the grad=0.01640427460591708\n",
      "Current iteration=230, the loss=0.8262622233271655, the grad=0.016302665268033038\n",
      "Current iteration=231, the loss=0.8262877881787619, the grad=0.016202073056484054\n",
      "Current iteration=232, the loss=0.8263130456343134, the grad=0.016102484504465775\n",
      "Current iteration=233, the loss=0.8263379990531645, the grad=0.01600388635443385\n",
      "Current iteration=234, the loss=0.8263626517642052, the grad=0.01590626555440124\n",
      "Current iteration=235, the loss=0.8263870070659061, the grad=0.01580960925431167\n",
      "Current iteration=236, the loss=0.826411068226366, the grad=0.0157139048024871\n",
      "Current iteration=237, the loss=0.8264348384833707, the grad=0.015619139742147374\n",
      "Current iteration=238, the loss=0.8264583210444582, the grad=0.015525301808000633\n",
      "Current iteration=239, the loss=0.8264815190869992, the grad=0.015432378922902366\n",
      "Current iteration=240, the loss=0.82650443575828, the grad=0.015340359194581776\n",
      "Current iteration=241, the loss=0.8265270741756001, the grad=0.015249230912433662\n",
      "Current iteration=242, the loss=0.8265494374263724, the grad=0.015158982544374287\n",
      "Current iteration=243, the loss=0.8265715285682352, the grad=0.015069602733759848\n",
      "Current iteration=244, the loss=0.8265933506291692, the grad=0.014981080296365912\n",
      "Current iteration=245, the loss=0.8266149066076218, the grad=0.014893404217426468\n",
      "Current iteration=246, the loss=0.8266361994726394, the grad=0.014806563648731208\n",
      "Current iteration=247, the loss=0.8266572321640028, the grad=0.014720547905779662\n",
      "Current iteration=248, the loss=0.8266780075923721, the grad=0.014635346464990926\n",
      "Current iteration=249, the loss=0.8266985286394338, the grad=0.01455094896096759\n",
      "Current iteration=250, the loss=0.8267187981580548, the grad=0.014467345183812796\n",
      "Current iteration=251, the loss=0.8267388189724427, the grad=0.014384525076499064\n",
      "Current iteration=252, the loss=0.8267585938783061, the grad=0.014302478732287773\n",
      "Current iteration=253, the loss=0.8267781256430232, the grad=0.014221196392198193\n",
      "Current iteration=254, the loss=0.8267974170058162, the grad=0.014140668442524932\n",
      "Current iteration=255, the loss=0.8268164706779205, the grad=0.014060885412402609\n",
      "Current iteration=256, the loss=0.8268352893427698, the grad=0.013981837971416907\n",
      "Current iteration=257, the loss=0.8268538756561745, the grad=0.013903516927260814\n",
      "Current iteration=258, the loss=0.8268722322465072, the grad=0.013825913223435066\n",
      "Current iteration=259, the loss=0.8268903617148928, the grad=0.013749017936992014\n",
      "Current iteration=260, the loss=0.8269082666353966, the grad=0.013672822276321599\n",
      "Current iteration=261, the loss=0.8269259495552189, the grad=0.013597317578978937\n",
      "Current iteration=262, the loss=0.8269434129948904, the grad=0.01352249530955228\n",
      "Current iteration=263, the loss=0.8269606594484683, the grad=0.013448347057570652\n",
      "Current iteration=264, the loss=0.8269776913837382, the grad=0.013374864535450287\n",
      "Current iteration=265, the loss=0.8269945112424117, the grad=0.013302039576478943\n",
      "Current iteration=266, the loss=0.8270111214403316, the grad=0.013229864132837488\n",
      "Current iteration=267, the loss=0.8270275243676749, the grad=0.013158330273657657\n",
      "Current iteration=268, the loss=0.8270437223891584, the grad=0.013087430183115556\n",
      "Current iteration=269, the loss=0.8270597178442437, the grad=0.013017156158559877\n",
      "Current iteration=270, the loss=0.8270755130473468, the grad=0.012947500608674292\n",
      "Current iteration=271, the loss=0.8270911102880438, the grad=0.012878456051673192\n",
      "Current iteration=272, the loss=0.8271065118312814, the grad=0.012810015113530096\n",
      "Current iteration=273, the loss=0.827121719917585, the grad=0.012742170526238123\n",
      "Current iteration=274, the loss=0.8271367367632697, the grad=0.012674915126101784\n",
      "Current iteration=275, the loss=0.8271515645606508, the grad=0.012608241852059453\n",
      "Current iteration=276, the loss=0.827166205478252, the grad=0.012542143744035955\n",
      "Current iteration=277, the loss=0.8271806616610193, the grad=0.012476613941324589\n",
      "Current iteration=278, the loss=0.8271949352305293, the grad=0.012411645680997976\n",
      "Current iteration=279, the loss=0.8272090282852002, the grad=0.012347232296347245\n",
      "Current iteration=280, the loss=0.8272229429005046, the grad=0.012283367215348862\n",
      "Current iteration=281, the loss=0.8272366811291764, the grad=0.012220043959158561\n",
      "Current iteration=282, the loss=0.8272502450014237, the grad=0.012157256140632015\n",
      "Current iteration=283, the loss=0.8272636365251373, the grad=0.012094997462871368\n",
      "Current iteration=284, the loss=0.8272768576861003, the grad=0.012033261717797469\n",
      "Current iteration=285, the loss=0.8272899104481974, the grad=0.011972042784747039\n",
      "Current iteration=286, the loss=0.8273027967536224, the grad=0.011911334629094465\n",
      "Current iteration=287, the loss=0.8273155185230866, the grad=0.01185113130089748\n",
      "Current iteration=288, the loss=0.8273280776560248, the grad=0.01179142693356663\n",
      "Current iteration=289, the loss=0.827340476030803, the grad=0.011732215742557676\n",
      "Current iteration=290, the loss=0.8273527155049235, the grad=0.011673492024086763\n",
      "Current iteration=291, the loss=0.8273647979152284, the grad=0.011615250153867766\n",
      "Current iteration=292, the loss=0.8273767250781044, the grad=0.011557484585871405\n",
      "Current iteration=293, the loss=0.8273884987896861, the grad=0.011500189851105739\n",
      "Current iteration=294, the loss=0.8274001208260567, the grad=0.011443360556417662\n",
      "Current iteration=295, the loss=0.8274115929434498, the grad=0.011386991383314825\n",
      "Current iteration=296, the loss=0.8274229168784484, the grad=0.011331077086807854\n",
      "Current iteration=297, the loss=0.827434094348185, the grad=0.01127561249427218\n",
      "Current iteration=298, the loss=0.8274451270505379, the grad=0.0112205925043294\n",
      "Current iteration=299, the loss=0.8274560166643281, the grad=0.011166012085747563\n",
      "end of the logistic_regression with w= [-8.21799309e-01  1.96359536e-01 -7.01226654e-01 -5.22331351e-02\n",
      "  6.70139913e-02  1.46802031e-01  5.67756170e-02 -5.33341611e-02\n",
      "  2.15061636e-01 -1.26900970e-01  3.94562419e-02 -2.03333922e-01\n",
      "  2.88529142e-01  1.87634850e-01  3.41201212e-01 -4.71323649e-03\n",
      " -6.50265640e-03  8.51036063e-02 -2.38806115e-03  6.82873304e-03\n",
      " -4.25215842e-03  6.79188662e-04 -1.21601001e-03 -8.93829669e-02\n",
      "  2.90842778e-02  5.79937288e-04 -4.28793946e-04 -1.50980995e-02\n",
      "  3.14599973e-03 -1.56744330e-03  5.77338223e-02]  and loss= 0.8274560166643281\n",
      "the accuracy on the train set is  0.744468\n"
     ]
    }
   ],
   "source": [
    "# test regularized logistic regression\n",
    "txOpti = dataClean_without_splitting(tx_tr)\n",
    "print(\"shape of txOpti \", txOpti.shape)\n",
    "print(\"shape of y \", y_tr.shape)\n",
    "max_iters = 300\n",
    "gamma = 0.08\n",
    "lambda_ = 1e-06\n",
    "\n",
    "initial_w = np.zeros(txOpti.shape[1])\n",
    "y_tr[np.where(y_tr == -1)] = 0\n",
    "w, loss = reg_logistic_regression(y_tr, txOpti, lambda_, initial_w, max_iters, gamma)\n",
    "\n",
    "label = predict_logistic(w, txOpti)\n",
    "y_tr[np.where(y_tr == 0)] = -1\n",
    "acc = calculate_accuracy(y_tr, label)\n",
    "\n",
    "print(\"end of the logistic_regression with w=\",w,\" and loss=\", loss)\n",
    "print(\"the accuracy on the train set is \", acc)\n",
    "\n",
    "teOpti = dataClean_without_splitting_te(tx_te, np.mean(tx_te, axis=0), np.std(tx_te, axis=0))\n",
    "y_pred = predict_logistic(w, teOpti)\n",
    "\n",
    "OUTPUT_PATH = 'logistic ridge regression 2, polynomial exp'\n",
    "create_csv_submission(ids_te, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc9b9982-bb31-408a-86a5-c11433e7df5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.0, 1.0, 0.0, 3.0]\n"
     ]
    }
   ],
   "source": [
    "jet = tx_tr[:, 22]\n",
    "\n",
    "values = []\n",
    "\n",
    "for x in jet: \n",
    "    if x not in values: \n",
    "        values.append(x) \n",
    "\n",
    "print(values) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495a314e-ef65-46e5-93b3-e01f401ab78f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
