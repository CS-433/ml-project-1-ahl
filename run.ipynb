{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "882a9018-5319-4a82-87e7-2b0d420475bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "from implementations import *\n",
    "from helpers import *\n",
    "from helper_functions import *\n",
    "from optimization import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6d56f0e3-9ee7-448f-a9dd-dd349209cc8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading the training dataset\n",
      "training dataset loaded\n"
     ]
    }
   ],
   "source": [
    "# load the training dataset\n",
    "print(\"loading the training dataset\")\n",
    "DATA_TRAIN_PATH = 'train.csv'\n",
    "y_tr, tx_tr, ids_tr = load_csv_data(DATA_TRAIN_PATH)\n",
    "print(\"training dataset loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "af90503c-e3fa-4e1b-a222-63ef11b39a80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading the test dataset\n",
      "test dataset loaded\n"
     ]
    }
   ],
   "source": [
    "# load the test dataset\n",
    "print(\"loading the test dataset\")\n",
    "DATA_TEST_PATH = 'test.csv'\n",
    "y_te, tx_te, ids_te = load_csv_data(DATA_TEST_PATH)\n",
    "print(\"test dataset loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa8da103-df5b-4f1e-b595-9ae0cd91750e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shape of the training y is  (250000,)  and the training tx is  (250000, 30)\n",
      "the shape of the test y is  (568238,)  and the test tx is  (568238, 30)\n"
     ]
    }
   ],
   "source": [
    "# printing the shapes of the arrays\n",
    "print(\"the shape of the training y is \",y_tr.shape,\" and the training tx is \", tx_tr.shape)\n",
    "print(\"the shape of the test y is \",y_te.shape,\" and the test tx is \",tx_te.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6884129-6892-4ec8-bce6-fe61104b9ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing the mean square gd\n",
    "initial_w = np.zeros(tx_tr.shape[1])\n",
    "max_iters = 500\n",
    "gamma = 0.00008\n",
    "\n",
    "#w, loss = mean_squared_error_gd(y_tr, tx_tr, initial_w, max_iters, gamma)\n",
    "\n",
    "#print(\"end of the mean_squared_error_gd with w=\",w,\" and loss=\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3abc58ab-10d3-4a01-b62a-6e3445c65b5b",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "dataClean() missing 1 required positional argument: 'y'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# test least square\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m txOpti \u001b[38;5;241m=\u001b[39m \u001b[43mdataClean\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtx_tr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m w, loss \u001b[38;5;241m=\u001b[39m least_squares(y_tr, txOpti)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend of the least_squares with w=\u001b[39m\u001b[38;5;124m\"\u001b[39m,w,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m and loss=\u001b[39m\u001b[38;5;124m\"\u001b[39m, loss)\n",
      "\u001b[0;31mTypeError\u001b[0m: dataClean() missing 1 required positional argument: 'y'"
     ]
    }
   ],
   "source": [
    "# test least square\n",
    "txOpti = dataClean(tx_tr)\n",
    "w, loss = least_squares(y_tr, txOpti)\n",
    "print(\"end of the least_squares with w=\",w,\" and loss=\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1f09a81-21c9-4587-ba44-9bb03ab90776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test mean squared error sgd\n",
    "\n",
    "initial_w = np.zeros(tx_tr.shape[1])\n",
    "max_iters = 100\n",
    "gamma = 0.00001\n",
    "\n",
    "#w, loss = mean_squared_error_sgd(y_tr, tx_tr, initial_w, max_iters, gamma)\n",
    "\n",
    "#print(\"end of the mean_squared_error_sgd with w=\",w,\" and loss=\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34c8ae3d-d431-4939-8e10-d0915a0fc194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of txOpti  (250000, 23)\n",
      "Current lambda=1e-06, the loss=0.35012063246180924\n",
      "Current lambda=1.820689655172414e-05, the loss=0.3501206518793699\n",
      "Current lambda=3.541379310344827e-05, the loss=0.3501207027717671\n",
      "Current lambda=5.2620689655172415e-05, the loss=0.35012078407937985\n",
      "Current lambda=6.982758620689655e-05, the loss=0.3501208943399873\n",
      "Current lambda=8.703448275862068e-05, the loss=0.350121032126924\n",
      "Current lambda=0.00010424137931034483, the loss=0.3501211960819501\n",
      "Current lambda=0.00012144827586206897, the loss=0.3501213849177364\n",
      "Current lambda=0.0001386551724137931, the loss=0.35012159741548876\n",
      "Current lambda=0.00015586206896551724, the loss=0.35012183242154193\n",
      "Current lambda=0.00017306896551724137, the loss=0.3501220888437942\n",
      "Current lambda=0.0001902758620689655, the loss=0.3501223656482302\n",
      "Current lambda=0.00020748275862068967, the loss=0.35012266185560537\n",
      "Current lambda=0.0002246896551724138, the loss=0.3501229765383166\n",
      "Current lambda=0.00024189655172413794, the loss=0.35012330881745757\n",
      "Current lambda=0.00025910344827586207, the loss=0.3501236578600525\n",
      "Current lambda=0.00027631034482758623, the loss=0.35012402287646216\n",
      "Current lambda=0.0002935172413793104, the loss=0.350124403117947\n",
      "Current lambda=0.0003107241379310345, the loss=0.35012479787438394\n",
      "Current lambda=0.00032793103448275866, the loss=0.35012520647211903\n",
      "Current lambda=0.00034513793103448277, the loss=0.35012562827195565\n",
      "Current lambda=0.00036234482758620693, the loss=0.35012606266726143\n",
      "Current lambda=0.00037955172413793104, the loss=0.3501265090821912\n",
      "Current lambda=0.0003967586206896552, the loss=0.35012696697001744\n",
      "Current lambda=0.00041396551724137936, the loss=0.35012743581155753\n",
      "Current lambda=0.00043117241379310347, the loss=0.35012791511369745\n",
      "Current lambda=0.00044837931034482763, the loss=0.35012840440800086\n",
      "Current lambda=0.00046558620689655174, the loss=0.3501289032493984\n",
      "Current lambda=0.0004827931034482759, the loss=0.35012941121495667\n",
      "Current lambda=0.0005, the loss=0.35012992790271524\n",
      "best lambda is  1e-06\n",
      "end of the ridge_regression with w= [-6.39472599e-01 -3.06548796e-01  7.21159032e-02 -1.63641568e-02\n",
      "  1.22515895e-01 -8.31319174e-02  3.05435989e-02 -1.02545421e-01\n",
      "  1.35997850e-01  2.44018911e-01 -1.71358026e-03 -1.69917173e-03\n",
      "  9.38150191e-02 -3.50573132e-04  2.17345682e-03  7.71334399e-02\n",
      "  5.45846163e-04  5.82734292e-03 -7.46010005e-02  2.46202321e-01\n",
      " -1.29631022e-01 -1.08994335e-01  1.05212777e-01]  and loss= 0.35012063246180924\n"
     ]
    }
   ],
   "source": [
    "# test ridge regression \n",
    "\n",
    "txOpti = dataClean(tx_tr)\n",
    "print(\"shape of txOpti \", txOpti.shape)\n",
    "\n",
    "lambda_ = best_lambda(y_tr, txOpti, 0.000001, 0.0005, 30)\n",
    "print(\"best lambda is \",lambda_)\n",
    "w, loss = ridge_regression(y_tr, txOpti, lambda_)\n",
    "\n",
    "#y_pred = predict(w, tx_te)\n",
    "#y_pred[y_pred==0] = -1\n",
    "\n",
    "#OUTPUT_PATH = 'sample-submission'\n",
    "#create_csv_submission(ids_te, y_pred, OUTPUT_PATH)\n",
    "\n",
    "print(\"end of the ridge_regression with w=\",w,\" and loss=\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5f5c7de-0c97-4975-93cb-71f4916eb96e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ml-project-1-ahl/optimization.py:28: RuntimeWarning: invalid value encountered in true_divide\n",
      "  standardize_dataset = (dataset - mean)/std\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of txOpti  (99913, 20)\n",
      "shape of yOpti  (99913, 20)\n",
      "end of the logistic_regression with w= [            nan             nan             nan             nan\n",
      "             nan             nan             nan             nan\n",
      "             nan             nan             nan             nan\n",
      "             nan             nan             nan             nan\n",
      "             nan             nan             nan             nan\n",
      " -2.20216529e+01 -8.90837356e-02 -9.71201484e+00  2.39131652e+00\n",
      "  1.22062735e+00  3.17727480e+00  8.06233589e-01  1.76831964e+00\n",
      " -4.14612204e+00  7.47326497e+00  4.32873868e+00 -4.70646320e-02\n",
      " -2.00705675e-01 -5.23762154e-01 -2.76219476e-01  2.01579973e-02\n",
      " -1.26157926e+00  2.46825128e-01  1.34874504e+00  1.31249739e+00\n",
      " -3.65928336e-02 -8.78085453e-02  1.31249769e+00 -1.42210262e+01\n",
      "  3.41739337e-01 -6.39694330e+00  4.15496098e+00  2.61553710e+00\n",
      "  4.10803161e+00  2.62587228e+00 -3.07700810e+00  1.67921390e+00\n",
      " -3.22844434e+00  1.00224940e+00 -2.76880396e+00  6.24517071e+00\n",
      "  5.73427989e+00  3.79960332e+00 -1.90991517e-01 -9.98874468e-02\n",
      " -2.21495392e-01 -2.57655540e-01  4.22807754e-02  6.89603811e-01\n",
      "  2.76868286e-01 -8.99479141e-01  5.86738696e-01  1.18936254e-01\n",
      "  1.72281079e-01  1.66318061e+00  1.15051380e-01 -1.84587189e-01\n",
      "  9.91082112e-01 -2.36866857e+01  5.46792603e-01 -6.94445725e+00\n",
      "  2.54365421e+00  4.19438661e+00  1.11412633e+00  1.46806096e-01\n",
      " -1.37722570e+00 -6.95148771e-01 -1.34340325e+00 -6.78800293e-01\n",
      " -3.08746564e+00  6.82745695e+00  3.93938066e+00  3.59843043e+00\n",
      " -4.64526927e-01  4.38656354e-03 -2.71167258e-02 -3.84003036e-01\n",
      "  6.92276302e-02  1.45832025e+00  2.80584717e-01 -8.94069013e-01\n",
      " -5.15021310e-01 -8.56609795e-02 -1.58948555e-01 -7.48795489e-01\n",
      "  2.09035755e-01  3.35695287e-02 -1.18511190e+00]  and loss= nan\n",
      "the accurcy on the train set is  0.36205762174507644\n"
     ]
    }
   ],
   "source": [
    "# test logistic regression\n",
    "\n",
    "txOpti, yOpti = dataClean(tx_tr, y_tr)\n",
    "print(\"shape of txOpti \", txOpti[0].shape)\n",
    "print(\"shape of yOpti \", txOpti[0].shape)\n",
    "#initial_w = np.zeros(txOpti.shape[1])\n",
    "# initial_w = w\n",
    "max_iters = 100\n",
    "gamma = 0.5\n",
    "ws = []\n",
    "losses = []\n",
    "y_pred = []\n",
    "accArray = []\n",
    "\n",
    "for i in range(4): \n",
    "    initial_w = np.zeros(txOpti[i].shape[1])\n",
    "    w, loss = logistic_regression(yOpti[i], txOpti[i], initial_w, max_iters, gamma)\n",
    "    label = predict(w, txOpti[i])\n",
    "    acc = calculate_accuracy(yOpti[i], label)\n",
    "    \n",
    "    ws.append(w)\n",
    "    losses.append(loss)\n",
    "    y_pred.append(label)\n",
    "    accArray.append(acc)\n",
    "\n",
    "tot_acc = accArray[0] + accArray[1] + accArray[2] + accArray[3]\n",
    "weights = np.array(list(ws[0])+list(ws[1])+list(ws[2])+list(ws[3]))\n",
    "loss = np.mean(losses)\n",
    "    \n",
    "#teOpti = dataClean(tx_te)\n",
    "#print(\"shape of teOpti \", teOpti.shape)\n",
    "\n",
    "#y_pred = predict(w, teOpti)\n",
    "\n",
    "#OUTPUT_PATH = 'sample-submission LR 2'\n",
    "#create_csv_submission(ids_te, y_pred, OUTPUT_PATH)\n",
    "\n",
    "print(\"end of the logistic_regression with w=\",weights,\" and loss=\", loss)\n",
    "print(\"the accurcy on the train set is \", tot_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e1c71eed-4b5e-41af-b0b6-57d490347155",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'T'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [22]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# test logistic regression without splitting \u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m txOpti \u001b[38;5;241m=\u001b[39m \u001b[43mdataClean_without_splitting\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtx_tr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape of txOpti \u001b[39m\u001b[38;5;124m\"\u001b[39m, txOpti\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape of y \u001b[39m\u001b[38;5;124m\"\u001b[39m, y_tr\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/ml-project-1-ahl/optimization.py:98\u001b[0m, in \u001b[0;36mdataClean_without_splitting\u001b[0;34m(tx)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdataClean_without_splitting\u001b[39m(tx):\n\u001b[1;32m     97\u001b[0m     tx \u001b[38;5;241m=\u001b[39m delete_useless_features(tx)\n\u001b[0;32m---> 98\u001b[0m     tx \u001b[38;5;241m=\u001b[39m \u001b[43mreplace_outliers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m     tx \u001b[38;5;241m=\u001b[39m standardize(tx)\n\u001b[1;32m    100\u001b[0m     tx \u001b[38;5;241m=\u001b[39m add_col_one(tx)\n",
      "File \u001b[0;32m~/ml-project-1-ahl/optimization.py:67\u001b[0m, in \u001b[0;36mreplace_outliers\u001b[0;34m(tx, r)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreplace_outliers\u001b[39m(tx, r):\n\u001b[0;32m---> 67\u001b[0m     tx_clean \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mcopy(\u001b[43mtx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m)\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(tx_clean\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]):\n\u001b[1;32m     70\u001b[0m         line \u001b[38;5;241m=\u001b[39m tx\u001b[38;5;241m.\u001b[39mT[i]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'T'"
     ]
    }
   ],
   "source": [
    "# test logistic regression without splitting \n",
    "txOpti = dataClean_without_splitting(tx_tr)\n",
    "print(\"shape of txOpti \", txOpti.shape)\n",
    "print(\"shape of y \", y_tr.shape)\n",
    "initial_w = np.zeros(txOpti.shape[1])\n",
    "max_iters = 200\n",
    "gamma = 0.5\n",
    "\n",
    "w, loss = logistic_regression(y_tr, txOpti, initial_w, max_iters, gamma)\n",
    "\n",
    "label = predict_logistic(w, txOpti)\n",
    "acc = calculate_accuracy(y_tr, label)\n",
    "\n",
    "print(\"end of the logistic_regression with w=\",w,\" and loss=\", loss)\n",
    "print(\"the accuracy on the train set is \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0b54129f-2e9e-489f-989e-cffe093f018d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[105.012 46.524 73.752 38.468 -999.000 -999.000 -999.000 2.492 12.316\n",
      " 120.665 1.280 -0.356 -999.000 31.804 -0.023 -0.033 40.516 -0.045 0.086\n",
      " 34.802 -0.024 179.739 1.000 38.960 -1.872 -2.093 -999.000 -999.000\n",
      " -999.000 40.513]\n",
      "[-49.023 49.240 81.182 57.896 -708.421 -601.237 -709.357 2.373 18.917\n",
      " 158.432 1.438 -0.128 -708.985 38.707 -0.011 -0.008 46.660 -0.020 0.044\n",
      " 41.717 -0.010 209.797 0.979 -348.330 -399.254 -399.260 -692.381 -709.122\n",
      " -709.119 73.065]\n",
      "(array([ 4,  5,  6, 12, 26, 27, 28]),)\n",
      "index : 4\n",
      "177457\n",
      "index : 5\n",
      "177457\n",
      "index : 6\n",
      "177457\n",
      "index : 12\n",
      "177457\n",
      "index : 26\n",
      "177457\n",
      "index : 27\n",
      "177457\n",
      "index : 28\n",
      "177457\n",
      "counts of each column  [177457, 177457, 177457, 177457, 177457, 177457, 177457]\n"
     ]
    }
   ],
   "source": [
    "#clean column , remove column that has only -999 elements. \n",
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.3f}\".format(x)})\n",
    "\n",
    "median = np.median(tx_tr, axis=0) \n",
    "mean = np.mean(tx_tr, axis=0) \n",
    "\n",
    "print(median)\n",
    "print(mean)\n",
    "\n",
    "indx = np.where(median == -999)\n",
    "print(indx)\n",
    "\n",
    "counts = []\n",
    "count = 0\n",
    "\n",
    "for i in indx[0]: \n",
    "    count = np.count_nonzero( tx_tr[:,i] == -999)\n",
    "    print(\"index :\", i)\n",
    "    print(count)\n",
    "    counts.append(count)\n",
    "\n",
    "print(\"counts of each column \", counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cd948ae1-5376-46f3-b929-6d5d233f0bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7,)\n",
      "(250000, 30)\n",
      "(250000, 23)\n"
     ]
    }
   ],
   "source": [
    "median = np.median(tx_tr, axis=0) \n",
    "indx = np.where(median == -999)\n",
    "tx1 = np.delete(tx_tr, indx[0], 1)\n",
    "print(indx[0].shape)\n",
    "print(tx_tr.shape)\n",
    "print(tx1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f062a94a-ff3d-4d4d-88b2-66b0ca8bf65c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of txOpti  (250000, 24)\n",
      "shape of y  (250000,)\n",
      "Current iteration=0, the loss=0.7132389574361694, the grad=1.0918258430282919\n",
      "Current iteration=1, the loss=0.6336696458366845, the grad=0.3576292264040938\n",
      "Current iteration=2, the loss=0.5950861281576438, the grad=0.13585954946257783\n",
      "Current iteration=3, the loss=0.5803096107003368, the grad=0.05454971342407829\n",
      "Current iteration=4, the loss=0.5742994770510081, the grad=0.022149189318559\n",
      "Current iteration=5, the loss=0.5718675218472237, the grad=0.009037995110171386\n",
      "Current iteration=6, the loss=0.5708785519265858, the grad=0.0036994300603347533\n",
      "Current iteration=7, the loss=0.5704756157237849, the grad=0.0015182023597675768\n",
      "Current iteration=8, the loss=0.5703111505942283, the grad=0.0006246947932911859\n",
      "Current iteration=9, the loss=0.5702439214915862, the grad=0.00025779046675845067\n",
      "Current iteration=10, the loss=0.570216402558284, the grad=0.0001067333971052026\n",
      "Current iteration=11, the loss=0.5702051236522848, the grad=4.435835106811579e-05\n",
      "Current iteration=12, the loss=0.5702004949906176, the grad=1.851476274694823e-05\n",
      "Current iteration=13, the loss=0.5701985930202728, the grad=7.765354877024095e-06\n",
      "Current iteration=14, the loss=0.5701978104380091, the grad=3.2743934839825503e-06\n",
      "Current iteration=15, the loss=0.5701974879888886, the grad=1.3887740993059865e-06\n",
      "Current iteration=16, the loss=0.5701973549338201, the grad=5.926963507733079e-07\n",
      "Current iteration=17, the loss=0.5701972999442202, the grad=2.545963122521087e-07\n",
      "Current iteration=18, the loss=0.5701972771799784, the grad=1.100908495917688e-07\n",
      "Current iteration=19, the loss=0.5701972677394193, the grad=4.7920992437034795e-08\n",
      "Current iteration=20, the loss=0.5701972638168962, the grad=2.09947017142995e-08\n",
      "Current iteration=21, the loss=0.5701972621838061, the grad=9.255112270025337e-09\n",
      "Current iteration=22, the loss=0.5701972615024303, the grad=4.103713771571534e-09\n",
      "Current iteration=23, the loss=0.5701972612174919, the grad=1.8293695586656387e-09\n",
      "Current iteration=24, the loss=0.5701972610980497, the grad=8.194928176336242e-10\n",
      "Current iteration=25, the loss=0.5701972610478546, the grad=3.6871877957060076e-10\n",
      "Current iteration=26, the loss=0.5701972610267043, the grad=1.665506245026597e-10\n",
      "Current iteration=27, the loss=0.5701972610177677, the grad=7.549281573255921e-11\n",
      "Current iteration=28, the loss=0.5701972610139808, the grad=3.4324461796297424e-11\n",
      "Current iteration=29, the loss=0.5701972610123717, the grad=1.5648732332539855e-11\n",
      "Current iteration=30, the loss=0.5701972610116856, the grad=7.151645430270919e-12\n",
      "Current iteration=31, the loss=0.5701972610113921, the grad=3.2754902840321684e-12\n",
      "Current iteration=32, the loss=0.5701972610112662, the grad=1.503023439982204e-12\n",
      "Current iteration=33, the loss=0.5701972610112122, the grad=6.908876480613027e-13\n",
      "Current iteration=34, the loss=0.5701972610111887, the grad=3.1814543423051734e-13\n",
      "Current iteration=35, the loss=0.5701972610111788, the grad=1.4671031762326608e-13\n",
      "Current iteration=36, the loss=0.5701972610111741, the grad=6.771169235215013e-14\n",
      "Current iteration=37, the loss=0.5701972610111725, the grad=3.1205362479210485e-14\n",
      "Current iteration=38, the loss=0.5701972610111717, the grad=1.4499113503267598e-14\n",
      "Current iteration=39, the loss=0.5701972610111713, the grad=6.676390574925388e-15\n",
      "Current iteration=40, the loss=0.5701972610111711, the grad=3.087546502672221e-15\n",
      "Current iteration=41, the loss=0.570197261011171, the grad=1.4546411626553702e-15\n",
      "Current iteration=42, the loss=0.570197261011171, the grad=7.004003409686158e-16\n",
      "Current iteration=43, the loss=0.570197261011171, the grad=3.416582181780386e-16\n",
      "Current iteration=44, the loss=0.570197261011171, the grad=2.5633126632811127e-16\n",
      "Current iteration=45, the loss=0.570197261011171, the grad=1.147618277360976e-16\n",
      "Current iteration=46, the loss=0.570197261011171, the grad=1.2580055369076056e-16\n",
      "Current iteration=47, the loss=0.570197261011171, the grad=1.2283379023942895e-16\n",
      "Current iteration=48, the loss=0.570197261011171, the grad=1.2148516260686207e-16\n",
      "Current iteration=49, the loss=0.570197261011171, the grad=1.2058999843443964e-16\n",
      "Current iteration=50, the loss=0.570197261011171, the grad=1.2058999486466685e-16\n",
      "Current iteration=51, the loss=0.570197261011171, the grad=1.2058999397222365e-16\n",
      "Current iteration=52, the loss=0.570197261011171, the grad=1.2058999372233953e-16\n",
      "Current iteration=53, the loss=0.570197261011171, the grad=1.205899936866418e-16\n",
      "Current iteration=54, the loss=0.570197261011171, the grad=1.2058999367474258e-16\n",
      "Current iteration=55, the loss=0.570197261011171, the grad=1.2058999367474258e-16\n",
      "Current iteration=56, the loss=0.570197261011171, the grad=1.2058999367474258e-16\n",
      "Current iteration=57, the loss=0.570197261011171, the grad=1.2058999367474258e-16\n",
      "Current iteration=58, the loss=0.570197261011171, the grad=1.2058999367474258e-16\n",
      "Current iteration=59, the loss=0.570197261011171, the grad=1.2058999367474258e-16\n",
      "Current iteration=60, the loss=0.570197261011171, the grad=1.2058999367474258e-16\n",
      "Current iteration=61, the loss=0.570197261011171, the grad=1.2058999367474258e-16\n",
      "Current iteration=62, the loss=0.570197261011171, the grad=1.2058999367474258e-16\n",
      "Current iteration=63, the loss=0.570197261011171, the grad=1.2058999367474258e-16\n",
      "Current iteration=64, the loss=0.570197261011171, the grad=1.2058999367474258e-16\n",
      "Current iteration=65, the loss=0.570197261011171, the grad=1.2058999367474258e-16\n",
      "Current iteration=66, the loss=0.570197261011171, the grad=1.2058999367474258e-16\n",
      "Current iteration=67, the loss=0.570197261011171, the grad=1.2058999367474258e-16\n",
      "Current iteration=68, the loss=0.570197261011171, the grad=1.2058999367474258e-16\n",
      "Current iteration=69, the loss=0.570197261011171, the grad=1.2058999367474258e-16\n",
      "Current iteration=70, the loss=0.570197261011171, the grad=1.2058999367474258e-16\n",
      "Current iteration=71, the loss=0.570197261011171, the grad=1.2058999367474258e-16\n",
      "Current iteration=72, the loss=0.570197261011171, the grad=1.2058999367474258e-16\n",
      "Current iteration=73, the loss=0.570197261011171, the grad=1.2058999367474258e-16\n",
      "Current iteration=74, the loss=0.570197261011171, the grad=1.2058999367474258e-16\n",
      "Current iteration=75, the loss=0.570197261011171, the grad=1.2058999367474258e-16\n",
      "Current iteration=76, the loss=0.570197261011171, the grad=1.2058999367474258e-16\n",
      "Current iteration=77, the loss=0.570197261011171, the grad=1.2058999367474258e-16\n",
      "Current iteration=78, the loss=0.570197261011171, the grad=1.2058999367474258e-16\n",
      "Current iteration=79, the loss=0.570197261011171, the grad=1.2058999367474258e-16\n",
      "Current iteration=80, the loss=0.570197261011171, the grad=1.2058999367474258e-16\n",
      "Current iteration=81, the loss=0.570197261011171, the grad=1.2058999367474258e-16\n",
      "Current iteration=82, the loss=0.570197261011171, the grad=1.2058999367474258e-16\n",
      "Current iteration=83, the loss=0.570197261011171, the grad=1.2058999367474258e-16\n",
      "Current iteration=84, the loss=0.570197261011171, the grad=1.2058999367474258e-16\n",
      "Current iteration=85, the loss=0.570197261011171, the grad=1.2058999367474258e-16\n",
      "Current iteration=86, the loss=0.570197261011171, the grad=1.2058999367474258e-16\n",
      "Current iteration=87, the loss=0.570197261011171, the grad=1.2058999367474258e-16\n",
      "Current iteration=88, the loss=0.570197261011171, the grad=1.2058999367474258e-16\n",
      "Current iteration=89, the loss=0.570197261011171, the grad=1.2058999367474258e-16\n",
      "Current iteration=90, the loss=0.570197261011171, the grad=1.2058999367474258e-16\n",
      "Current iteration=91, the loss=0.570197261011171, the grad=1.2058999367474258e-16\n",
      "Current iteration=92, the loss=0.570197261011171, the grad=1.2058999367474258e-16\n",
      "Current iteration=93, the loss=0.570197261011171, the grad=1.2058999367474258e-16\n",
      "Current iteration=94, the loss=0.570197261011171, the grad=1.2058999367474258e-16\n",
      "Current iteration=95, the loss=0.570197261011171, the grad=1.2058999367474258e-16\n",
      "Current iteration=96, the loss=0.570197261011171, the grad=1.2058999367474258e-16\n",
      "Current iteration=97, the loss=0.570197261011171, the grad=1.2058999367474258e-16\n",
      "Current iteration=98, the loss=0.570197261011171, the grad=1.2058999367474258e-16\n",
      "Current iteration=99, the loss=0.570197261011171, the grad=1.2058999367474258e-16\n",
      "end of the logistic_regression with w= [-6.70517457e-01  1.23205425e-02 -2.58690832e-01  1.06232115e-01\n",
      "  5.84871741e-02  3.19949081e-02 -3.16527424e-02  7.57764864e-02\n",
      " -1.53174030e-01  1.54700462e-01  1.73714727e-01  9.46227871e-06\n",
      " -2.65395329e-03 -2.62288865e-02 -1.09415096e-03  3.71347635e-03\n",
      " -5.55903926e-02  5.08978951e-03  6.32733694e-02  8.51415309e-02\n",
      "  6.25394089e-02  2.32601128e-02  2.49443363e-02  7.00649643e-02]  and loss= 0.570197261011171\n",
      "the accuracy on the train set is  0.708012\n"
     ]
    }
   ],
   "source": [
    "# test regularized logistic regression\n",
    "txOpti = dataClean_without_splitting(tx_tr)\n",
    "print(\"shape of txOpti \", txOpti.shape)\n",
    "print(\"shape of y \", y_tr.shape)\n",
    "initial_w = np.zeros(txOpti.shape[1])\n",
    "max_iters = 200\n",
    "gamma = 0.5\n",
    "lambda_ = 0.5\n",
    "\n",
    "w, loss = reg_logistic_regression(y_tr, txOpti, lambda_, initial_w, max_iters, gamma)\n",
    "\n",
    "label = predict_logistic(w, txOpti)\n",
    "acc = calculate_accuracy(y_tr, label)\n",
    "\n",
    "print(\"end of the logistic_regression with w=\",w,\" and loss=\", loss)\n",
    "print(\"the accuracy on the train set is \", acc)\n",
    "\n",
    "teOpti = dataClean(tx_te)\n",
    "#print(\"shape of teOpti \", teOpti.shape)\n",
    "\n",
    "#y_pred = predict(w, teOpti)\n",
    "\n",
    "#OUTPUT_PATH = 'sample-submission LR 2'\n",
    "#create_csv_submission(ids_te, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc9b9982-bb31-408a-86a5-c11433e7df5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.0, 1.0, 0.0, 3.0]\n"
     ]
    }
   ],
   "source": [
    "jet = tx_tr[:, 22]\n",
    "\n",
    "values = []\n",
    "\n",
    "for x in jet: \n",
    "    if x not in values: \n",
    "        values.append(x) \n",
    "\n",
    "print(values) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495a314e-ef65-46e5-93b3-e01f401ab78f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
