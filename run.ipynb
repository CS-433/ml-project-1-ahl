{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "882a9018-5319-4a82-87e7-2b0d420475bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from implementations import *\n",
    "from helpers import *\n",
    "from helper_functions import *\n",
    "from optimization import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d56f0e3-9ee7-448f-a9dd-dd349209cc8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading the training dataset\n",
      "training dataset loaded\n"
     ]
    }
   ],
   "source": [
    "# load the training dataset\n",
    "print(\"loading the training dataset\")\n",
    "DATA_TRAIN_PATH = 'train.csv'\n",
    "y_tr, tx_tr, ids_tr = load_csv_data(DATA_TRAIN_PATH)\n",
    "print(\"training dataset loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af90503c-e3fa-4e1b-a222-63ef11b39a80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading the test dataset\n",
      "test dataset loaded\n"
     ]
    }
   ],
   "source": [
    "# load the test dataset\n",
    "print(\"loading the test dataset\")\n",
    "DATA_TEST_PATH = 'test.csv'\n",
    "y_te, tx_te, ids_te = load_csv_data(DATA_TEST_PATH)\n",
    "print(\"test dataset loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa8da103-df5b-4f1e-b595-9ae0cd91750e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shape of the training y is  (250000,)  and the training tx is  (250000, 30)\n",
      "the shape of the test y is  (568238,)  and the test tx is  (568238, 30)\n"
     ]
    }
   ],
   "source": [
    "# printing the shapes of the arrays\n",
    "print(\"the shape of the training y is \",y_tr.shape,\" and the training tx is \", tx_tr.shape)\n",
    "print(\"the shape of the test y is \",y_te.shape,\" and the test tx is \",tx_te.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6884129-6892-4ec8-bce6-fe61104b9ff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, the loss=12796.178310949084, the grad=-83.42721036786665\n",
      "Current iteration=1, the loss=2680491373.0276556, the grad=37489.38639399656\n",
      "Current iteration=2, the loss=561706185275329.8, the grad=-17128797.794126377\n",
      "Current iteration=3, the loss=1.177076162364914e+20, the grad=7840049095.731387\n",
      "Current iteration=4, the loss=2.4666068135676807e+25, the grad=-3588907012061.712\n",
      "Current iteration=5, the loss=5.168866185241528e+30, the grad=1642893992715460.5\n",
      "Current iteration=6, the loss=1.0831551057908984e+36, the grad=-7.520681452936573e+17\n",
      "Current iteration=7, the loss=2.269791751527176e+41, the grad=3.4427451308580327e+20\n",
      "Current iteration=8, the loss=4.756432913215095e+46, the grad=-1.5759867201147976e+23\n",
      "Current iteration=9, the loss=9.967281819001249e+51, the grad=7.214400278342999e+25\n",
      "Current iteration=10, the loss=2.0886809227009536e+57, the grad=-3.3025387024973844e+28\n",
      "Current iteration=11, the loss=4.376908445127169e+62, the grad=1.5118043719125051e+31\n",
      "Current iteration=12, the loss=9.171974200948061e+67, the grad=-6.920592504202905e+33\n",
      "Current iteration=13, the loss=1.9220212576416524e+73, the grad=3.1680422083077794e+36\n",
      "Current iteration=14, the loss=4.027666927415197e+78, the grad=-1.4502358616728893e+39\n",
      "Current iteration=15, the loss=8.440125630087429e+83, the grad=6.638750105559774e+41\n",
      "Current iteration=16, the loss=1.7686596716023663e+89, the grad=-3.0390231085052896e+44\n",
      "Current iteration=17, the loss=3.706292028167576e+94, the grad=1.3911747403015717e+47\n",
      "Current iteration=18, the loss=7.766672593158337e+99, the grad=-6.368385790277964e+49\n",
      "Current iteration=19, the loss=1.627535086573851e+105, the grad=2.9152583351982595e+52\n",
      "Current iteration=20, the loss=3.4105602190085135e+110, the grad=-1.3345188939271183e+55\n",
      "Current iteration=21, the loss=7.1469556038696145e+115, the grad=6.109032111308043e+57\n",
      "Current iteration=22, the loss=1.4976710898989041e+121, the grad=-2.7965339049767677e+60\n",
      "Current iteration=23, the loss=3.138425390951808e+126, the grad=1.2801703672842678e+63\n",
      "Current iteration=24, the loss=6.576686964850131e+131, the grad=-5.860240658467371e+65\n",
      "Current iteration=25, the loss=1.3781691786693101e+137, the grad=2.6826445489444907e+68\n",
      "Current iteration=26, the loss=2.8880046977837636e+142, the grad=-1.2280351943538974e+71\n",
      "Current iteration=27, the loss=6.0519211019319906e+147, the grad=5.6215812831602205e+73\n",
      "Current iteration=28, the loss=1.2682025431647015e+153, the grad=-2.573393357818551e+76\n",
      "Current iteration=29, the loss=2.6575655290284247e+158, the grad=1.1780232359002417e+79\n",
      "Current iteration=30, the loss=5.56902726551535e+163, the grad=-5.392641354671306e+81\n",
      "Current iteration=31, the loss=1.1670103463221747e+169, the grad=2.46859144148272e+84\n",
      "Current iteration=32, the loss=2.4455135223637204e+174, the grad=-1.130048023624441e+87\n",
      "Current iteration=33, the loss=5.1246644101411964e+179, the grad=5.173025046746858e+89\n",
      "Current iteration=34, the loss=1.0738924596574717e+185, the grad=-2.368057602405382e+92\n",
      "Current iteration=35, the loss=2.2503815325487804e+190, the grad=1.0840266106649562e+95\n",
      "Current iteration=36, the loss=4.715758078468943e+195, the grad=-4.962352653229879e+97\n",
      "Current iteration=37, the loss=9.882046192166311e+200, the grad=2.271618022357621e+100\n",
      "Current iteration=38, the loss=2.070819480540744e+206, the grad=-1.0398794281863988e+103\n",
      "Current iteration=39, the loss=4.339479129723609e+211, the grad=4.760259931566238e+105\n",
      "Current iteration=40, the loss=9.093539680431008e+216, the grad=-2.1791059618897637e+108\n",
      "Current iteration=41, the loss=1.905585012569012e+222, the grad=9.975301478087868e+110\n",
      "Current iteration=42, the loss=3.993224165439098e+227, the grad=-4.566397473046602e+113\n",
      "Current iteration=43, the loss=8.367949543195348e+232, the grad=2.090361472046801e+116\n",
      "Current iteration=44, the loss=1.753534904539061e+238, the grad=-9.569055496394088e+118\n",
      "Current iteration=45, the loss=3.67459751706707e+243, the grad=4.3804300986953825e+121\n",
      "Current iteration=46, the loss=7.700255568043474e+248, the grad=-2.0052311178242343e+124\n",
      "Current iteration=47, the loss=1.613617152294565e+254, the grad=9.179353956791091e+126\n",
      "Current iteration=48, the loss=3.3813946708275286e+259, the grad=-4.202036279762241e+129\n",
      "Current iteration=49, the loss=7.0858381144758605e+264, the grad=1.9235677128862625e+132\n",
      "Current iteration=50, the loss=1.4848636930119475e+270, the grad=-8.805523083841263e+134\n",
      "Current iteration=51, the loss=3.111587015120748e+275, the grad=4.030907581814147e+137\n",
      "Current iteration=52, the loss=6.520446151544595e+280, the grad=-1.8452300650875988e+140\n",
      "Current iteration=53, the loss=1.3663837073681473e+286, the grad=8.446916541735228e+142\n",
      "Current iteration=54, the loss=2.8633078049710684e+291, the grad=-3.866748131466899e+145\n",
      "Current iteration=55, the loss=6.000167845824068e+296, the grad=1.7700827323589565e+148\n",
      "Current iteration=56, the loss=1.2573574561406553e+302, the grad=-8.102914316807942e+150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:179: RuntimeWarning: overflow encountered in reduce\n",
      "  ret = umr_sum(arr, axis, dtype, out, keepdims, where=where)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=57, the loss=inf, the grad=3.7092741048341244e+153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ml-project-1-ahl/helper_functions.py:25: RuntimeWarning: overflow encountered in square\n",
      "  return (1 / 2) * np.mean(e**2)\n",
      "/home/ml-project-1-ahl/implementations.py:36: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=58, the loss=inf, the grad=-1.6979957885341552e+156\n",
      "Current iteration=59, the loss=inf, the grad=7.772921645564562e+158\n",
      "Current iteration=60, the loss=inf, the grad=-3.5582132368092603e+161\n",
      "Current iteration=61, the loss=inf, the grad=1.6288445987139566e+164\n",
      "Current iteration=62, the loss=inf, the grad=-7.456367986362624e+166\n",
      "Current iteration=63, the loss=inf, the grad=3.41330435033213e+169\n",
      "Current iteration=64, the loss=inf, the grad=-1.5625096037782444e+172\n",
      "Current iteration=65, the loss=inf, the grad=7.152706032972663e+174\n",
      "Current iteration=66, the loss=inf, the grad=-3.274296904826221e+177\n",
      "Current iteration=67, the loss=inf, the grad=1.4988761136739909e+180\n",
      "Current iteration=68, the loss=inf, the grad=-6.861410768311749e+182\n",
      "Current iteration=69, the loss=inf, the grad=3.140950563025931e+185\n",
      "Current iteration=70, the loss=inf, the grad=-1.4378341091216056e+188\n",
      "Current iteration=71, the loss=inf, the grad=6.581978556713927e+190\n",
      "Current iteration=72, the loss=inf, the grad=-3.0130347754448714e+193\n",
      "Current iteration=73, the loss=inf, the grad=1.3792780513968338e+196\n",
      "Current iteration=74, the loss=inf, the grad=-6.313926273168085e+198\n",
      "Current iteration=75, the loss=inf, the grad=2.89032838176676e+201\n",
      "Current iteration=76, the loss=inf, the grad=-1.3231066998593166e+204\n",
      "Current iteration=77, the loss=inf, the grad=6.0567904680177645e+206\n",
      "Current iteration=78, the loss=inf, the grad=-2.772619228469741e+209\n",
      "Current iteration=79, the loss=inf, the grad=1.2692229369123346e+212\n",
      "Current iteration=80, the loss=inf, the grad=-5.810126565678733e+214\n",
      "Current iteration=81, the loss=inf, the grad=2.6597038020230326e+217\n",
      "Current iteration=82, the loss=inf, the grad=-1.2175336000911023e+220\n",
      "Current iteration=83, the loss=inf, the grad=5.57350809598896e+222\n",
      "Current iteration=84, the loss=inf, the grad=-2.5513868770217194e+225\n",
      "Current iteration=85, the loss=inf, the grad=1.167949320989295e+228\n",
      "Current iteration=86, the loss=inf, the grad=-5.3465259568619415e+230\n",
      "Current iteration=87, the loss=inf, the grad=2.447481178651288e+233\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m max_iters \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m500\u001b[39m\n\u001b[1;32m      4\u001b[0m gamma \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.00008\u001b[39m\n\u001b[0;32m----> 6\u001b[0m w, loss \u001b[38;5;241m=\u001b[39m \u001b[43mmean_squared_error_gd\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_tr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtx_tr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_w\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend of the mean_squared_error_gd with w=\u001b[39m\u001b[38;5;124m\"\u001b[39m,w,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m and loss=\u001b[39m\u001b[38;5;124m\"\u001b[39m, loss)\n",
      "File \u001b[0;32m~/ml-project-1-ahl/implementations.py:28\u001b[0m, in \u001b[0;36mmean_squared_error_gd\u001b[0;34m(y, tx, initial_w, max_iters, gamma)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# update w by gradient descent\u001b[39;00m\n\u001b[1;32m     27\u001b[0m w \u001b[38;5;241m=\u001b[39m w \u001b[38;5;241m-\u001b[39m (gamma \u001b[38;5;241m*\u001b[39m grad)\n\u001b[0;32m---> 28\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_mse\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28miter\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCurrent iteration=\u001b[39m\u001b[38;5;132;01m{i}\u001b[39;00m\u001b[38;5;124m, the loss=\u001b[39m\u001b[38;5;132;01m{l}\u001b[39;00m\u001b[38;5;124m, the grad=\u001b[39m\u001b[38;5;132;01m{we}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(i\u001b[38;5;241m=\u001b[39m\u001b[38;5;28miter\u001b[39m, l\u001b[38;5;241m=\u001b[39mloss, we\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mmean(grad)))\n",
      "File \u001b[0;32m~/ml-project-1-ahl/helper_functions.py:24\u001b[0m, in \u001b[0;36mcalculate_mse\u001b[0;34m(y, tx, w)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalculate_mse\u001b[39m(y, tx, w):\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;124;03m\"\"\"Calculate the mse.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m     e \u001b[38;5;241m=\u001b[39m y \u001b[38;5;241m-\u001b[39m \u001b[43mtx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(e\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# testing the mean square gd\n",
    "initial_w = np.zeros(tx_tr.shape[1])\n",
    "max_iters = 500\n",
    "gamma = 0.00008\n",
    "\n",
    "w, loss = mean_squared_error_gd(y_tr, tx_tr, initial_w, max_iters, gamma)\n",
    "\n",
    "print(\"end of the mean_squared_error_gd with w=\",w,\" and loss=\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3abc58ab-10d3-4a01-b62a-6e3445c65b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end of the least_squares with w= [ 8.03494312e-05 -7.20202273e-03 -6.05417273e-03 -5.47559065e-04\n",
      " -1.93874700e-02  4.73451621e-04 -2.60379054e-02  3.25106300e-01\n",
      " -3.80780282e-05 -2.72724919e+00 -2.21220140e-01  9.50794091e-02\n",
      "  6.40351613e-02  2.73550887e+00 -3.31801241e-04 -9.54325120e-04\n",
      "  2.74026561e+00 -5.34164891e-04  9.73498581e-04  3.69225052e-03\n",
      "  3.54487449e-04 -5.43344598e-04 -3.30448035e-01 -1.40800498e-03\n",
      "  8.31432888e-04  1.02117272e-03 -1.68047416e-03 -5.83664818e-03\n",
      " -1.11087997e-02  2.72770912e+00]  and loss= 0.3396868094770935\n"
     ]
    }
   ],
   "source": [
    "# test least square\n",
    "w, loss = least_squares(y_tr, tx_tr)\n",
    "print(\"end of the least_squares with w=\",w,\" and loss=\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1f09a81-21c9-4587-ba44-9bb03ab90776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end of the mean_squared_error_sgd with w= [ 8.03494312e-05 -7.20202273e-03 -6.05417273e-03 -5.47559065e-04\n",
      " -1.93874700e-02  4.73451621e-04 -2.60379054e-02  3.25106300e-01\n",
      " -3.80780282e-05 -2.72724919e+00 -2.21220140e-01  9.50794091e-02\n",
      "  6.40351613e-02  2.73550887e+00 -3.31801241e-04 -9.54325120e-04\n",
      "  2.74026561e+00 -5.34164891e-04  9.73498581e-04  3.69225052e-03\n",
      "  3.54487449e-04 -5.43344598e-04 -3.30448035e-01 -1.40800498e-03\n",
      "  8.31432888e-04  1.02117272e-03 -1.68047416e-03 -5.83664818e-03\n",
      " -1.11087997e-02  2.72770912e+00]  and loss= 0.3396868094770935\n"
     ]
    }
   ],
   "source": [
    "# test mean squared error sgd\n",
    "\n",
    "initial_w = np.zeros(tx_tr.shape[1])\n",
    "max_iters = 100\n",
    "gamma = 0.00001\n",
    "\n",
    "#w, loss = mean_squared_error_sgd(y_tr, tx_tr, initial_w, max_iters, gamma)\n",
    "\n",
    "print(\"end of the mean_squared_error_sgd with w=\",w,\" and loss=\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34c8ae3d-d431-4939-8e10-d0915a0fc194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current lambda=0.0001, the loss=0.33968798676609097\n",
      "Current lambda=0.00013738237958832623, the loss=0.33968803954866844\n",
      "Current lambda=0.00018873918221350977, the loss=0.33968813859041375\n",
      "Current lambda=0.0002592943797404667, the loss=0.3396883245147402\n",
      "Current lambda=0.0003562247890262444, the loss=0.3396886731920951\n",
      "Current lambda=0.0004893900918477494, the loss=0.3396893258059021\n",
      "Current lambda=0.0006723357536499335, the loss=0.33969054373021085\n",
      "Current lambda=0.0009236708571873865, the loss=0.33969280743924646\n",
      "Current lambda=0.0012689610031679222, the loss=0.3396969916896993\n",
      "Current lambda=0.0017433288221999873, the loss=0.33970466833847507\n",
      "Current lambda=0.002395026619987486, the loss=0.3397186122395501\n",
      "Current lambda=0.0032903445623126675, the loss=0.33974360637308687\n",
      "Current lambda=0.004520353656360241, the loss=0.33978763536530615\n",
      "Current lambda=0.006210169418915616, the loss=0.3398634706893294\n",
      "Current lambda=0.008531678524172805, the loss=0.33999040783666146\n",
      "Current lambda=0.011721022975334805, the loss=0.34019545283472974\n",
      "Current lambda=0.01610262027560939, the loss=0.3405126532289999\n",
      "Current lambda=0.02212216291070448, the loss=0.3409789456162246\n",
      "Current lambda=0.03039195382313198, the loss=0.34162564114876065\n",
      "Current lambda=0.041753189365604, the loss=0.34246709117086294\n",
      "Current lambda=0.05736152510448681, the loss=0.34349132395827114\n",
      "Current lambda=0.07880462815669913, the loss=0.34465867526889005\n",
      "Current lambda=0.1082636733874054, the loss=0.34591102937226975\n",
      "Current lambda=0.14873521072935117, the loss=0.34718767370275866\n",
      "Current lambda=0.20433597178569418, the loss=0.34843939327798257\n",
      "Current lambda=0.2807216203941176, the loss=0.3496342050611894\n",
      "Current lambda=0.38566204211634725, the loss=0.35075424601787725\n",
      "Current lambda=0.5298316906283708, the loss=0.351788603311994\n",
      "Current lambda=0.7278953843983146, the loss=0.35272780782090674\n",
      "Current lambda=1.0, the loss=0.3535627339505689\n",
      "best lambda is  0.0001\n",
      "end of the ridge_regression with w= [ 8.06960854e-05 -7.20578084e-03 -6.04517159e-03 -5.52739531e-04\n",
      " -1.94754484e-02  4.73682819e-04 -2.60450820e-02  3.24395228e-01\n",
      " -3.82847694e-05  4.40588717e-03 -2.20862099e-01  9.50768388e-02\n",
      "  6.40870833e-02  3.85465642e-03 -3.32608132e-04 -9.55589083e-04\n",
      "  8.59668812e-03 -5.32284362e-04  9.71536530e-04  3.69559157e-03\n",
      "  3.55703473e-04 -5.43892997e-04 -3.29573140e-01 -1.39974366e-03\n",
      "  8.26957837e-04  1.01646507e-03 -1.67549174e-03 -5.82354748e-03\n",
      " -1.10844940e-02 -3.95386302e-03]  and loss= 0.33968798676609097\n"
     ]
    }
   ],
   "source": [
    "# test ridge regression \n",
    "\n",
    "#initial_w = np.zeros(tx_tr.shape[1])\n",
    "#max_iters = 100\n",
    "#lambda_ = 0.0005\n",
    "\n",
    "#w, loss = ridge_regression(y_tr, tx_tr, lambda_)\n",
    "lambda_ = ridge_regression_best_lambda(y_tr, tx_tr)\n",
    "print(\"best lambda is \",lambda_)\n",
    "w, loss = ridge_regression(y_tr, tx_tr, lambda_)\n",
    "\n",
    "#y_pred = predict(w, tx_te)\n",
    "#y_pred[y_pred==0] = -1\n",
    "\n",
    "#OUTPUT_PATH = 'sample-submission'\n",
    "#create_csv_submission(ids_te, y_pred, OUTPUT_PATH)\n",
    "\n",
    "print(\"end of the ridge_regression with w=\",w,\" and loss=\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f5c7de-0c97-4975-93cb-71f4916eb96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test logistic regression \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
