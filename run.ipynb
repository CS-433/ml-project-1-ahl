{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "882a9018-5319-4a82-87e7-2b0d420475bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from implementations import *\n",
    "from helpers import *\n",
    "from helper_functions import *\n",
    "from optimization import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6001fe01-6df9-475b-9bd1-0dc91e667ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d56f0e3-9ee7-448f-a9dd-dd349209cc8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading the training dataset\n",
      "training dataset loaded\n"
     ]
    }
   ],
   "source": [
    "# load the training dataset\n",
    "print(\"loading the training dataset\")\n",
    "DATA_TRAIN_PATH = 'train.csv'\n",
    "y_tr, tx_tr, ids_tr = load_csv_data(DATA_TRAIN_PATH)\n",
    "print(\"training dataset loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af90503c-e3fa-4e1b-a222-63ef11b39a80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading the test dataset\n",
      "test dataset loaded\n"
     ]
    }
   ],
   "source": [
    "# load the test dataset\n",
    "print(\"loading the test dataset\")\n",
    "DATA_TEST_PATH = 'test.csv'\n",
    "y_te, tx_te, ids_te = load_csv_data(DATA_TEST_PATH)\n",
    "print(\"test dataset loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa8da103-df5b-4f1e-b595-9ae0cd91750e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shape of the training y is  (250000,)  and the training tx is  (250000, 30)\n",
      "the shape of the test y is  (568238,)  and the test tx is  (568238, 30)\n"
     ]
    }
   ],
   "source": [
    "# printing the shapes of the arrays\n",
    "print(\"the shape of the training y is \",y_tr.shape,\" and the training tx is \", tx_tr.shape)\n",
    "print(\"the shape of the test y is \",y_te.shape,\" and the test tx is \",tx_te.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5e092ef-cd07-432f-83fc-1472fc29ddb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(formatter={'float': lambda x: \"{0:10.3f}\".format(x)})\n",
    "\n",
    "def print_col_info(t):\n",
    "    mean_ = np.nanmean(t, axis=0)\n",
    "    max_ = np.nanmax(t, axis=0)\n",
    "    min_ = np.nanmin(t, axis=0)\n",
    "    median_ = np.nanmedian(t, axis=0)\n",
    "    std_ = np.nanstd(t, axis=0)\n",
    "    print(t.shape)\n",
    "\n",
    "    table = np.vstack([range(t.shape[1]), mean_, max_, min_, median_, std_])\n",
    "\n",
    "    for line in table.T:\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6884129-6892-4ec8-bce6-fe61104b9ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing the mean square gd\n",
    "initial_w = np.zeros(tx_tr.shape[1])\n",
    "max_iters = 500\n",
    "gamma = 0.00008\n",
    "\n",
    "#w, loss = mean_squared_error_gd(y_tr, tx_tr, initial_w, max_iters, gamma)\n",
    "\n",
    "#print(\"end of the mean_squared_error_gd with w=\",w,\" and loss=\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3abc58ab-10d3-4a01-b62a-6e3445c65b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end of the least_squares with w= [-3.14664000e-01  8.91640668e-02 -2.75720992e-01 -7.01312146e-02\n",
      "  3.18652180e-03  7.18952546e-02  1.92091255e-02 -7.18571998e-03\n",
      "  9.49270656e-02 -4.27126248e-02  8.93445861e-03 -6.26613695e-02\n",
      "  1.03495757e-01  7.09054025e-02  1.55596716e-01 -1.38113243e-03\n",
      " -1.72800879e-03  6.05737448e-02 -7.37853585e-04  2.70933535e-03\n",
      "  2.99658031e-02  1.72500182e-04 -1.18433871e-03 -5.26513161e-02\n",
      " -5.56719522e-03  2.24121971e-04 -4.34743233e-04 -5.26989925e-03\n",
      "  1.05572455e-03 -1.08202820e-03  5.62311347e-02]  and loss= 0.34020330634759505\n"
     ]
    }
   ],
   "source": [
    "# test least square\n",
    "txOpti = dataClean_without_splitting(tx_tr)\n",
    "w, loss = least_squares(y_tr, txOpti)\n",
    "# print(\"end of the least_squares with w=\",w,\" and loss=\", loss)\n",
    "print(\"end of the least_squares with loss=\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1f09a81-21c9-4587-ba44-9bb03ab90776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test mean squared error sgd\n",
    "\n",
    "initial_w = np.zeros(tx_tr.shape[1])\n",
    "max_iters = 100\n",
    "gamma = 0.00001\n",
    "\n",
    "#w, loss = mean_squared_error_sgd(y_tr, tx_tr, initial_w, max_iters, gamma)\n",
    "\n",
    "#print(\"end of the mean_squared_error_sgd with w=\",w,\" and loss=\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34c8ae3d-d431-4939-8e10-d0915a0fc194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of txOpti  (250000, 31)\n",
      "Current lambda=1e-06, the loss=0.3402033063484495\n",
      "Current lambda=1.6102620275609392e-06, the loss=0.3402033063498104\n",
      "Current lambda=2.592943797404667e-06, the loss=0.3402033063533395\n",
      "Current lambda=4.1753189365604e-06, the loss=0.34020330636248947\n",
      "Current lambda=6.723357536499335e-06, the loss=0.34020330638621415\n",
      "Current lambda=1.082636733874054e-05, the loss=0.3402033064477264\n",
      "Current lambda=1.7433288221999873e-05, the loss=0.3402033066072053\n",
      "Current lambda=2.8072162039411757e-05, the loss=0.34020330702064727\n",
      "Current lambda=4.520353656360241e-05, the loss=0.3402033080923542\n",
      "Current lambda=7.278953843983146e-05, the loss=0.3402033108698735\n",
      "Current lambda=0.00011721022975334806, the loss=0.34020331806616994\n",
      "Current lambda=0.00018873918221350977, the loss=0.3402033367022097\n",
      "Current lambda=0.0003039195382313198, the loss=0.34020338492645996\n",
      "Current lambda=0.0004893900918477494, the loss=0.3402035095628104\n",
      "Current lambda=0.0007880462815669912, the loss=0.34020383105793367\n",
      "Current lambda=0.0012689610031679222, the loss=0.3402046577741084\n",
      "Current lambda=0.0020433597178569417, the loss=0.3402067733046572\n",
      "Current lambda=0.0032903445623126675, the loss=0.3402121460157202\n",
      "Current lambda=0.005298316906283708, the loss=0.3402256344965031\n",
      "Current lambda=0.008531678524172805, the loss=0.34025892535199315\n",
      "Current lambda=0.013738237958832637, the loss=0.3403391154283818\n",
      "Current lambda=0.022122162910704502, the loss=0.3405259781489304\n",
      "Current lambda=0.03562247890262444, the loss=0.34094317849051897\n",
      "Current lambda=0.05736152510448681, the loss=0.34182742942818606\n",
      "Current lambda=0.09236708571873865, the loss=0.3435930526554792\n",
      "Current lambda=0.14873521072935117, the loss=0.3468926754417651\n",
      "Current lambda=0.2395026619987486, the loss=0.3526134456010867\n",
      "Current lambda=0.38566204211634725, the loss=0.36168969081436114\n",
      "Current lambda=0.6210169418915616, the loss=0.37466434360426026\n",
      "Current lambda=1.0, the loss=0.39120851328144873\n",
      "best lambda is  1e-06\n",
      "end of the logistic_regression with w= [-9.55378936e-02 -9.55378936e-02  2.12138310e-01 -2.35283411e-01\n",
      "  2.32427196e-02 -3.29959632e-02 -5.11788361e-02 -5.37617261e-02\n",
      " -6.53060804e-02 -1.01861744e-02 -1.81273726e-02 -6.82429084e-02\n",
      " -8.71392215e-02  6.01093885e-02  6.63343121e-02  1.39172459e-01\n",
      " -1.16018680e-03 -6.44355833e-04  4.06107966e-02  8.00284107e-04\n",
      "  9.67229621e-04 -4.31311499e-02  3.73150457e-04  3.23514383e-02\n",
      " -1.49821361e-01  5.79559875e-02 -1.99407298e-04 -1.68600893e-04\n",
      "  3.88660485e-02  5.70953663e-04 -7.54988681e-04  1.32073854e-01\n",
      " -9.55378936e-02 -7.18121740e-02  2.85660228e-02 -5.54760263e-02\n",
      "  1.06926765e-02  1.19981058e-02  1.14899367e-02 -5.18282980e-03\n",
      " -6.91303720e-03  2.93068556e-03  1.96839842e-02  3.60677969e-02\n",
      "  8.12716857e-02  1.75020030e-02 -3.44828936e-02 -3.63903035e-02\n",
      " -1.43180977e-03 -1.27670377e-02 -6.17808165e-02 -3.60432659e-04\n",
      "  2.71858619e-02 -8.48993274e-05 -2.64646417e-02 -1.36712813e-02\n",
      " -1.29636010e-02  7.10941784e-02  2.81039021e-04 -3.65054378e-03\n",
      "  3.71410822e-02 -4.04176994e-04 -4.01186816e-02]  and loss= 0.2986496238386134\n",
      "the accuracy on the train set is  0.716456\n"
     ]
    }
   ],
   "source": [
    "# test ridge regression \n",
    "\n",
    "txOpti = dataClean_without_splitting(tx_tr)\n",
    "print(\"shape of txOpti \", txOpti.shape)\n",
    "\n",
    "lambda_ = best_lambda(y_tr, txOpti, -6, 0, 30)\n",
    "degree = 2\n",
    "print(\"best lambda is \",lambda_)\n",
    "\n",
    "txOpti_poly = build_poly(txOpti, degree)\n",
    "\n",
    "w, loss = ridge_regression(y_tr, txOpti_poly, lambda_)\n",
    "\n",
    "label = predict(w, txOpti_poly)\n",
    "acc = calculate_accuracy(y_tr, label)\n",
    "\n",
    "print(\"end of the logistic_regression with w=\",w,\" and loss=\", loss)\n",
    "print(\"the accuracy on the train set is \", acc)\n",
    "\n",
    "#y_pred = predict(w, tx_te)\n",
    "#y_pred[y_pred==0] = -1\n",
    "\n",
    "#OUTPUT_PATH = 'sample-submission'\n",
    "#create_csv_submission(ids_te, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3d2b412-9501-4e1b-96d9-bbc5952a2aad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99913, 19)\n",
      "(99913,)\n",
      "(77544, 23)\n",
      "(77544,)\n",
      "(50379, 30)\n",
      "(50379,)\n",
      "(22164, 30)\n",
      "(22164,)\n",
      "shape of tx_tr  (250000, 30)\n",
      "shape of y_tr  (250000,)\n",
      "shape of txOpti  (99913, 19)\n",
      "shape of yOpti  (99913,)\n"
     ]
    }
   ],
   "source": [
    "txOpti, yOpti = dataClean(tx_tr, y_tr)\n",
    "print(\"shape of tx_tr \", tx_tr.shape)\n",
    "print(\"shape of y_tr \", y_tr.shape)\n",
    "print(\"shape of txOpti \", txOpti[0].shape)\n",
    "print(\"shape of yOpti \", yOpti[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365c9054-74dd-47e3-b825-032ff28c94a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "txs, ys = split_train(tx_tr, y_tr)\n",
    "for i in range(4):\n",
    "   print_col_info(txs[i]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31d214a2-ad56-4b10-a046-00d709d50eb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(txOpti[0].shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0c197bd6-ff00-4908-ba64-3d3dba6bfe4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000, 30)\n",
      "[     0.000    -49.023   1192.026   -999.000    105.012    406.345]\n",
      "[     1.000     49.240    690.075      0.000     46.524     35.345]\n",
      "[     2.000     81.182   1349.351      6.329     73.752     40.829]\n",
      "[     3.000     57.896   2834.999      0.000     38.468     63.656]\n",
      "[     4.000   -708.421      8.503   -999.000   -999.000    454.480]\n",
      "[     5.000   -601.237   4974.979   -999.000   -999.000    657.971]\n",
      "[     6.000   -709.357     16.690   -999.000   -999.000    453.019]\n",
      "[     7.000      2.373      5.684      0.208      2.492      0.783]\n",
      "[     8.000     18.917   2834.999      0.000     12.316     22.273]\n",
      "[     9.000    158.432   1852.462     46.104    120.665    115.706]\n",
      "[    10.000      1.438     19.773      0.047      1.280      0.845]\n",
      "[    11.000     -0.128      1.414     -1.414     -0.356      1.194]\n",
      "[    12.000   -708.985      1.000   -999.000   -999.000    453.596]\n",
      "[    13.000     38.707    764.408     20.000     31.804     22.412]\n",
      "[    14.000     -0.011      2.497     -2.499     -0.023      1.214]\n",
      "[    15.000     -0.008      3.142     -3.142     -0.033      1.817]\n",
      "[    16.000     46.660    560.271     26.000     40.516     22.065]\n",
      "[    17.000     -0.020      2.503     -2.505     -0.045      1.265]\n",
      "[    18.000      0.044      3.142     -3.142      0.086      1.817]\n",
      "[    19.000     41.717   2842.617      0.109     34.802     32.895]\n",
      "[    20.000     -0.010      3.142     -3.142     -0.024      1.812]\n",
      "[    21.000    209.797   2003.976     13.678    179.739    126.499]\n",
      "[    22.000      0.979      3.000      0.000      1.000      0.977]\n",
      "[    23.000   -348.330   1120.573   -999.000     38.960    532.962]\n",
      "[    24.000   -399.254      4.499   -999.000     -1.872    489.337]\n",
      "[    25.000   -399.260      3.141   -999.000     -2.093    489.333]\n",
      "[    26.000   -692.381    721.456   -999.000   -999.000    479.875]\n",
      "[    27.000   -709.122      4.500   -999.000   -999.000    453.384]\n",
      "[    28.000   -709.119      3.142   -999.000   -999.000    453.388]\n",
      "[    29.000     73.065   1633.433      0.000     40.513     98.015]\n"
     ]
    }
   ],
   "source": [
    "print_col_info(tx_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e3711aba-4248-4dbd-b96f-3cb663f1266e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99913, 20)\n",
      "[     0.000      1.000      1.000      1.000      1.000      0.000]\n",
      "[     1.000      0.000      3.193     -3.071     -0.059      1.000]\n",
      "[     2.000     -0.000      2.203     -2.210      0.120      1.000]\n",
      "[     3.000     -0.000      2.698     -2.620     -0.049      1.000]\n",
      "[     4.000      0.000      2.191     -1.028     -0.500      1.000]\n",
      "[     5.000      0.000      2.637     -2.660      0.144      1.000]\n",
      "[     6.000      0.000      2.191     -1.028     -0.500      1.000]\n",
      "[     7.000     -0.000      2.599     -1.881     -0.124      1.000]\n",
      "[     8.000     -0.000      2.258     -2.059     -0.034      1.000]\n",
      "[     9.000      0.000      6.081     -0.486     -0.337      1.000]\n",
      "[    10.000     -0.000      2.768     -1.248     -0.262      1.000]\n",
      "[    11.000      0.000      2.079     -2.046     -0.009      1.000]\n",
      "[    12.000      0.000      1.875     -1.842     -0.017      1.000]\n",
      "[    13.000     -0.000      2.516     -1.485     -0.166      1.000]\n",
      "[    14.000     -0.000      2.001     -1.952     -0.031      1.000]\n",
      "[    15.000      0.000      1.827     -1.894      0.019      1.000]\n",
      "[    16.000     -0.000      2.246     -2.067     -0.040      1.000]\n",
      "[    17.000     -0.000      1.883     -1.848     -0.013      1.000]\n",
      "[    18.000      0.000      2.269     -2.111     -0.040      1.000]\n",
      "[    19.000        nan        nan        nan        nan        nan]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_155/4131380513.py:4: RuntimeWarning: Mean of empty slice\n",
      "  mean_ = np.nanmean(t, axis=0)\n",
      "/tmp/ipykernel_155/4131380513.py:5: RuntimeWarning: All-NaN slice encountered\n",
      "  max_ = np.nanmax(t, axis=0)\n",
      "/tmp/ipykernel_155/4131380513.py:6: RuntimeWarning: All-NaN slice encountered\n",
      "  min_ = np.nanmin(t, axis=0)\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/lib/nanfunctions.py:1096: RuntimeWarning: All-NaN slice encountered\n",
      "  result = np.apply_along_axis(_nanmedian1d, axis, a, overwrite_input)\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/lib/nanfunctions.py:1879: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n"
     ]
    }
   ],
   "source": [
    "print_col_info(txOpti[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3345e46f-a085-4380-98bd-47747612daff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000, 31)\n",
      "[     0.000      1.000      1.000      1.000      1.000      0.000]\n",
      "[     1.000     -0.000      3.247     -3.151     -0.043      1.000]\n",
      "[     2.000     -0.000      2.024     -1.595     -0.042      1.000]\n",
      "[     3.000     -0.000      2.857     -2.802     -0.062      1.000]\n",
      "[     4.000      0.000      2.862     -1.218     -0.213      1.000]\n",
      "[     5.000     -0.000      4.245     -2.663     -0.072      1.000]\n",
      "[     6.000      0.000      5.977     -1.927     -0.161      1.000]\n",
      "[     7.000      0.000      4.701     -4.793      0.063      1.000]\n",
      "[     8.000     -0.000      2.177     -2.270      0.133      1.000]\n",
      "[     9.000     -0.000      2.625     -1.101     -0.431      1.000]\n",
      "[    10.000     -0.000      2.824     -1.323     -0.265      1.000]\n",
      "[    11.000      0.000      2.523     -2.376     -0.082      1.000]\n",
      "[    12.000      0.000      1.292     -1.077     -0.191      1.000]\n",
      "[    13.000      0.000      2.536     -2.120     -0.006      1.000]\n",
      "[    14.000     -0.000      2.958     -1.212     -0.271      1.000]\n",
      "[    15.000      0.000      2.081     -2.064     -0.008      1.000]\n",
      "[    16.000     -0.000      1.873     -1.852     -0.014      1.000]\n",
      "[    17.000     -0.000      2.876     -1.358     -0.214      1.000]\n",
      "[    18.000     -0.000      2.030     -2.008     -0.024      1.000]\n",
      "[    19.000     -0.000      1.833     -1.901      0.019      1.000]\n",
      "[    20.000      0.000      2.783     -1.936     -0.108      1.000]\n",
      "[    21.000      0.000      1.869     -1.858     -0.007      1.000]\n",
      "[    22.000      0.000      2.571     -2.192     -0.130      1.000]\n",
      "[    23.000      0.000      1.598     -1.070      0.264      1.000]\n",
      "[    24.000     -0.000      3.860     -1.476     -0.192      1.000]\n",
      "[    25.000     -0.000      2.649     -2.645      0.002      1.000]\n",
      "[    26.000     -0.000      2.423     -2.377     -0.009      1.000]\n",
      "[    27.000     -0.000      5.715     -1.949     -0.132      1.000]\n",
      "[    28.000      0.000      3.688     -3.693      0.000      1.000]\n",
      "[    29.000      0.000      3.449     -3.432     -0.000      1.000]\n",
      "[    30.000      0.000      2.841     -0.892     -0.217      1.000]\n"
     ]
    }
   ],
   "source": [
    "print_col_info(dataClean_without_splitting(tx_tr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f5c7de-0c97-4975-93cb-71f4916eb96e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preiteration, the loss=1.1116528187695085, the grad=1.2101686786807626\n",
      "Current iteration=0, the loss=0.6820644832880365, the grad=1.2101686786807626\n",
      "Current iteration=1, the loss=0.3066142986530485, the grad=1.0084808414933755\n",
      "Current iteration=2, the loss=-0.029604470798316438, the grad=0.9022029903141847\n",
      "Current iteration=3, the loss=-0.3430157651992159, the grad=0.8429430779572851\n",
      "Current iteration=4, the loss=-0.6426399526354077, the grad=0.8069356321121939\n",
      "Current iteration=5, the loss=-0.9334824700879437, the grad=0.7837460355696775\n",
      "Current iteration=6, the loss=-1.2184617607782389, the grad=0.768153758030385\n",
      "Current iteration=7, the loss=-1.4993589564021186, the grad=0.7572877596218334\n",
      "Current iteration=8, the loss=-1.7773062813172344, the grad=0.749473080028805\n",
      "Current iteration=9, the loss=-2.053050822127832, the grad=0.743692027158274\n",
      "Current iteration=10, the loss=-2.3271028598182393, the grad=0.739305993711053\n",
      "Current iteration=11, the loss=-2.5998222001307125, the grad=0.7359027122073518\n",
      "Current iteration=12, the loss=-2.871470003321026, the grad=0.7332089485985198\n",
      "Current iteration=13, the loss=-3.142240825730545, the grad=0.731039084424358\n",
      "Current iteration=14, the loss=-3.412282992801663, the grad=0.7292640583243318\n",
      "Current iteration=15, the loss=-3.6817119075898614, the grad=0.7277921639762052\n",
      "Current iteration=16, the loss=-3.9506189716217652, the grad=0.72655691230289\n",
      "Current iteration=17, the loss=-4.219077712303591, the grad=0.7255091935439982\n",
      "Current iteration=18, the loss=-4.4871480887889055, the grad=0.7246121109416708\n",
      "Current iteration=19, the loss=-4.754879582521029, the grad=0.7238375078411282\n",
      "Current iteration=20, the loss=-5.0223134590641045, the grad=0.7231635891383298\n",
      "Current iteration=21, the loss=-5.2894844531387255, the grad=0.722573263245416\n",
      "Current iteration=22, the loss=-5.5564220444273875, the grad=0.7220529669929637\n",
      "Current iteration=23, the loss=-5.82315143781813, the grad=0.7215918197788939\n",
      "Current iteration=24, the loss=-6.089694326639505, the grad=0.7211810058304194\n",
      "Current iteration=25, the loss=-6.356069494129806, the grad=0.720813316931817\n",
      "Current iteration=26, the loss=-6.622293292631126, the grad=0.7204828096580934\n",
      "Current iteration=27, the loss=-6.88838002917155, the grad=0.7201845454229034\n",
      "Current iteration=28, the loss=-7.154342278536891, the grad=0.7199143911789653\n",
      "Current iteration=29, the loss=-7.420191139571575, the grad=0.719668865066591\n",
      "Current iteration=30, the loss=-7.685936446592421, the grad=0.7194450157418039\n",
      "Current iteration=31, the loss=-7.95158694498913, the grad=0.7192403272026272\n",
      "Current iteration=32, the loss=-8.21715043801228, the grad=0.7190526431072743\n",
      "Current iteration=33, the loss=-8.482633910202278, the grad=0.7188801061285609\n",
      "Current iteration=34, the loss=-8.74804363174551, the grad=0.7187211090065282\n",
      "Current iteration=35, the loss=-9.013385247154488, the grad=0.718574254775342\n",
      "Current iteration=36, the loss=-9.278663850984628, the grad=0.7184383242393468\n",
      "Current iteration=37, the loss=-9.543884052769554, the grad=0.7183122492178289\n",
      "Current iteration=38, the loss=-9.809050032941625, the grad=0.7181950904110872\n",
      "Current iteration=39, the loss=-10.074165591177131, the grad=0.7180860189920445\n",
      "Current iteration=40, the loss=-10.339234188345932, the grad=0.7179843012191884\n",
      "Current iteration=41, the loss=-10.604258983037633, the grad=0.7178892855136063\n",
      "Current iteration=42, the loss=-10.869242863469603, the grad=0.7178003915564066\n",
      "Current iteration=43, the loss=-11.134188475447012, the grad=0.7177171010511526\n",
      "Current iteration=44, the loss=-11.39909824693544, the grad=0.7176389498650643\n",
      "Current iteration=45, the loss=-11.663974409716564, the grad=0.7175655213172148\n",
      "Current iteration=46, the loss=-11.928819018523964, the grad=0.7174964404250745\n",
      "Current iteration=47, the loss=-12.193633967994884, the grad=0.7174313689551385\n",
      "Current iteration=48, the loss=-12.458421007723539, the grad=0.7173700011508922\n",
      "Current iteration=49, the loss=-12.723181755659303, the grad=0.7173120600335192\n",
      "Current iteration=50, the loss=-12.98791771005808, the grad=0.7172572941886811\n",
      "Current iteration=51, the loss=-13.252630260165544, the grad=0.7172054749672478\n",
      "Current iteration=52, the loss=-13.517320695786209, the grad=0.7171563940397391\n",
      "Current iteration=53, the loss=-13.781990215871055, the grad=0.7171098612539815\n",
      "Current iteration=54, the loss=-14.04663993623889, the grad=0.717065702753485\n",
      "Current iteration=55, the loss=-14.311270896531168, the grad=0.7170237593206735\n",
      "Current iteration=56, the loss=-14.5758840664873, the grad=0.7169838849145771\n",
      "Current iteration=57, the loss=-14.840480351616215, the grad=0.7169459453771797\n",
      "Current iteration=58, the loss=-15.105060598330528, the grad=0.71690981728641\n",
      "Current iteration=59, the loss=-15.369625598601454, the grad=0.7168753869369696\n",
      "Current iteration=60, the loss=-15.634176094185506, the grad=0.7168425494328796\n",
      "Current iteration=61, the loss=-15.89871278046799, the grad=0.7168112078778809\n",
      "Current iteration=62, the loss=-16.163236309962933, the grad=0.7167812726517444\n",
      "Current iteration=63, the loss=-16.427747295504435, the grad=0.7167526607621786\n",
      "Current iteration=64, the loss=-16.692246313160634, the grad=0.7167252952633879\n",
      "Current iteration=65, the loss=-16.95673390489762, the grad=0.716699104733525\n",
      "Current iteration=66, the loss=-17.221210581017953, the grad=0.7166740228042826\n",
      "Current iteration=67, the loss=-17.485676822395416, the grad=0.7166499877367362\n",
      "Current iteration=68, the loss=-17.75013308252553, the grad=0.7166269420382858\n",
      "Current iteration=69, the loss=-18.014579789409137, the grad=0.716604832116193\n",
      "Current iteration=70, the loss=-18.279017347284523, the grad=0.7165836079637488\n",
      "Current iteration=71, the loss=-18.543446138222055, the grad=0.7165632228755988\n",
      "Current iteration=72, the loss=-18.80786652359375, the grad=0.7165436331891553\n",
      "Current iteration=73, the loss=-19.072278845428958, the grad=0.7165247980493895\n",
      "Current iteration=74, the loss=-19.336683427666284, the grad=0.7165066791946136\n",
      "Current iteration=75, the loss=-19.601080577310796, the grad=0.7164892407611296\n",
      "Current iteration=76, the loss=-19.865470585504674, the grad=0.7164724491048658\n",
      "Current iteration=77, the loss=-20.12985372851877, the grad=0.716456272638326\n",
      "Current iteration=78, the loss=-20.39423026867167, the grad=0.7164406816813673\n",
      "Current iteration=79, the loss=-20.65860045518236, the grad=0.7164256483244742\n",
      "Current iteration=80, the loss=-20.922964524961984, the grad=0.7164111463033501\n",
      "Current iteration=81, the loss=-21.187322703349647, the grad=0.7163971508837615\n",
      "Current iteration=82, the loss=-21.4516752047968, the grad=0.7163836387556864\n",
      "Current iteration=83, the loss=-21.71602223350424, the grad=0.7163705879359198\n",
      "Current iteration=84, the loss=-21.980363984015646, the grad=0.7163579776783673\n",
      "Current iteration=85, the loss=-22.24470064177084, the grad=0.716345788391343\n",
      "Current iteration=86, the loss=-22.50903238362203, the grad=0.7163340015612524\n",
      "Current iteration=87, the loss=-22.773359378315845, the grad=0.7163225996820977\n",
      "Current iteration=88, the loss=-23.037681786943708, the grad=0.7163115661903133\n",
      "Current iteration=89, the loss=-23.30199976336298, the grad=0.7163008854044614\n",
      "Current iteration=90, the loss=-23.566313454590983, the grad=0.7162905424693915\n",
      "Current iteration=91, the loss=-23.830623001174022, the grad=0.7162805233044792\n",
      "Current iteration=92, the loss=-24.094928537533026, the grad=0.7162708145556139\n",
      "Current iteration=93, the loss=-24.359230192287683, the grad=0.7162614035506213\n",
      "Current iteration=94, the loss=-24.623528088560537, the grad=0.7162522782578518\n",
      "Current iteration=95, the loss=-24.887822344262414, the grad=0.7162434272476718\n",
      "Current iteration=96, the loss=-25.15211307236056, the grad=0.7162348396566305\n",
      "Current iteration=97, the loss=-25.41640038113063, the grad=0.7162265051540949\n",
      "Current iteration=98, the loss=-25.680684374393728, the grad=0.7162184139111563\n",
      "Current iteration=99, the loss=-25.94496515173938, the grad=0.7162105565716326\n",
      "Preiteration, the loss=1.0542287815505347, the grad=1.0116634446104686\n",
      "Current iteration=0, the loss=0.7654166000639903, the grad=1.0116634446104686\n",
      "Current iteration=1, the loss=0.5220008643498985, the grad=0.831542829863052\n",
      "Current iteration=2, the loss=0.3086086390013071, the grad=0.7378047777261539\n",
      "Current iteration=3, the loss=0.11307660365499542, the grad=0.6825487135015733\n",
      "Current iteration=4, the loss=-0.07106772346247545, the grad=0.6465059542627551\n",
      "Current iteration=5, the loss=-0.24750165366348548, the grad=0.6215913434490186\n",
      "Current iteration=6, the loss=-0.41846574287666766, the grad=0.6036861234733145\n",
      "Current iteration=7, the loss=-0.5854009411423415, the grad=0.5904252003045245\n",
      "Current iteration=8, the loss=-0.7492748237752418, the grad=0.5803535236482165\n",
      "Current iteration=9, the loss=-0.9107610652117815, the grad=0.5725355866358462\n",
      "Current iteration=10, the loss=-1.0703436271822298, the grad=0.5663500945064038\n",
      "Current iteration=11, the loss=-1.2283798363277554, the grad=0.5613732683253599\n",
      "Current iteration=12, the loss=-1.3851399324414335, the grad=0.5573091891215444\n",
      "Current iteration=13, the loss=-1.540832636844426, the grad=0.5539467758833846\n",
      "Current iteration=14, the loss=-1.695622147815567, the grad=0.5511324950291016\n",
      "Current iteration=15, the loss=-1.8496397290688433, the grad=0.548752649005566\n",
      "Current iteration=16, the loss=-2.0029917992839517, the grad=0.5467216416111401\n",
      "Current iteration=17, the loss=-2.155765702644259, the grad=0.5449740522326745\n",
      "Current iteration=18, the loss=-2.3080339076954366, the grad=0.5434591849785716\n",
      "Current iteration=19, the loss=-2.4598571185246736, the grad=0.5421372554333556\n",
      "Current iteration=20, the loss=-2.6112866184060235, the grad=0.5409766798716814\n",
      "Current iteration=21, the loss=-2.7623660619375126, the grad=0.5399521189291334\n",
      "Current iteration=22, the loss=-2.9131328642046754, the grad=0.5390430456809016\n",
      "Current iteration=23, the loss=-3.0636192909213675, the grad=0.5382326836418841\n",
      "Current iteration=24, the loss=-3.2138533235120756, the grad=0.5375072093728152\n",
      "Current iteration=25, the loss=-3.3638593525837486, the grad=0.5368551468605226\n",
      "Current iteration=26, the loss=-3.513658738969797, the grad=0.5362669026130268\n",
      "Current iteration=27, the loss=-3.6632702714575243, the grad=0.5357344052079096\n",
      "Current iteration=28, the loss=-3.812710543097273, the grad=0.535250823224143\n",
      "Current iteration=29, the loss=-3.9619942627558298, the grad=0.5348103425966616\n",
      "Current iteration=30, the loss=-4.11113451472806, the grad=0.5344079894519441\n",
      "Current iteration=31, the loss=-4.26014297635845, the grad=0.5340394880671714\n",
      "Current iteration=32, the loss=-4.409030101471841, the grad=0.5337011461832263\n",
      "Current iteration=33, the loss=-4.557805275777687, the grad=0.5333897617893953\n",
      "Current iteration=34, the loss=-4.706476949158151, the grad=0.5331025468880377\n",
      "Current iteration=35, the loss=-4.855052748780021, the grad=0.5328370647812068\n",
      "Current iteration=36, the loss=-5.0035395762133374, the grad=0.5325911781964922\n",
      "Current iteration=37, the loss=-5.1519436911443615, the grad=0.5323630061556548\n",
      "Current iteration=38, the loss=-5.3002707837989025, the grad=0.5321508879365436\n",
      "Current iteration=39, the loss=-5.448526037816134, the grad=0.5319533528219385\n",
      "Current iteration=40, the loss=-5.596714185011213, the grad=0.5317690945943939\n",
      "Current iteration=41, the loss=-5.744839553221387, the grad=0.5315969499427822\n",
      "Current iteration=42, the loss=-5.892906108232585, the grad=0.5314358801081465\n",
      "Current iteration=43, the loss=-6.0409174906219425, the grad=0.5312849552241088\n",
      "Current iteration=44, the loss=-6.188877048219352, the grad=0.5311433409082762\n",
      "Current iteration=45, the loss=-6.336787864782012, the grad=0.5310102867417555\n",
      "Current iteration=46, the loss=-6.484652785385568, the grad=0.5308851163385472\n",
      "Current iteration=47, the loss=-6.632474438960299, the grad=0.5307672187586528\n",
      "Current iteration=48, the loss=-6.780255258338223, the grad=0.5306560410608768\n",
      "Current iteration=49, the loss=-6.927997498124311, the grad=0.5305510818255487\n",
      "Current iteration=50, the loss=-7.075703250660963, the grad=0.5304518855053701\n",
      "Current iteration=51, the loss=-7.223374460317503, the grad=0.5303580374855164\n",
      "Current iteration=52, the loss=-7.371012936304985, the grad=0.5302691597530027\n",
      "Current iteration=53, the loss=-7.518620364189746, the grad=0.5301849070909205\n",
      "Current iteration=54, the loss=-7.666198316256264, the grad=0.5301049637260888\n",
      "Current iteration=55, the loss=-7.813748260850481, the grad=0.5300290403694313\n",
      "Current iteration=56, the loss=-7.96127157081793, the grad=0.5299568715973805\n",
      "Current iteration=57, the loss=-8.108769531136694, the grad=0.5298882135301495\n",
      "Current iteration=58, the loss=-8.256243345832829, the grad=0.5298228417690399\n",
      "Current iteration=59, the loss=-8.40369414425525, the grad=0.529760549560291\n",
      "Current iteration=60, the loss=-8.551122986777782, the grad=0.5297011461575009\n",
      "Current iteration=61, the loss=-8.698530869988144, the grad=0.5296444553584593\n",
      "Current iteration=62, the loss=-8.845918731416528, the grad=0.5295903141955066\n",
      "Current iteration=63, the loss=-8.993287453850547, the grad=0.529538571761288\n",
      "Current iteration=64, the loss=-9.14063786927787, the grad=0.5294890881541557\n",
      "Current iteration=65, the loss=-9.287970762493314, the grad=0.5294417335295001\n",
      "Current iteration=66, the loss=-9.435286874403076, the grad=0.5293963872450299\n",
      "Current iteration=67, the loss=-9.582586905055223, the grad=0.5293529370895297\n",
      "Current iteration=68, the loss=-9.729871516422431, the grad=0.5293112785859099\n",
      "Current iteration=69, the loss=-9.877141334960164, the grad=0.5292713143604965\n",
      "Current iteration=70, the loss=-10.024396953961137, the grad=0.5292329535714615\n",
      "Current iteration=71, the loss=-10.171638935724607, the grad=0.5291961113901521\n",
      "Current iteration=72, the loss=-10.318867813557265, the grad=0.5291607085297974\n",
      "Current iteration=73, the loss=-10.46608409362074, the grad=0.5291266708167213\n",
      "Current iteration=74, the loss=-10.613288256639217, the grad=0.5290939287997319\n",
      "Current iteration=75, the loss=-10.760480759479389, the grad=0.5290624173938651\n",
      "Current iteration=76, the loss=-10.907662036613662, the grad=0.5290320755550671\n",
      "Current iteration=77, the loss=-11.05483250147663, the grad=0.5290028459827902\n",
      "Current iteration=78, the loss=-11.201992547723743, the grad=0.5289746748478006\n",
      "Current iteration=79, the loss=-11.349142550400295, the grad=0.5289475115427839\n",
      "Current iteration=80, the loss=-11.496282867028118, the grad=0.528921308453596\n",
      "Current iteration=81, the loss=-11.643413838616684, the grad=0.5288960207492331\n",
      "Current iteration=82, the loss=-11.790535790604675, the grad=0.5288716061887847\n",
      "Current iteration=83, the loss=-11.937649033737527, the grad=0.5288480249438279\n",
      "Current iteration=84, the loss=-12.084753864885998, the grad=0.528825239434863\n",
      "Current iteration=85, the loss=-12.231850567810387, the grad=0.5288032141805384\n",
      "Current iteration=86, the loss=-12.37893941387448, the grad=0.5287819156585354\n",
      "Current iteration=87, the loss=-12.526020662713185, the grad=0.5287613121770947\n",
      "Current iteration=88, the loss=-12.673094562857205, the grad=0.5287413737562657\n",
      "Current iteration=89, the loss=-12.820161352318035, the grad=0.528722072018045\n",
      "Current iteration=90, the loss=-12.967221259136215, the grad=0.5287033800846531\n",
      "Current iteration=91, the loss=-13.114274501895432, the grad=0.52868527248427\n",
      "Current iteration=92, the loss=-13.261321290205045, the grad=0.5286677250636088\n",
      "Current iteration=93, the loss=-13.408361825153193, the grad=0.5286507149067663\n",
      "Current iteration=94, the loss=-13.555396299732642, the grad=0.5286342202598452\n",
      "Current iteration=95, the loss=-13.702424899241235, the grad=0.5286182204608773\n",
      "Current iteration=96, the loss=-13.84944780165869, the grad=0.5286026958746317\n",
      "Current iteration=97, the loss=-13.996465178001444, the grad=0.52858762783192\n",
      "Current iteration=98, the loss=-14.143477192656936, the grad=0.5285729985730483\n",
      "Current iteration=99, the loss=-14.290484003698792, the grad=0.5285587911950935\n",
      "Preiteration, the loss=0.9680043635218494, the grad=0.9719781076452336\n",
      "Current iteration=0, the loss=0.6961193239323353, the grad=0.9719781076452336\n",
      "Current iteration=1, the loss=0.5070165323105272, the grad=0.7165129430973985\n",
      "Current iteration=2, the loss=0.35355823069536535, the grad=0.6110922113394349\n",
      "Current iteration=3, the loss=0.21860547693539897, the grad=0.5553273350740426\n",
      "Current iteration=4, the loss=0.094927411493419, the grad=0.5205781301288136\n",
      "Current iteration=5, the loss=-0.02118393214871284, the grad=0.4967959676856103\n",
      "Current iteration=6, the loss=-0.13188598115445313, the grad=0.47952169357010166\n",
      "Current iteration=7, the loss=-0.23854615770828186, the grad=0.46644990353924476\n",
      "Current iteration=8, the loss=-0.34208587521000516, the grad=0.4562569740159923\n",
      "Current iteration=9, the loss=-0.4431550617588907, the grad=0.4481233774620044\n",
      "Current iteration=10, the loss=-0.5422286562213176, the grad=0.44151230024801036\n",
      "Current iteration=11, the loss=-0.6396636356786657, the grad=0.43605655939597776\n"
     ]
    }
   ],
   "source": [
    "# test logistic regression\n",
    "#initial_w = np.zeros(txOpti.shape[1])\n",
    "# initial_w = w\n",
    "max_iters = 100\n",
    "gamma = 0.5\n",
    "ws = []\n",
    "losses = []\n",
    "y_pred = []\n",
    "accArray = []\n",
    "\n",
    "for i in range(4): \n",
    "    initial_w = np.zeros(txOpti[i].shape[1])\n",
    "    w, loss = logistic_regression(yOpti[i], txOpti[i], initial_w, max_iters, gamma)\n",
    "    label = predict(w, txOpti[i])\n",
    "    acc = calculate_accuracy(yOpti[i], label)\n",
    "    \n",
    "    ws.append(w)\n",
    "    losses.append(loss)\n",
    "    y_pred.append(label)\n",
    "    accArray.append(acc)\n",
    "\n",
    "tot_acc = accArray[0] + accArray[1] + accArray[2] + accArray[3]\n",
    "weights = np.array(list(ws[0])+list(ws[1])+list(ws[2])+list(ws[3]))\n",
    "loss = np.mean(losses)\n",
    "    \n",
    "#teOpti = dataClean(tx_te)\n",
    "#print(\"shape of teOpti \", teOpti.shape)\n",
    "\n",
    "#y_pred = predict(w, teOpti)\n",
    "\n",
    "#OUTPUT_PATH = 'sample-submission LR 2'\n",
    "#create_csv_submission(ids_te, y_pred, OUTPUT_PATH)\n",
    "\n",
    "print(\"end of the logistic_regression with w=\",weights,\" and loss=\", loss)\n",
    "print(\"the accurcy on the train set is \", tot_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e1c71eed-4b5e-41af-b0b6-57d490347155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preiteration, the loss=1.1116528187695085, the grad=203.77505807296845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ml-project-1-ahl/helper_functions.py:62: RuntimeWarning: divide by zero encountered in log\n",
      "  return -np.sum(y * np.log(sigmoid(v)) + (1 - y) * np.log(1-sigmoid(v)))/N\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, the loss=nan, the grad=203.77505807296845\n",
      "Current iteration=1, the loss=nan, the grad=99.47051114151297\n",
      "Current iteration=2, the loss=nan, the grad=99.47051114151297\n",
      "Current iteration=3, the loss=nan, the grad=99.47051114151297\n",
      "Current iteration=4, the loss=nan, the grad=99.47051114151297\n",
      "Current iteration=5, the loss=nan, the grad=99.47051114151297\n",
      "Current iteration=6, the loss=nan, the grad=99.47051114151297\n",
      "Current iteration=7, the loss=nan, the grad=99.47051114151297\n",
      "Current iteration=8, the loss=nan, the grad=99.47051114151297\n",
      "Current iteration=9, the loss=nan, the grad=99.47051114151297\n",
      "Current iteration=10, the loss=nan, the grad=99.47051114151297\n",
      "Current iteration=11, the loss=nan, the grad=99.47051114151297\n",
      "Current iteration=12, the loss=nan, the grad=99.47051114151297\n",
      "Current iteration=13, the loss=nan, the grad=99.47051114151297\n",
      "Current iteration=14, the loss=nan, the grad=99.47051114151297\n",
      "Current iteration=15, the loss=nan, the grad=99.47051114151297\n",
      "Current iteration=16, the loss=nan, the grad=99.47051114151297\n",
      "Current iteration=17, the loss=nan, the grad=99.47051114151297\n",
      "Current iteration=18, the loss=nan, the grad=99.47051114151297\n",
      "Current iteration=19, the loss=nan, the grad=99.47051114151297\n",
      "Current iteration=20, the loss=nan, the grad=99.47051114151297\n",
      "Current iteration=21, the loss=nan, the grad=99.47051114151297\n",
      "Current iteration=22, the loss=nan, the grad=99.47051114151297\n",
      "Current iteration=23, the loss=nan, the grad=99.47051114151297\n",
      "Current iteration=24, the loss=nan, the grad=99.47051114151297\n",
      "Current iteration=25, the loss=nan, the grad=99.47051114151297\n",
      "Current iteration=26, the loss=nan, the grad=99.47051114151297\n",
      "Current iteration=27, the loss=nan, the grad=99.47051114151297\n",
      "Current iteration=28, the loss=nan, the grad=99.47051114151297\n",
      "Current iteration=29, the loss=nan, the grad=99.47051114151297\n",
      "Current iteration=30, the loss=nan, the grad=99.47051114151297\n",
      "Current iteration=31, the loss=nan, the grad=99.47051114151297\n",
      "Current iteration=32, the loss=nan, the grad=99.47051114151297\n",
      "Current iteration=33, the loss=nan, the grad=99.47051114151297\n",
      "Current iteration=34, the loss=nan, the grad=99.47051114151297\n",
      "Current iteration=35, the loss=nan, the grad=99.47051114151297\n",
      "Current iteration=36, the loss=nan, the grad=99.47051114151297\n",
      "Current iteration=37, the loss=nan, the grad=99.47051114151297\n",
      "Current iteration=38, the loss=nan, the grad=99.47051114151297\n",
      "Current iteration=39, the loss=nan, the grad=99.47051114151297\n",
      "Current iteration=40, the loss=nan, the grad=99.47051114151297\n",
      "Current iteration=41, the loss=nan, the grad=99.47051114151297\n",
      "Current iteration=42, the loss=nan, the grad=99.47051114151297\n",
      "Current iteration=43, the loss=nan, the grad=99.47051114151297\n",
      "Current iteration=44, the loss=nan, the grad=99.47051114151297\n",
      "Current iteration=45, the loss=nan, the grad=99.47051114151297\n",
      "Current iteration=46, the loss=nan, the grad=99.47051114151297\n",
      "Current iteration=47, the loss=nan, the grad=99.47051114151297\n",
      "Current iteration=48, the loss=nan, the grad=99.47051114151297\n",
      "Current iteration=49, the loss=nan, the grad=99.47051114151297\n",
      "Current iteration=50, the loss=nan, the grad=99.47051114151297\n",
      "Current iteration=51, the loss=nan, the grad=99.47051114151297\n",
      "Current iteration=52, the loss=nan, the grad=99.47051114151297\n",
      "Current iteration=53, the loss=nan, the grad=99.47051114151297\n",
      "Current iteration=54, the loss=nan, the grad=99.47051114151297\n",
      "Current iteration=55, the loss=nan, the grad=99.47051114151297\n",
      "Current iteration=56, the loss=nan, the grad=99.47051114151297\n",
      "Current iteration=57, the loss=nan, the grad=99.47051114151297\n",
      "Current iteration=58, the loss=nan, the grad=99.47051114151297\n",
      "Current iteration=59, the loss=nan, the grad=99.47051114151297\n",
      "Current iteration=60, the loss=nan, the grad=99.47051114151297\n",
      "Current iteration=61, the loss=nan, the grad=99.47051114151297\n",
      "Current iteration=62, the loss=nan, the grad=99.47051114151297\n",
      "Current iteration=63, the loss=nan, the grad=99.47051114151297\n",
      "Current iteration=64, the loss=nan, the grad=99.47051114151297\n",
      "Current iteration=65, the loss=nan, the grad=99.47051114151297\n",
      "Current iteration=66, the loss=nan, the grad=99.47051114151297\n",
      "Current iteration=67, the loss=nan, the grad=99.47051114151297\n",
      "Current iteration=68, the loss=nan, the grad=99.47051114151297\n",
      "Current iteration=69, the loss=nan, the grad=99.47051114151297\n",
      "Current iteration=70, the loss=nan, the grad=99.47051114151297\n",
      "Current iteration=71, the loss=nan, the grad=99.47051114151297\n",
      "Current iteration=72, the loss=nan, the grad=99.47051114151297\n",
      "Current iteration=73, the loss=nan, the grad=99.47051114151297\n",
      "Current iteration=74, the loss=nan, the grad=99.47051114151297\n",
      "Current iteration=75, the loss=nan, the grad=99.47051114151297\n",
      "Current iteration=76, the loss=nan, the grad=99.47051114151297\n",
      "Current iteration=77, the loss=nan, the grad=99.47051114151297\n",
      "Current iteration=78, the loss=nan, the grad=99.47051114151297\n",
      "Current iteration=79, the loss=nan, the grad=99.47051114151297\n",
      "Current iteration=80, the loss=nan, the grad=99.47051114151297\n",
      "Current iteration=81, the loss=nan, the grad=99.47051114151297\n",
      "Current iteration=82, the loss=nan, the grad=99.47051114151297\n",
      "Current iteration=83, the loss=nan, the grad=99.47051114151297\n",
      "Current iteration=84, the loss=nan, the grad=99.47051114151297\n",
      "Current iteration=85, the loss=nan, the grad=99.47051114151297\n",
      "Current iteration=86, the loss=nan, the grad=99.47051114151297\n",
      "Current iteration=87, the loss=nan, the grad=99.47051114151297\n",
      "Current iteration=88, the loss=nan, the grad=99.47051114151297\n",
      "Current iteration=89, the loss=nan, the grad=99.47051114151297\n",
      "Current iteration=90, the loss=nan, the grad=99.47051114151297\n",
      "Current iteration=91, the loss=nan, the grad=99.47051114151297\n",
      "Current iteration=92, the loss=nan, the grad=99.47051114151297\n",
      "Current iteration=93, the loss=nan, the grad=99.47051114151297\n",
      "Current iteration=94, the loss=nan, the grad=99.47051114151297\n",
      "Current iteration=95, the loss=nan, the grad=99.47051114151297\n",
      "Current iteration=96, the loss=nan, the grad=99.47051114151297\n",
      "Current iteration=97, the loss=nan, the grad=99.47051114151297\n",
      "Current iteration=98, the loss=nan, the grad=99.47051114151297\n",
      "Current iteration=99, the loss=nan, the grad=99.47051114151297\n",
      "end of the logistic_regression with w= [   -24.736  -2550.517  -1865.082  -1647.582   -250.764    -66.455\n",
      "   -250.764  -1645.634    -37.755     31.610   -636.794      1.815\n",
      "      1.292  -1001.166      4.334     -3.130   -925.830      2.009\n",
      "  -2729.757      0.000]  and loss= nan\n",
      "the accuracy on the train set is  0.7448580264830402\n"
     ]
    }
   ],
   "source": [
    "# test logistic regression without splitting \n",
    "# txOpti = dataClean_without_splitting(tx_tr)\n",
    "txOpti = txOpti[0]\n",
    "y_tr = yOpti[0]\n",
    "# print(\"shape of txOpti \", txOpti.shape)\n",
    "# print(\"shape of y \", y_tr.shape)\n",
    "initial_w = np.zeros(txOpti.shape[1])\n",
    "# initial_w = w\n",
    "max_iters = 100\n",
    "gamma = 0.5\n",
    "\n",
    "w, loss = logistic_regression(y_tr, txOpti, initial_w, max_iters, gamma)\n",
    "\n",
    "label = predict_logistic(w, txOpti)\n",
    "acc = calculate_accuracy(y_tr, label)\n",
    "\n",
    "print(\"end of the logistic_regression with w=\",w,\" and loss=\", loss)\n",
    "print(\"the accuracy on the train set is \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb24421-5cdf-4b2b-8000-6a6fd916d419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preiteration, the loss=1.1116528187695085, the grad=203.77505807296845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ml-project-1-ahl/helper_functions.py:62: RuntimeWarning: divide by zero encountered in log\n",
      "  return -np.sum(y * np.log(sigmoid(v)) + (1 - y) * np.log(1-sigmoid(v)))/N\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, the loss=nan, the grad=203.77505807296845\n",
      "Current iteration=1, the loss=nan, the grad=99.47051114151297\n",
      "Current iteration=2, the loss=nan, the grad=99.47051114151297\n",
      "Current iteration=3, the loss=nan, the grad=99.47051114151297\n",
      "Current iteration=4, the loss=nan, the grad=99.47051114151297\n",
      "Current iteration=5, the loss=nan, the grad=99.47051114151297\n",
      "Current iteration=6, the loss=nan, the grad=99.47051114151297\n",
      "Current iteration=7, the loss=nan, the grad=99.47051114151297\n",
      "Current iteration=8, the loss=nan, the grad=99.47051114151297\n",
      "Current iteration=9, the loss=nan, the grad=99.47051114151297\n",
      "Current iteration=10, the loss=nan, the grad=99.47051114151297\n",
      "Current iteration=11, the loss=nan, the grad=99.47051114151297\n",
      "Current iteration=12, the loss=nan, the grad=99.47051114151297\n",
      "Current iteration=13, the loss=nan, the grad=99.47051114151297\n",
      "Current iteration=14, the loss=nan, the grad=99.47051114151297\n",
      "Current iteration=15, the loss=nan, the grad=99.47051114151297\n",
      "Current iteration=16, the loss=nan, the grad=99.47051114151297\n",
      "Current iteration=17, the loss=nan, the grad=99.47051114151297\n",
      "Current iteration=18, the loss=nan, the grad=99.47051114151297\n",
      "Current iteration=19, the loss=nan, the grad=99.47051114151297\n",
      "Current iteration=20, the loss=nan, the grad=99.47051114151297\n",
      "Current iteration=21, the loss=nan, the grad=99.47051114151297\n",
      "Current iteration=22, the loss=nan, the grad=99.47051114151297\n",
      "Current iteration=23, the loss=nan, the grad=99.47051114151297\n",
      "Current iteration=24, the loss=nan, the grad=99.47051114151297\n",
      "Current iteration=25, the loss=nan, the grad=99.47051114151297\n",
      "Current iteration=26, the loss=nan, the grad=99.47051114151297\n",
      "Current iteration=27, the loss=nan, the grad=99.47051114151297\n",
      "Current iteration=28, the loss=nan, the grad=99.47051114151297\n",
      "Current iteration=29, the loss=nan, the grad=99.47051114151297\n",
      "Current iteration=30, the loss=nan, the grad=99.47051114151297\n",
      "Current iteration=31, the loss=nan, the grad=99.47051114151297\n",
      "Current iteration=32, the loss=nan, the grad=99.47051114151297\n",
      "Current iteration=33, the loss=nan, the grad=99.47051114151297\n",
      "Current iteration=34, the loss=nan, the grad=99.47051114151297\n"
     ]
    }
   ],
   "source": [
    "# test logistic regression without splitting \n",
    "# txOpti = dataClean_without_splitting(tx_tr)\n",
    "txOpti = txOpti[0]\n",
    "y_tr = yOpti[0]\n",
    "# print(\"shape of txOpti \", txOpti.shape)\n",
    "# print(\"shape of y \", y_tr.shape)\n",
    "initial_w = np.zeros(txOpti.shape[1])\n",
    "# initial_w = w\n",
    "max_iters = 100\n",
    "gamma = 0.5\n",
    "\n",
    "w, loss = logistic_regression(y_tr, txOpti, initial_w, max_iters, gamma)\n",
    "\n",
    "label = predict_logistic(w, txOpti)\n",
    "acc = calculate_accuracy(y_tr, label)\n",
    "\n",
    "print(\"end of the logistic_regression with w=\",w,\" and loss=\", loss)\n",
    "print(\"the accuracy on the train set is \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "273f5528-0c1a-40cc-b818-c2c1cb6dfe49",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = 'logreg_nosplit_072'\n",
    "create_csv_submission(ids_te, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0b54129f-2e9e-489f-989e-cffe093f018d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[105.012 46.524 73.752 38.468 -999.000 -999.000 -999.000 2.492 12.316\n",
      " 120.665 1.280 -0.356 -999.000 31.804 -0.023 -0.033 40.516 -0.045 0.086\n",
      " 34.802 -0.024 179.739 1.000 38.960 -1.872 -2.093 -999.000 -999.000\n",
      " -999.000 40.513]\n",
      "[-49.023 49.240 81.182 57.896 -708.421 -601.237 -709.357 2.373 18.917\n",
      " 158.432 1.438 -0.128 -708.985 38.707 -0.011 -0.008 46.660 -0.020 0.044\n",
      " 41.717 -0.010 209.797 0.979 -348.330 -399.254 -399.260 -692.381 -709.122\n",
      " -709.119 73.065]\n",
      "(array([ 4,  5,  6, 12, 26, 27, 28]),)\n",
      "index : 4\n",
      "177457\n",
      "index : 5\n",
      "177457\n",
      "index : 6\n",
      "177457\n",
      "index : 12\n",
      "177457\n",
      "index : 26\n",
      "177457\n",
      "index : 27\n",
      "177457\n",
      "index : 28\n",
      "177457\n",
      "counts of each column  [177457, 177457, 177457, 177457, 177457, 177457, 177457]\n"
     ]
    }
   ],
   "source": [
    "#clean column , remove column that has only -999 elements. \n",
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.3f}\".format(x)})\n",
    "\n",
    "median = np.median(tx_tr, axis=0) \n",
    "mean = np.mean(tx_tr, axis=0) \n",
    "\n",
    "print(median)\n",
    "print(mean)\n",
    "\n",
    "indx = np.where(median == -999)\n",
    "print(indx)\n",
    "\n",
    "counts = []\n",
    "count = 0\n",
    "\n",
    "for i in indx[0]: \n",
    "    count = np.count_nonzero( tx_tr[:,i] == -999)\n",
    "    print(\"index :\", i)\n",
    "    print(count)\n",
    "    counts.append(count)\n",
    "\n",
    "print(\"counts of each column \", counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cd948ae1-5376-46f3-b929-6d5d233f0bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7,)\n",
      "(250000, 30)\n",
      "(250000, 23)\n"
     ]
    }
   ],
   "source": [
    "median = np.median(tx_tr, axis=0) \n",
    "indx = np.where(median == -999)\n",
    "tx1 = np.delete(tx_tr, indx[0], 1)\n",
    "print(indx[0].shape)\n",
    "print(tx_tr.shape)\n",
    "print(tx1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f062a94a-ff3d-4d4d-88b2-66b0ca8bf65c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of txOpti  (250000, 31)\n",
      "shape of y  (250000,)\n",
      "Current iteration=0, the loss=0.6973502970447647, the grad=1.1243646744140259\n",
      "Current iteration=1, the loss=0.616856534188257, the grad=0.35775207387661284\n",
      "Current iteration=2, the loss=0.5787623124787262, the grad=0.13476825801025497\n",
      "Current iteration=3, the loss=0.564397663619976, the grad=0.05411016582063273\n",
      "Current iteration=4, the loss=0.5585559036642221, the grad=0.021992177977755455\n",
      "Current iteration=5, the loss=0.55619834459064, the grad=0.008982988395528583\n",
      "Current iteration=6, the loss=0.5552395145275486, the grad=0.0036806184561833937\n",
      "Current iteration=7, the loss=0.5548487730060062, the grad=0.0015121462884724345\n",
      "Current iteration=8, the loss=0.5546891818181314, the grad=0.000623030677537163\n",
      "Current iteration=9, the loss=0.5546238925676787, the grad=0.00025755098978636376\n",
      "Current iteration=10, the loss=0.5545971424561065, the grad=0.00010688905965023133\n",
      "Current iteration=11, the loss=0.5545861670841631, the grad=4.4571897484197776e-05\n",
      "Current iteration=12, the loss=0.5545816577035999, the grad=1.8691225805750818e-05\n",
      "Current iteration=13, the loss=0.5545798023246145, the grad=7.890152757992928e-06\n",
      "Current iteration=14, the loss=0.5545790377902131, the grad=3.356070640053841e-06\n",
      "Current iteration=15, the loss=0.554578722250707, the grad=1.4396996292875658e-06\n",
      "Current iteration=16, the loss=0.5545785917966201, the grad=6.233537279741305e-07\n",
      "Current iteration=17, the loss=0.5545785377617962, the grad=2.725441310341249e-07\n",
      "Current iteration=18, the loss=0.5545785153345633, the grad=1.203536063264773e-07\n",
      "Current iteration=19, the loss=0.5545785060053415, the grad=5.3670299655409156e-08\n",
      "Current iteration=20, the loss=0.5545785021151414, the grad=2.4157769267819357e-08\n",
      "Current iteration=21, the loss=0.5545785004886542, the grad=1.0967650356241122e-08\n",
      "Current iteration=22, the loss=0.5545784998066579, the grad=5.017872207527512e-09\n",
      "Current iteration=23, the loss=0.5545784995197981, the grad=2.3113136285105184e-09\n",
      "Current iteration=24, the loss=0.5545784993987326, the grad=1.0708245219602705e-09\n",
      "Current iteration=25, the loss=0.5545784993474535, the grad=4.985537825846342e-10\n",
      "Current iteration=26, the loss=0.5545784993256495, the grad=2.3307493971784656e-10\n",
      "Current iteration=27, the loss=0.5545784993163405, the grad=1.0933847918207444e-10\n",
      "Current iteration=28, the loss=0.5545784993123488, the grad=5.143947639828713e-11\n",
      "Current iteration=29, the loss=0.5545784993106297, the grad=2.4258122293004844e-11\n",
      "Current iteration=30, the loss=0.5545784993098859, the grad=1.1462958467462051e-11\n",
      "Current iteration=31, the loss=0.5545784993095623, the grad=5.425974468989549e-12\n",
      "Current iteration=32, the loss=0.5545784993094212, the grad=2.572038257655237e-12\n",
      "Current iteration=33, the loss=0.554578499309359, the grad=1.220857343432909e-12\n",
      "Current iteration=34, the loss=0.5545784993093317, the grad=5.802092325566183e-13\n",
      "Current iteration=35, the loss=0.5545784993093194, the grad=2.7593254805362506e-13\n",
      "Current iteration=36, the loss=0.5545784993093137, the grad=1.3136354029050334e-13\n",
      "Current iteration=37, the loss=0.5545784993093115, the grad=6.25485407412349e-14\n",
      "Current iteration=38, the loss=0.5545784993093104, the grad=2.98260769380818e-14\n",
      "Current iteration=39, the loss=0.5545784993093101, the grad=1.4260744741849016e-14\n",
      "Current iteration=40, the loss=0.5545784993093099, the grad=6.810994468538426e-15\n",
      "Current iteration=41, the loss=0.5545784993093098, the grad=3.242635713813682e-15\n",
      "Current iteration=42, the loss=0.5545784993093098, the grad=1.564202869746153e-15\n",
      "Current iteration=43, the loss=0.5545784993093098, the grad=7.589364811512361e-16\n",
      "Current iteration=44, the loss=0.5545784993093098, the grad=3.7513400800189536e-16\n",
      "Current iteration=45, the loss=0.5545784993093098, the grad=2.8277301325301986e-16\n",
      "Current iteration=46, the loss=0.5545784993093098, the grad=9.418921512876441e-17\n",
      "Current iteration=47, the loss=0.5545784993093097, the grad=1.3091157970780883e-16\n",
      "Current iteration=48, the loss=0.5545784993093096, the grad=1.4040643095373702e-16\n",
      "Current iteration=49, the loss=0.5545784993093096, the grad=1.388334940675765e-16\n",
      "Current iteration=50, the loss=0.5545784993093096, the grad=1.3883501540759582e-16\n",
      "Current iteration=51, the loss=0.5545784993093096, the grad=1.3443052021480764e-16\n",
      "Current iteration=52, the loss=0.5545784993093096, the grad=1.3442290207904782e-16\n",
      "Current iteration=53, the loss=0.5545784993093096, the grad=1.3441985194010038e-16\n",
      "Current iteration=54, the loss=0.5545784993093096, the grad=1.3441959701325958e-16\n",
      "Current iteration=55, the loss=0.5545784993093096, the grad=1.3441905992875715e-16\n",
      "Current iteration=56, the loss=0.5545784993093096, the grad=1.3441956366187672e-16\n",
      "Current iteration=57, the loss=0.5545784993093096, the grad=1.3441955978684954e-16\n",
      "Current iteration=58, the loss=0.5545784993093096, the grad=1.3441955881809273e-16\n",
      "Current iteration=59, the loss=0.5545784993093096, the grad=1.3441955859124885e-16\n",
      "Current iteration=60, the loss=0.5545784993093096, the grad=1.3441955851919254e-16\n",
      "Current iteration=61, the loss=0.5545784993093096, the grad=1.3441955850584879e-16\n",
      "Current iteration=62, the loss=0.5545784993093096, the grad=1.3441955849784254e-16\n",
      "Current iteration=63, the loss=0.5545784993093096, the grad=1.344195584951738e-16\n",
      "Current iteration=64, the loss=0.5545784993093096, the grad=1.344195584951738e-16\n",
      "Current iteration=65, the loss=0.5545784993093096, the grad=1.344195584951738e-16\n",
      "Current iteration=66, the loss=0.5545784993093096, the grad=1.344195584951738e-16\n",
      "Current iteration=67, the loss=0.5545784993093096, the grad=1.344195584951738e-16\n",
      "Current iteration=68, the loss=0.5545784993093096, the grad=1.344195584951738e-16\n",
      "Current iteration=69, the loss=0.5545784993093096, the grad=1.344195584951738e-16\n",
      "Current iteration=70, the loss=0.5545784993093096, the grad=1.344195584951738e-16\n",
      "Current iteration=71, the loss=0.5545784993093096, the grad=1.344195584951738e-16\n",
      "Current iteration=72, the loss=0.5545784993093096, the grad=1.344195584951738e-16\n",
      "Current iteration=73, the loss=0.5545784993093096, the grad=1.344195584951738e-16\n",
      "Current iteration=74, the loss=0.5545784993093096, the grad=1.344195584951738e-16\n",
      "Current iteration=75, the loss=0.5545784993093096, the grad=1.344195584951738e-16\n",
      "Current iteration=76, the loss=0.5545784993093096, the grad=1.344195584951738e-16\n",
      "Current iteration=77, the loss=0.5545784993093096, the grad=1.344195584951738e-16\n",
      "Current iteration=78, the loss=0.5545784993093096, the grad=1.344195584951738e-16\n",
      "Current iteration=79, the loss=0.5545784993093096, the grad=1.344195584951738e-16\n",
      "Current iteration=80, the loss=0.5545784993093096, the grad=1.344195584951738e-16\n",
      "Current iteration=81, the loss=0.5545784993093096, the grad=1.344195584951738e-16\n",
      "Current iteration=82, the loss=0.5545784993093096, the grad=1.344195584951738e-16\n",
      "Current iteration=83, the loss=0.5545784993093096, the grad=1.344195584951738e-16\n",
      "Current iteration=84, the loss=0.5545784993093096, the grad=1.344195584951738e-16\n",
      "Current iteration=85, the loss=0.5545784993093096, the grad=1.344195584951738e-16\n",
      "Current iteration=86, the loss=0.5545784993093096, the grad=1.344195584951738e-16\n",
      "Current iteration=87, the loss=0.5545784993093096, the grad=1.344195584951738e-16\n",
      "Current iteration=88, the loss=0.5545784993093096, the grad=1.344195584951738e-16\n",
      "Current iteration=89, the loss=0.5545784993093096, the grad=1.344195584951738e-16\n",
      "Current iteration=90, the loss=0.5545784993093096, the grad=1.344195584951738e-16\n",
      "Current iteration=91, the loss=0.5545784993093096, the grad=1.344195584951738e-16\n",
      "Current iteration=92, the loss=0.5545784993093096, the grad=1.344195584951738e-16\n",
      "Current iteration=93, the loss=0.5545784993093096, the grad=1.344195584951738e-16\n",
      "Current iteration=94, the loss=0.5545784993093096, the grad=1.344195584951738e-16\n",
      "Current iteration=95, the loss=0.5545784993093096, the grad=1.344195584951738e-16\n",
      "Current iteration=96, the loss=0.5545784993093096, the grad=1.344195584951738e-16\n",
      "Current iteration=97, the loss=0.5545784993093096, the grad=1.344195584951738e-16\n",
      "Current iteration=98, the loss=0.5545784993093096, the grad=1.344195584951738e-16\n",
      "Current iteration=99, the loss=0.5545784993093096, the grad=1.344195584951738e-16\n",
      "end of the logistic_regression with loss= 0.5545784993093096\n",
      "the accuracy on the train set is  0.714248\n",
      "shape of teOpti  (568238, 31)\n"
     ]
    }
   ],
   "source": [
    "# test regularized logistic regression\n",
    "# txOpti = dataClean_without_splitting(tx_tr)\n",
    "print(\"shape of txOpti \", txOpti.shape)\n",
    "print(\"shape of y \", y_tr.shape)\n",
    "initial_w = np.zeros(txOpti.shape[1])\n",
    "max_iters = 100\n",
    "gamma = 0.5\n",
    "lambda_ = 0.5\n",
    "\n",
    "w, loss = reg_logistic_regression(y_tr, txOpti, lambda_, initial_w, max_iters, gamma)\n",
    "\n",
    "label = predict_logistic(w, txOpti)\n",
    "acc = calculate_accuracy(y_tr, label)\n",
    "\n",
    "# print(\"end of the logistic_regression with w=\",w,\" and loss=\", loss)\n",
    "print(\"end of the logistic_regression with loss=\", loss)\n",
    "print(\"the accuracy on the train set is \", acc)\n",
    "\n",
    "teOpti = dataClean_without_splitting(tx_te)\n",
    "print(\"shape of teOpti \", teOpti.shape)\n",
    "\n",
    "y_pred = predict_logistic(w, teOpti)\n",
    "\n",
    "OUTPUT_PATH = 'sample-submission LR regression r=1.5, gamma,lambda=0.5'\n",
    "create_csv_submission(ids_te, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc9b9982-bb31-408a-86a5-c11433e7df5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.0, 1.0, 0.0, 3.0]\n"
     ]
    }
   ],
   "source": [
    "jet = tx_tr[:, 22]\n",
    "\n",
    "values = []\n",
    "\n",
    "for x in jet: \n",
    "    if x not in values: \n",
    "        values.append(x) \n",
    "\n",
    "print(values) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495a314e-ef65-46e5-93b3-e01f401ab78f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
