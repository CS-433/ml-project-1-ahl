{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "882a9018-5319-4a82-87e7-2b0d420475bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "from implementations import *\n",
    "from helpers import *\n",
    "from helper_functions import *\n",
    "from optimization import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d56f0e3-9ee7-448f-a9dd-dd349209cc8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading the training dataset\n",
      "training dataset loaded\n"
     ]
    }
   ],
   "source": [
    "# load the training dataset\n",
    "print(\"loading the training dataset\")\n",
    "DATA_TRAIN_PATH = 'train.csv'\n",
    "y_tr, tx_tr, ids_tr = load_csv_data(DATA_TRAIN_PATH)\n",
    "print(\"training dataset loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af90503c-e3fa-4e1b-a222-63ef11b39a80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading the test dataset\n",
      "test dataset loaded\n"
     ]
    }
   ],
   "source": [
    "# load the test dataset\n",
    "print(\"loading the test dataset\")\n",
    "DATA_TEST_PATH = 'test.csv'\n",
    "y_te, tx_te, ids_te = load_csv_data(DATA_TEST_PATH)\n",
    "print(\"test dataset loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa8da103-df5b-4f1e-b595-9ae0cd91750e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shape of the training y is  (250000,)  and the training tx is  (250000, 30)\n",
      "the shape of the test y is  (568238,)  and the test tx is  (568238, 30)\n"
     ]
    }
   ],
   "source": [
    "# printing the shapes of the arrays\n",
    "print(\"the shape of the training y is \",y_tr.shape,\" and the training tx is \", tx_tr.shape)\n",
    "print(\"the shape of the test y is \",y_te.shape,\" and the test tx is \",tx_te.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6884129-6892-4ec8-bce6-fe61104b9ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing the mean square gd\n",
    "initial_w = np.zeros(tx_tr.shape[1])\n",
    "max_iters = 500\n",
    "gamma = 0.00008\n",
    "\n",
    "#w, loss = mean_squared_error_gd(y_tr, tx_tr, initial_w, max_iters, gamma)\n",
    "\n",
    "#print(\"end of the mean_squared_error_gd with w=\",w,\" and loss=\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3abc58ab-10d3-4a01-b62a-6e3445c65b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end of the least_squares with w= [-3.14664000e-01 -6.17391433e-02 -2.63661630e-01  4.00443166e-02\n",
      " -3.23831409e-03  2.36819497e+01 -1.06665608e-01 -1.31956774e+01\n",
      "  1.10025577e-01 -4.10429034e-02  1.47792868e-02 -7.39266340e-02\n",
      "  1.29692142e-01 -1.91168474e+00  1.51246931e-01 -1.97869684e-03\n",
      " -5.69064483e-04  5.85794408e-02 -5.37734703e-04  3.94036228e-03\n",
      "  4.26378298e-02  2.24787819e-03  2.81790068e-03 -1.56586861e-01\n",
      "  1.87663734e-02  9.31168600e-03  1.26628842e-02  1.49544782e-01\n",
      " -2.93365581e+00 -5.58783256e+00  5.36587380e-02]  and loss= 0.3393721979818556\n"
     ]
    }
   ],
   "source": [
    "# test least square\n",
    "txOpti = dataClean_without_splitting(tx_tr)\n",
    "w, loss = least_squares(y_tr, txOpti)\n",
    "print(\"end of the least_squares with w=\",w,\" and loss=\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1f09a81-21c9-4587-ba44-9bb03ab90776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test mean squared error sgd\n",
    "\n",
    "initial_w = np.zeros(tx_tr.shape[1])\n",
    "max_iters = 100\n",
    "gamma = 0.00001\n",
    "\n",
    "#w, loss = mean_squared_error_sgd(y_tr, tx_tr, initial_w, max_iters, gamma)\n",
    "\n",
    "#print(\"end of the mean_squared_error_sgd with w=\",w,\" and loss=\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34c8ae3d-d431-4939-8e10-d0915a0fc194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of txOpti  (250000, 31)\n",
      "Current lambda=1e-06, the loss=0.33948075708431097\n",
      "Current lambda=1.6102620275609392e-06, the loss=0.33957303863352495\n",
      "Current lambda=2.592943797404667e-06, the loss=0.3397317059940319\n",
      "Current lambda=4.1753189365604e-06, the loss=0.3399849888270534\n",
      "Current lambda=6.723357536499335e-06, the loss=0.3403502544581446\n",
      "Current lambda=1.082636733874054e-05, the loss=0.3408127985026071\n",
      "Current lambda=1.7433288221999873e-05, the loss=0.3413191893017609\n",
      "Current lambda=2.8072162039411757e-05, the loss=0.3418003259438394\n",
      "Current lambda=4.520353656360241e-05, the loss=0.34220482264575247\n",
      "Current lambda=7.278953843983146e-05, the loss=0.3425137911381104\n",
      "Current lambda=0.00011721022975334806, the loss=0.34273389111920344\n",
      "Current lambda=0.00018873918221350977, the loss=0.34288337153176474\n",
      "Current lambda=0.0003039195382313198, the loss=0.3429818717475476\n",
      "Current lambda=0.0004893900918477494, the loss=0.3430458955636129\n",
      "Current lambda=0.0007880462815669912, the loss=0.34308800051044336\n",
      "Current lambda=0.0012689610031679222, the loss=0.34311768271520704\n",
      "Current lambda=0.0020433597178569417, the loss=0.3431428721814706\n",
      "Current lambda=0.0032903445623126675, the loss=0.3431718654864535\n",
      "Current lambda=0.005298316906283708, the loss=0.3432166491786177\n",
      "Current lambda=0.008531678524172805, the loss=0.3433000921654003\n",
      "Current lambda=0.013738237958832637, the loss=0.34347044824207595\n",
      "Current lambda=0.022122162910704502, the loss=0.34382559293641096\n",
      "Current lambda=0.03562247890262444, the loss=0.3445461290886513\n",
      "Current lambda=0.05736152510448681, the loss=0.345932840106749\n",
      "Current lambda=0.09236708571873865, the loss=0.34844405311421345\n",
      "Current lambda=0.14873521072935117, the loss=0.35272556416929574\n",
      "Current lambda=0.2395026619987486, the loss=0.35958800509040906\n",
      "Current lambda=0.38566204211634725, the loss=0.3698140350368722\n",
      "Current lambda=0.6210169418915616, the loss=0.38371616361737965\n",
      "Current lambda=1.0, the loss=0.40066268021525386\n",
      "best lambda is  1e-06\n",
      "end of the logistic_regression with w= [-3.54749214e+00 -3.54749214e+00  8.74563846e-02 -2.32985161e-01\n",
      "  1.28997883e-01 -5.27523945e-02 -2.86611330e+00 -2.67360255e-01\n",
      " -3.18619660e+00 -1.47661886e-02 -2.65417164e-02 -5.19191639e-02\n",
      " -9.73057884e-02  7.83974245e-02  1.36919836e+00  1.35746045e-01\n",
      " -1.34376458e-03 -8.48081064e-04  3.65558768e-02  1.45272284e-03\n",
      "  9.12601207e-04  4.43246971e-03  5.16439522e-04  2.30844597e-02\n",
      " -1.72547946e-01  2.63529770e-02 -8.19227175e-02 -1.61321073e-03\n",
      "  2.23454279e-01 -2.38612378e+00 -2.45184889e+00  2.20898341e-01\n",
      " -3.54749214e+00 -1.91794056e-02  2.48662851e-02 -9.78046343e-02\n",
      "  1.50593736e-02 -9.77670292e-01  1.04173230e-01 -1.77620027e+00\n",
      " -1.08831092e-02  5.49025468e-03  1.56825353e-02  4.06194690e-02\n",
      "  9.37383076e-02  1.22073741e+01 -3.74529400e-02 -3.57005525e-02\n",
      " -2.73662470e-04 -1.73051392e-02 -6.82803811e-02  8.70046886e-04\n",
      "  2.00067210e-02 -4.76754662e-04 -2.02582953e-02 -1.10033966e-02\n",
      " -7.63359453e-04  9.10249232e-02  1.26169640e-03 -3.85955674e-02\n",
      "  6.42922953e-01  2.44773928e-01 -6.81965208e-02]  and loss= 0.307613222756184\n",
      "the accuracy on the train set is  0.708456\n"
     ]
    }
   ],
   "source": [
    "# test ridge regression \n",
    "\n",
    "txOpti = dataClean_without_splitting(tx_tr)\n",
    "print(\"shape of txOpti \", txOpti.shape)\n",
    "\n",
    "lambda_ = best_lambda(y_tr, txOpti, -6, 0, 30)\n",
    "degree = 2\n",
    "print(\"best lambda is \",lambda_)\n",
    "\n",
    "txOpti_poly = build_poly(txOpti, degree)\n",
    "\n",
    "w, loss = ridge_regression(y_tr, txOpti_poly, lambda_)\n",
    "\n",
    "label = predict(w, txOpti_poly)\n",
    "acc = calculate_accuracy(y_tr, label)\n",
    "\n",
    "print(\"end of the logistic_regression with w=\",w,\" and loss=\", loss)\n",
    "print(\"the accuracy on the train set is \", acc)\n",
    "\n",
    "#y_pred = predict(w, tx_te)\n",
    "#y_pred[y_pred==0] = -1\n",
    "\n",
    "#OUTPUT_PATH = 'sample-submission'\n",
    "#create_csv_submission(ids_te, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d5f5c7de-0c97-4975-93cb-71f4916eb96e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of txOpti  (99913, 20)\n",
      "shape of yOpti  (99913, 20)\n",
      "Preiteration, the loss=1.1116528187695083, the grad=210.0425291113847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ml-project-1-ahl/helper_functions.py:61: RuntimeWarning: divide by zero encountered in log\n",
      "  return -np.sum(y * np.log(sigmoid(v)) + (1 - y) * np.log(1-sigmoid(v)))/N\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:86: RuntimeWarning: invalid value encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, the loss=nan, the grad=210.0425291113847\n",
      "Current iteration=1, the loss=nan, the grad=103.84048931520206\n",
      "Current iteration=2, the loss=nan, the grad=103.84048931520206\n",
      "Current iteration=3, the loss=nan, the grad=103.84048931520206\n",
      "Current iteration=4, the loss=nan, the grad=103.84048931520206\n",
      "Current iteration=5, the loss=nan, the grad=103.84048931520206\n",
      "Current iteration=6, the loss=nan, the grad=103.84048931520206\n",
      "Current iteration=7, the loss=nan, the grad=103.84048931520206\n",
      "Current iteration=8, the loss=nan, the grad=103.84048931520206\n",
      "Current iteration=9, the loss=nan, the grad=103.84048931520206\n",
      "Current iteration=10, the loss=nan, the grad=103.84048931520206\n",
      "Current iteration=11, the loss=nan, the grad=103.84048931520206\n",
      "Current iteration=12, the loss=nan, the grad=103.84048931520206\n",
      "Current iteration=13, the loss=nan, the grad=103.84048931520206\n",
      "Current iteration=14, the loss=nan, the grad=103.84048931520206\n",
      "Current iteration=15, the loss=nan, the grad=103.84048931520206\n",
      "Current iteration=16, the loss=nan, the grad=103.84048931520206\n",
      "Current iteration=17, the loss=nan, the grad=103.84048931520206\n",
      "Current iteration=18, the loss=nan, the grad=103.84048931520206\n",
      "Current iteration=19, the loss=nan, the grad=103.84048931520206\n",
      "Current iteration=20, the loss=nan, the grad=103.84048931520206\n",
      "Current iteration=21, the loss=nan, the grad=103.84048931520206\n",
      "Current iteration=22, the loss=nan, the grad=103.84048931520206\n",
      "Current iteration=23, the loss=nan, the grad=103.84048931520206\n",
      "Current iteration=24, the loss=nan, the grad=103.84048931520206\n",
      "Current iteration=25, the loss=nan, the grad=103.84048931520206\n",
      "Current iteration=26, the loss=nan, the grad=103.84048931520206\n",
      "Current iteration=27, the loss=nan, the grad=103.84048931520206\n",
      "Current iteration=28, the loss=nan, the grad=103.84048931520206\n",
      "Current iteration=29, the loss=nan, the grad=103.84048931520206\n",
      "Current iteration=30, the loss=nan, the grad=103.84048931520206\n",
      "Current iteration=31, the loss=nan, the grad=103.84048931520206\n",
      "Current iteration=32, the loss=nan, the grad=103.84048931520206\n",
      "Current iteration=33, the loss=nan, the grad=103.84048931520206\n",
      "Current iteration=34, the loss=nan, the grad=103.84048931520206\n",
      "Current iteration=35, the loss=nan, the grad=103.84048931520206\n",
      "Current iteration=36, the loss=nan, the grad=103.84048931520206\n",
      "Current iteration=37, the loss=nan, the grad=103.84048931520206\n",
      "Current iteration=38, the loss=nan, the grad=103.84048931520206\n",
      "Current iteration=39, the loss=nan, the grad=103.84048931520206\n",
      "Current iteration=40, the loss=nan, the grad=103.84048931520206\n",
      "Current iteration=41, the loss=nan, the grad=103.84048931520206\n",
      "Current iteration=42, the loss=nan, the grad=103.84048931520206\n",
      "Current iteration=43, the loss=nan, the grad=103.84048931520206\n",
      "Current iteration=44, the loss=nan, the grad=103.84048931520206\n",
      "Current iteration=45, the loss=nan, the grad=103.84048931520206\n",
      "Current iteration=46, the loss=nan, the grad=103.84048931520206\n",
      "Current iteration=47, the loss=nan, the grad=103.84048931520206\n",
      "Current iteration=48, the loss=nan, the grad=103.84048931520206\n",
      "Current iteration=49, the loss=nan, the grad=103.84048931520206\n",
      "Current iteration=50, the loss=nan, the grad=103.84048931520206\n",
      "Current iteration=51, the loss=nan, the grad=103.84048931520206\n",
      "Current iteration=52, the loss=nan, the grad=103.84048931520206\n",
      "Current iteration=53, the loss=nan, the grad=103.84048931520206\n",
      "Current iteration=54, the loss=nan, the grad=103.84048931520206\n",
      "Current iteration=55, the loss=nan, the grad=103.84048931520206\n",
      "Current iteration=56, the loss=nan, the grad=103.84048931520206\n",
      "Current iteration=57, the loss=nan, the grad=103.84048931520206\n",
      "Current iteration=58, the loss=nan, the grad=103.84048931520206\n",
      "Current iteration=59, the loss=nan, the grad=103.84048931520206\n",
      "Current iteration=60, the loss=nan, the grad=103.84048931520206\n",
      "Current iteration=61, the loss=nan, the grad=103.84048931520206\n",
      "Current iteration=62, the loss=nan, the grad=103.84048931520206\n",
      "Current iteration=63, the loss=nan, the grad=103.84048931520206\n",
      "Current iteration=64, the loss=nan, the grad=103.84048931520206\n",
      "Current iteration=65, the loss=nan, the grad=103.84048931520206\n",
      "Current iteration=66, the loss=nan, the grad=103.84048931520206\n",
      "Current iteration=67, the loss=nan, the grad=103.84048931520206\n",
      "Current iteration=68, the loss=nan, the grad=103.84048931520206\n",
      "Current iteration=69, the loss=nan, the grad=103.84048931520206\n",
      "Current iteration=70, the loss=nan, the grad=103.84048931520206\n",
      "Current iteration=71, the loss=nan, the grad=103.84048931520206\n",
      "Current iteration=72, the loss=nan, the grad=103.84048931520206\n",
      "Current iteration=73, the loss=nan, the grad=103.84048931520206\n",
      "Current iteration=74, the loss=nan, the grad=103.84048931520206\n",
      "Current iteration=75, the loss=nan, the grad=103.84048931520206\n",
      "Current iteration=76, the loss=nan, the grad=103.84048931520206\n",
      "Current iteration=77, the loss=nan, the grad=103.84048931520206\n",
      "Current iteration=78, the loss=nan, the grad=103.84048931520206\n",
      "Current iteration=79, the loss=nan, the grad=103.84048931520206\n",
      "Current iteration=80, the loss=nan, the grad=103.84048931520206\n",
      "Current iteration=81, the loss=nan, the grad=103.84048931520206\n",
      "Current iteration=82, the loss=nan, the grad=103.84048931520206\n",
      "Current iteration=83, the loss=nan, the grad=103.84048931520206\n",
      "Current iteration=84, the loss=nan, the grad=103.84048931520206\n",
      "Current iteration=85, the loss=nan, the grad=103.84048931520206\n",
      "Current iteration=86, the loss=nan, the grad=103.84048931520206\n",
      "Current iteration=87, the loss=nan, the grad=103.84048931520206\n",
      "Current iteration=88, the loss=nan, the grad=103.84048931520206\n",
      "Current iteration=89, the loss=nan, the grad=103.84048931520206\n",
      "Current iteration=90, the loss=nan, the grad=103.84048931520206\n",
      "Current iteration=91, the loss=nan, the grad=103.84048931520206\n",
      "Current iteration=92, the loss=nan, the grad=103.84048931520206\n",
      "Current iteration=93, the loss=nan, the grad=103.84048931520206\n",
      "Current iteration=94, the loss=nan, the grad=103.84048931520206\n",
      "Current iteration=95, the loss=nan, the grad=103.84048931520206\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [31]\u001b[0m, in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m4\u001b[39m): \n\u001b[1;32m     16\u001b[0m     initial_w \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(txOpti[i]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m---> 17\u001b[0m     w, loss \u001b[38;5;241m=\u001b[39m \u001b[43mlogistic_regression\u001b[49m\u001b[43m(\u001b[49m\u001b[43myOpti\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtxOpti\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_w\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     label \u001b[38;5;241m=\u001b[39m predict(w, txOpti[i])\n\u001b[1;32m     19\u001b[0m     acc \u001b[38;5;241m=\u001b[39m calculate_accuracy(yOpti[i], label)\n",
      "File \u001b[0;32m~/ml-project-1-ahl/implementations.py:168\u001b[0m, in \u001b[0;36mlogistic_regression\u001b[0;34m(y, tx, initial_w, max_iters, gamma)\u001b[0m\n\u001b[1;32m    164\u001b[0m grad \u001b[38;5;241m=\u001b[39m calculate_gradient(y, tx, w)\n\u001b[1;32m    166\u001b[0m w \u001b[38;5;241m=\u001b[39m w \u001b[38;5;241m-\u001b[39m gamma \u001b[38;5;241m*\u001b[39m grad\n\u001b[0;32m--> 168\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28miter\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    172\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCurrent iteration=\u001b[39m\u001b[38;5;132;01m{i}\u001b[39;00m\u001b[38;5;124m, the loss=\u001b[39m\u001b[38;5;132;01m{l}\u001b[39;00m\u001b[38;5;124m, the grad=\u001b[39m\u001b[38;5;132;01m{we}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    173\u001b[0m             i\u001b[38;5;241m=\u001b[39m\u001b[38;5;28miter\u001b[39m, l\u001b[38;5;241m=\u001b[39mloss, we\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(grad)\n\u001b[1;32m    174\u001b[0m         )\n\u001b[1;32m    175\u001b[0m     )\n",
      "File \u001b[0;32m~/ml-project-1-ahl/helper_functions.py:61\u001b[0m, in \u001b[0;36mcalculate_loss\u001b[0;34m(y, tx, w)\u001b[0m\n\u001b[1;32m     55\u001b[0m v \u001b[38;5;241m=\u001b[39m tx \u001b[38;5;241m@\u001b[39m w\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m#print(\"############################## v= \",v)    \u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m#sig = sigmoid(v)\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m#print(\"############################## sig= \",sig)\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m#log1 = np.log(sig)\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m#log2 = np.log(1-sig)\u001b[39;00m\n\u001b[0;32m---> 61\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39msum(y \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mlog(\u001b[43msigmoid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m y) \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39msigmoid(v)))\u001b[38;5;241m/\u001b[39mN\n",
      "File \u001b[0;32m~/ml-project-1-ahl/helper_functions.py:40\u001b[0m, in \u001b[0;36msigmoid\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m     38\u001b[0m neg \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(t\u001b[38;5;241m<\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     39\u001b[0m t[pos]\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39m(\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m+\u001b[39mnp\u001b[38;5;241m.\u001b[39mexp(\u001b[38;5;241m-\u001b[39mt[pos]))\n\u001b[0;32m---> 40\u001b[0m t[neg]\u001b[38;5;241m=\u001b[39m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m[\u001b[49m\u001b[43mneg\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m/\u001b[39m(\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m+\u001b[39mnp\u001b[38;5;241m.\u001b[39mexp(t[neg]))\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m t\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# test logistic regression\n",
    "\n",
    "txOpti, yOpti = dataClean(tx_tr, y_tr)\n",
    "print(\"shape of txOpti \", txOpti[0].shape)\n",
    "print(\"shape of yOpti \", txOpti[0].shape)\n",
    "#initial_w = np.zeros(txOpti.shape[1])\n",
    "# initial_w = w\n",
    "max_iters = 100\n",
    "gamma = 0.5\n",
    "ws = []\n",
    "losses = []\n",
    "y_pred = []\n",
    "accArray = []\n",
    "\n",
    "for i in range(4): \n",
    "    initial_w = np.zeros(txOpti[i].shape[1])\n",
    "    w, loss = logistic_regression(yOpti[i], txOpti[i], initial_w, max_iters, gamma)\n",
    "    label = predict(w, txOpti[i])\n",
    "    acc = calculate_accuracy(yOpti[i], label)\n",
    "    \n",
    "    ws.append(w)\n",
    "    losses.append(loss)\n",
    "    y_pred.append(label)\n",
    "    accArray.append(acc)\n",
    "\n",
    "tot_acc = accArray[0] + accArray[1] + accArray[2] + accArray[3]\n",
    "weights = np.array(list(ws[0])+list(ws[1])+list(ws[2])+list(ws[3]))\n",
    "loss = np.mean(losses)\n",
    "    \n",
    "#teOpti = dataClean(tx_te)\n",
    "#print(\"shape of teOpti \", teOpti.shape)\n",
    "\n",
    "#y_pred = predict(w, teOpti)\n",
    "\n",
    "#OUTPUT_PATH = 'sample-submission LR 2'\n",
    "#create_csv_submission(ids_te, y_pred, OUTPUT_PATH)\n",
    "\n",
    "print(\"end of the logistic_regression with w=\",weights,\" and loss=\", loss)\n",
    "print(\"the accurcy on the train set is \", tot_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e1c71eed-4b5e-41af-b0b6-57d490347155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of txOpti  (250000, 31)\n",
      "shape of y  (250000,)\n",
      "Preiteration, the loss=1.0624754799064415, the grad=1.1261387558041078\n",
      "Current iteration=0, the loss=0.679184067185061, the grad=1.1261387558041078\n",
      "Current iteration=1, the loss=0.4702392454195871, the grad=0.8538167005002987\n",
      "Current iteration=2, the loss=0.24733569478501988, the grad=0.7566691907262045\n",
      "Current iteration=3, the loss=0.03965120507362577, the grad=0.7032345486116555\n",
      "Current iteration=4, the loss=-0.15856309305254304, the grad=0.6682047651428545\n",
      "Current iteration=5, the loss=-0.35012637130185936, the grad=0.6444012668452963\n",
      "Current iteration=6, the loss=-0.5369500971543492, the grad=0.627674520555563\n",
      "Current iteration=7, the loss=-0.7202943361917574, the grad=0.6155608377377577\n",
      "Current iteration=8, the loss=-0.9010072801331301, the grad=0.6065432404283911\n",
      "Current iteration=9, the loss=-1.0796755361909431, the grad=0.5996599381310498\n",
      "Current iteration=10, the loss=-1.2567164527783987, the grad=0.5942854181664731\n",
      "Current iteration=11, the loss=-1.432435229554529, the grad=0.5900031659469835\n",
      "Current iteration=12, the loss=-1.6070608347616442, the grad=0.5865295049844736\n",
      "Current iteration=13, the loss=-1.7807690698092549, the grad=0.5836669773605504\n",
      "Current iteration=14, the loss=-1.9536977215151448, the grad=0.581275231683962\n",
      "Current iteration=15, the loss=-2.1259567522067955, the grad=0.5792524983627501\n",
      "Current iteration=16, the loss=-2.2976353137395087, the grad=0.577523583144488\n",
      "Current iteration=17, the loss=-2.468806684989985, the grad=0.5760319429235286\n",
      "Current iteration=18, the loss=-2.639531822369016, the grad=0.5747343615740087\n",
      "Current iteration=19, the loss=-2.8098619642207976, the grad=0.5735973096637521\n",
      "Current iteration=20, the loss=-2.9798405765809237, the grad=0.5725944129329611\n",
      "Current iteration=21, the loss=-3.149504831446974, the grad=0.5717046628607949\n",
      "Current iteration=22, the loss=-3.3188867471285572, the grad=0.5709111318968276\n",
      "Current iteration=23, the loss=-3.4880140801408883, the grad=0.5702000372555529\n",
      "Current iteration=24, the loss=-3.6569110315191744, the grad=0.5695600490831492\n",
      "Current iteration=25, the loss=-3.8255988124929385, the grad=0.5689817724239842\n",
      "Current iteration=26, the loss=-3.994096102149425, the grad=0.5684573544980335\n",
      "Current iteration=27, the loss=-4.162419421129158, the grad=0.567980183511137\n",
      "Current iteration=28, the loss=-4.330583439314479, the grad=0.5675446551538\n",
      "Current iteration=29, the loss=-4.4986012310999435, the grad=0.5671459897416609\n",
      "Current iteration=30, the loss=-4.666484488647109, the grad=0.5667800876618474\n",
      "Current iteration=31, the loss=-4.834243701173881, the grad=0.5664434140950909\n",
      "Current iteration=32, the loss=-5.001888306570754, the grad=0.5661329063304815\n",
      "Current iteration=33, the loss=-5.169426820307931, the grad=0.5658458986751824\n",
      "Current iteration=34, the loss=-5.336866945582818, the grad=0.5655800611848153\n",
      "Current iteration=35, the loss=-5.504215667875167, the grad=0.5653333493375885\n",
      "Current iteration=36, the loss=-5.67147933646841, the grad=0.5651039624398185\n",
      "Current iteration=37, the loss=-5.838663735017989, the grad=0.5648903090474066\n",
      "Current iteration=38, the loss=-6.0057741428697735, the grad=0.5646909780626213\n",
      "Current iteration=39, the loss=-6.172815388530563, the grad=0.5645047144506611\n",
      "Current iteration=40, the loss=-6.339791896451345, the grad=0.5643303987390983\n",
      "Current iteration=41, the loss=-6.506707728088933, the grad=0.5641670296322339\n",
      "Current iteration=42, the loss=-6.673566618053483, the grad=0.5640137092038796\n",
      "Current iteration=43, the loss=-6.840372006019963, the grad=0.5638696302351157\n",
      "Current iteration=44, the loss=-7.0071270649756965, the grad=0.5637340653448384\n",
      "Current iteration=45, the loss=-7.173834726288357, the grad=0.5636063576254308\n",
      "Current iteration=46, the loss=-7.340497702006408, the grad=0.5634859125473739\n",
      "Current iteration=47, the loss=-7.507118504743392, the grad=0.5633721909379761\n",
      "Current iteration=48, the loss=-7.673699465447078, the grad=0.5632647028727674\n",
      "Current iteration=49, the loss=-7.840242749311916, the grad=0.563163002345183\n",
      "Current iteration=50, the loss=-8.006750370057684, the grad=0.5630666826022419\n",
      "Current iteration=51, the loss=-8.173224202766855, the grad=0.5629753720519789\n",
      "Current iteration=52, the loss=-8.339665995447582, the grad=0.562888730663299\n",
      "Current iteration=53, the loss=-8.506077379467436, the grad=0.5628064467911709\n",
      "Current iteration=54, the loss=-8.672459878984265, the grad=0.5627282343702835\n",
      "Current iteration=55, the loss=-8.838814919484706, the grad=0.5626538304287708\n",
      "Current iteration=56, the loss=-9.005143835527, the grad=0.5625829928806927\n",
      "Current iteration=57, the loss=-9.171447877773048, the grad=0.5625154985619135\n",
      "Current iteration=58, the loss=-9.337728219384408, the grad=0.562451141479007\n",
      "Current iteration=59, the loss=-9.503985961848, the grad=0.5623897312450614\n",
      "Current iteration=60, the loss=-9.670222140289827, the grad=0.5623310916798063\n",
      "Current iteration=61, the loss=-9.836437728327981, the grad=0.5622750595545435\n",
      "Current iteration=62, the loss=-10.002633642510768, the grad=0.5622214834649347\n",
      "Current iteration=63, the loss=-10.168810746380414, the grad=0.5621702228169212\n",
      "Current iteration=64, the loss=-10.334969854198484, the grad=0.5621211469129208\n",
      "Current iteration=65, the loss=-10.501111734365123, the grad=0.5620741341271042\n",
      "Current iteration=66, the loss=-10.667237112560976, the grad=0.5620290711599162\n",
      "Current iteration=67, the loss=-10.833346674637403, the grad=0.5619858523632376\n",
      "Current iteration=68, the loss=-10.999441069277974, the grad=0.5619443791286192\n",
      "Current iteration=69, the loss=-11.165520910452031, the grad=0.5619045593319101\n",
      "Current iteration=70, the loss=-11.331586779678759, the grad=0.5618663068284221\n",
      "Current iteration=71, the loss=-11.49763922811851, the grad=0.5618295409934053\n",
      "Current iteration=72, the loss=-11.663678778506444, the grad=0.561794186303254\n",
      "Current iteration=73, the loss=-11.829705926941985, the grad=0.5617601719533536\n",
      "Current iteration=74, the loss=-11.995721144546483, the grad=0.5617274315089402\n",
      "Current iteration=75, the loss=-12.161724878999978, the grad=0.5616959025857524\n",
      "Current iteration=76, the loss=-12.327717555967357, the grad=0.561665526557597\n",
      "Current iteration=77, the loss=-12.49369958042277, the grad=0.5616362482882675\n",
      "Current iteration=78, the loss=-12.659671337880715, the grad=0.5616080158855117\n",
      "Current iteration=79, the loss=-12.82563319554128, the grad=0.5615807804750124\n",
      "Current iteration=80, the loss=-12.991585503356307, the grad=0.5615544959925243\n",
      "Current iteration=81, the loss=-13.157528595022814, the grad=0.5615291189925301\n",
      "Current iteration=82, the loss=-13.323462788909241, the grad=0.5615046084719197\n",
      "Current iteration=83, the loss=-13.48938838891976, the grad=0.56148092570737\n",
      "Current iteration=84, the loss=-13.65530568530133, the grad=0.5614580341052078\n",
      "Current iteration=85, the loss=-13.821214955397934, the grad=0.5614358990626833\n",
      "Current iteration=86, the loss=-13.987116464355761, the grad=0.5614144878396699\n",
      "Current iteration=87, the loss=-14.153010465783149, the grad=0.5613937694398947\n",
      "Current iteration=88, the loss=-14.318897202368527, the grad=0.5613737145009162\n",
      "Current iteration=89, the loss=-14.484776906459418, the grad=0.5613542951921014\n",
      "Current iteration=90, the loss=-14.650649800605246, the grad=0.561335485119962\n",
      "Current iteration=91, the loss=-14.816516098066675, the grad=0.5613172592402285\n",
      "Current iteration=92, the loss=-14.982376003293714, the grad=0.561299593776146\n",
      "Current iteration=93, the loss=-15.14822971237479, the grad=0.5612824661424684\n",
      "Current iteration=94, the loss=-15.314077413458891, the grad=0.5612658548747105\n",
      "Current iteration=95, the loss=-15.479919287152478, the grad=0.5612497395632589\n",
      "Current iteration=96, the loss=-15.64575550689309, the grad=0.5612341007919387\n",
      "Current iteration=97, the loss=-15.811586239300981, the grad=0.5612189200807207\n",
      "Current iteration=98, the loss=-15.977411644510479, the grad=0.5612041798322329\n",
      "Current iteration=99, the loss=-16.14323187648221, the grad=0.5611898632818055\n",
      "end of the logistic_regression with w= [-24.46434689  -0.2164016  -10.71139374   3.0499935    1.94830525\n",
      "   0.59703361  -1.67353134   0.55737959   3.53218036  -1.22714432\n",
      "   1.44113304  -4.80032898   5.73689387   0.58604007   4.91807622\n",
      "  -0.0999658   -0.0734076   -0.59726864  -0.04795197   0.15320527\n",
      "  -2.21606706   0.21108758   1.13509242   0.19975619   1.52869258\n",
      "   0.66773227   0.74783139   0.65256532   0.5833705    0.58286767\n",
      "   1.46091821]  and loss= -16.14323187648221\n",
      "the accuracy on the train set is  0.706972\n"
     ]
    }
   ],
   "source": [
    "# test logistic regression without splitting \n",
    "txOpti = dataClean_without_splitting(tx_tr)\n",
    "print(\"shape of txOpti \", txOpti.shape)\n",
    "print(\"shape of y \", y_tr.shape)\n",
    "initial_w = np.zeros(txOpti.shape[1])\n",
    "max_iters = 100\n",
    "gamma = 0.5\n",
    "\n",
    "w, loss = logistic_regression(y_tr, txOpti, initial_w, max_iters, gamma)\n",
    "\n",
    "label = predict_logistic(w, txOpti)\n",
    "acc = calculate_accuracy(y_tr, label)\n",
    "\n",
    "print(\"end of the logistic_regression with w=\",w,\" and loss=\", loss)\n",
    "print(\"the accuracy on the train set is \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0b54129f-2e9e-489f-989e-cffe093f018d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[105.012 46.524 73.752 38.468 -999.000 -999.000 -999.000 2.492 12.316\n",
      " 120.665 1.280 -0.356 -999.000 31.804 -0.023 -0.033 40.516 -0.045 0.086\n",
      " 34.802 -0.024 179.739 1.000 38.960 -1.872 -2.093 -999.000 -999.000\n",
      " -999.000 40.513]\n",
      "[-49.023 49.240 81.182 57.896 -708.421 -601.237 -709.357 2.373 18.917\n",
      " 158.432 1.438 -0.128 -708.985 38.707 -0.011 -0.008 46.660 -0.020 0.044\n",
      " 41.717 -0.010 209.797 0.979 -348.330 -399.254 -399.260 -692.381 -709.122\n",
      " -709.119 73.065]\n",
      "(array([ 4,  5,  6, 12, 26, 27, 28]),)\n",
      "index : 4\n",
      "177457\n",
      "index : 5\n",
      "177457\n",
      "index : 6\n",
      "177457\n",
      "index : 12\n",
      "177457\n",
      "index : 26\n",
      "177457\n",
      "index : 27\n",
      "177457\n",
      "index : 28\n",
      "177457\n",
      "counts of each column  [177457, 177457, 177457, 177457, 177457, 177457, 177457]\n"
     ]
    }
   ],
   "source": [
    "#clean column , remove column that has only -999 elements. \n",
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.3f}\".format(x)})\n",
    "\n",
    "median = np.median(tx_tr, axis=0) \n",
    "mean = np.mean(tx_tr, axis=0) \n",
    "\n",
    "print(median)\n",
    "print(mean)\n",
    "\n",
    "indx = np.where(median == -999)\n",
    "print(indx)\n",
    "\n",
    "counts = []\n",
    "count = 0\n",
    "\n",
    "for i in indx[0]: \n",
    "    count = np.count_nonzero( tx_tr[:,i] == -999)\n",
    "    print(\"index :\", i)\n",
    "    print(count)\n",
    "    counts.append(count)\n",
    "\n",
    "print(\"counts of each column \", counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cd948ae1-5376-46f3-b929-6d5d233f0bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7,)\n",
      "(250000, 30)\n",
      "(250000, 23)\n"
     ]
    }
   ],
   "source": [
    "median = np.median(tx_tr, axis=0) \n",
    "indx = np.where(median == -999)\n",
    "tx1 = np.delete(tx_tr, indx[0], 1)\n",
    "print(indx[0].shape)\n",
    "print(tx_tr.shape)\n",
    "print(tx1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f062a94a-ff3d-4d4d-88b2-66b0ca8bf65c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of txOpti  (250000, 24)\n",
      "shape of y  (250000,)\n",
      "Current iteration=0, the loss=0.7070366274465562, the grad=1.0991914599924302\n",
      "Current iteration=1, the loss=0.6223439392176738, the grad=0.357515024179797\n",
      "Current iteration=2, the loss=0.5832462612934024, the grad=0.13691403689386653\n",
      "Current iteration=3, the loss=0.5679408656189062, the grad=0.055114316590955496\n",
      "Current iteration=4, the loss=0.561741397073609, the grad=0.022421000076055977\n",
      "Current iteration=5, the loss=0.5592254737046597, the grad=0.009165963245399178\n",
      "Current iteration=6, the loss=0.5582009730840245, the grad=0.003758517828891186\n",
      "Current iteration=7, the loss=0.5577828898901592, the grad=0.00154497753493535\n",
      "Current iteration=8, the loss=0.5576119844514998, the grad=0.0006366126656023305\n",
      "Current iteration=9, the loss=0.5575420204953335, the grad=0.0002630022845962372\n",
      "Current iteration=10, the loss=0.5575133423312943, the grad=0.00010897088488415756\n",
      "Current iteration=11, the loss=0.557501573049589, the grad=4.5299426082263684e-05\n",
      "Current iteration=12, the loss=0.5574967374161942, the grad=1.890109179450164e-05\n",
      "Current iteration=13, the loss=0.5574947483050923, the grad=7.919177457615603e-06\n",
      "Current iteration=14, the loss=0.557493929133049, the grad=3.3331587602904853e-06\n",
      "Current iteration=15, the loss=0.5574935913654574, the grad=1.4098892568881526e-06\n",
      "Current iteration=16, the loss=0.5574934519180541, the grad=5.995336993698218e-07\n",
      "Current iteration=17, the loss=0.5574933942705568, the grad=2.563626442012718e-07\n",
      "Current iteration=18, the loss=0.557493370405628, the grad=1.1025032714180748e-07\n",
      "Current iteration=19, the loss=0.5574933605113322, the grad=4.7688545289629934e-08\n",
      "Current iteration=20, the loss=0.5574933564027443, the grad=2.0746018511632244e-08\n",
      "Current iteration=21, the loss=0.5574933546938181, the grad=9.075583522598355e-09\n",
      "Current iteration=22, the loss=0.5574933539817555, the grad=3.991420566155174e-09\n",
      "Current iteration=23, the loss=0.557493353684507, the grad=1.7642395824586338e-09\n",
      "Current iteration=24, the loss=0.5574933535601793, the grad=7.834446937748873e-10\n",
      "Current iteration=25, the loss=0.5574933535080708, the grad=3.493923551792802e-10\n",
      "Current iteration=26, the loss=0.5574933534861838, the grad=1.56424148201085e-10\n",
      "Current iteration=27, the loss=0.5574933534769705, the grad=7.027765281213403e-11\n",
      "Current iteration=28, the loss=0.5574933534730828, the grad=3.1673301796552045e-11\n",
      "Current iteration=29, the loss=0.5574933534714385, the grad=1.4315179265961283e-11\n",
      "Current iteration=30, the loss=0.5574933534707414, the grad=6.486665448433092e-12\n",
      "Current iteration=31, the loss=0.5574933534704449, the grad=2.945841808673486e-12\n",
      "Current iteration=32, the loss=0.5574933534703186, the grad=1.3406087713482706e-12\n",
      "Current iteration=33, the loss=0.5574933534702645, the grad=6.112264835002368e-13\n",
      "Current iteration=34, the loss=0.5574933534702415, the grad=2.7925231460907097e-13\n",
      "Current iteration=35, the loss=0.5574933534702315, the grad=1.2761878340226263e-13\n",
      "Current iteration=36, the loss=0.5574933534702271, the grad=5.849236359840727e-14\n",
      "Current iteration=37, the loss=0.5574933534702254, the grad=2.683462023359561e-14\n",
      "Current iteration=38, the loss=0.5574933534702244, the grad=1.2341565184310789e-14\n",
      "Current iteration=39, the loss=0.5574933534702242, the grad=5.6925768594211974e-15\n",
      "Current iteration=40, the loss=0.5574933534702241, the grad=2.647894724052281e-15\n",
      "Current iteration=41, the loss=0.5574933534702239, the grad=1.2289520340948985e-15\n",
      "Current iteration=42, the loss=0.5574933534702239, the grad=5.798366739030739e-16\n",
      "Current iteration=43, the loss=0.5574933534702239, the grad=2.8895914172974435e-16\n",
      "Current iteration=44, the loss=0.5574933534702239, the grad=1.7787766598875978e-16\n",
      "Current iteration=45, the loss=0.5574933534702239, the grad=1.4272054578842085e-16\n",
      "Current iteration=46, the loss=0.5574933534702239, the grad=1.1904520105201007e-16\n",
      "Current iteration=47, the loss=0.5574933534702239, the grad=1.176226947125341e-16\n",
      "Current iteration=48, the loss=0.5574933534702239, the grad=1.1633645927922808e-16\n",
      "Current iteration=49, the loss=0.5574933534702239, the grad=1.2199086128212092e-16\n",
      "Current iteration=50, the loss=0.5574933534702239, the grad=1.1730646643926338e-16\n",
      "Current iteration=51, the loss=0.5574933534702239, the grad=1.222965152304754e-16\n",
      "Current iteration=52, the loss=0.5574933534702239, the grad=1.1731869107129534e-16\n",
      "Current iteration=53, the loss=0.5574933534702239, the grad=1.2228493241659996e-16\n",
      "Current iteration=54, the loss=0.5574933534702239, the grad=1.17318540776177e-16\n",
      "Current iteration=55, the loss=0.5574933534702239, the grad=1.2228459596992526e-16\n",
      "Current iteration=56, the loss=0.5574933534702239, the grad=1.1731869107129534e-16\n",
      "Current iteration=57, the loss=0.5574933534702239, the grad=1.2228435565030517e-16\n",
      "Current iteration=58, the loss=0.5574933534702239, the grad=1.1731869107129534e-16\n",
      "Current iteration=59, the loss=0.5574933534702239, the grad=1.2228435565030517e-16\n",
      "Current iteration=60, the loss=0.5574933534702239, the grad=1.1731869107129534e-16\n",
      "Current iteration=61, the loss=0.5574933534702239, the grad=1.2228435565030517e-16\n",
      "Current iteration=62, the loss=0.5574933534702239, the grad=1.1731869107129534e-16\n",
      "Current iteration=63, the loss=0.5574933534702239, the grad=1.2228435565030517e-16\n",
      "Current iteration=64, the loss=0.5574933534702239, the grad=1.1731869107129534e-16\n",
      "Current iteration=65, the loss=0.5574933534702239, the grad=1.2228435565030517e-16\n",
      "Current iteration=66, the loss=0.5574933534702239, the grad=1.1731869107129534e-16\n",
      "Current iteration=67, the loss=0.5574933534702239, the grad=1.2228435565030517e-16\n",
      "Current iteration=68, the loss=0.5574933534702239, the grad=1.1731869107129534e-16\n",
      "Current iteration=69, the loss=0.5574933534702239, the grad=1.2228435565030517e-16\n",
      "Current iteration=70, the loss=0.5574933534702239, the grad=1.1731869107129534e-16\n",
      "Current iteration=71, the loss=0.5574933534702239, the grad=1.2228435565030517e-16\n",
      "Current iteration=72, the loss=0.5574933534702239, the grad=1.1731869107129534e-16\n",
      "Current iteration=73, the loss=0.5574933534702239, the grad=1.2228435565030517e-16\n",
      "Current iteration=74, the loss=0.5574933534702239, the grad=1.1731869107129534e-16\n",
      "Current iteration=75, the loss=0.5574933534702239, the grad=1.2228435565030517e-16\n",
      "Current iteration=76, the loss=0.5574933534702239, the grad=1.1731869107129534e-16\n",
      "Current iteration=77, the loss=0.5574933534702239, the grad=1.2228435565030517e-16\n",
      "Current iteration=78, the loss=0.5574933534702239, the grad=1.1731869107129534e-16\n",
      "Current iteration=79, the loss=0.5574933534702239, the grad=1.2228435565030517e-16\n",
      "Current iteration=80, the loss=0.5574933534702239, the grad=1.1731869107129534e-16\n",
      "Current iteration=81, the loss=0.5574933534702239, the grad=1.2228435565030517e-16\n",
      "Current iteration=82, the loss=0.5574933534702239, the grad=1.1731869107129534e-16\n",
      "Current iteration=83, the loss=0.5574933534702239, the grad=1.2228435565030517e-16\n",
      "Current iteration=84, the loss=0.5574933534702239, the grad=1.1731869107129534e-16\n",
      "Current iteration=85, the loss=0.5574933534702239, the grad=1.2228435565030517e-16\n",
      "Current iteration=86, the loss=0.5574933534702239, the grad=1.1731869107129534e-16\n",
      "Current iteration=87, the loss=0.5574933534702239, the grad=1.2228435565030517e-16\n",
      "Current iteration=88, the loss=0.5574933534702239, the grad=1.1731869107129534e-16\n",
      "Current iteration=89, the loss=0.5574933534702239, the grad=1.2228435565030517e-16\n",
      "Current iteration=90, the loss=0.5574933534702239, the grad=1.1731869107129534e-16\n",
      "Current iteration=91, the loss=0.5574933534702239, the grad=1.2228435565030517e-16\n",
      "Current iteration=92, the loss=0.5574933534702239, the grad=1.1731869107129534e-16\n",
      "Current iteration=93, the loss=0.5574933534702239, the grad=1.2228435565030517e-16\n",
      "Current iteration=94, the loss=0.5574933534702239, the grad=1.1731869107129534e-16\n",
      "Current iteration=95, the loss=0.5574933534702239, the grad=1.2228435565030517e-16\n",
      "Current iteration=96, the loss=0.5574933534702239, the grad=1.1731869107129534e-16\n",
      "Current iteration=97, the loss=0.5574933534702239, the grad=1.2228435565030517e-16\n",
      "Current iteration=98, the loss=0.5574933534702239, the grad=1.1731869107129534e-16\n",
      "Current iteration=99, the loss=0.5574933534702239, the grad=1.2228435565030517e-16\n",
      "end of the logistic_regression with w= [-0.67145633  0.01277643 -0.25251074  0.15082405  0.0461801   0.03041994\n",
      " -0.02695106  0.07800049 -0.16041958  0.15636097  0.17235857 -0.00090193\n",
      " -0.00201319 -0.02658078  0.00313275  0.00490235 -0.07081156  0.00549641\n",
      "  0.0684512   0.08558615  0.06799837  0.02327072  0.02511678  0.06837195]  and loss= 0.5574933534702239\n",
      "the accuracy on the train set is  0.717288\n",
      "shape of teOpti  (568238, 24)\n"
     ]
    }
   ],
   "source": [
    "# test regularized logistic regression\n",
    "txOpti = dataClean_without_splitting(tx_tr)\n",
    "print(\"shape of txOpti \", txOpti.shape)\n",
    "print(\"shape of y \", y_tr.shape)\n",
    "initial_w = np.zeros(txOpti.shape[1])\n",
    "max_iters = 100\n",
    "gamma = 0.5\n",
    "lambda_ = 0.5\n",
    "\n",
    "w, loss = reg_logistic_regression(y_tr, txOpti, lambda_, initial_w, max_iters, gamma)\n",
    "\n",
    "label = predict_logistic(w, txOpti)\n",
    "acc = calculate_accuracy(y_tr, label)\n",
    "\n",
    "print(\"end of the logistic_regression with w=\",w,\" and loss=\", loss)\n",
    "print(\"the accuracy on the train set is \", acc)\n",
    "\n",
    "teOpti = dataClean_without_splitting(tx_te)\n",
    "print(\"shape of teOpti \", teOpti.shape)\n",
    "\n",
    "y_pred = predict_logistic(w, teOpti)\n",
    "\n",
    "OUTPUT_PATH = 'sample-submission LR regression r=1.5, gamma,lambda=0.5'\n",
    "create_csv_submission(ids_te, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc9b9982-bb31-408a-86a5-c11433e7df5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.0, 1.0, 0.0, 3.0]\n"
     ]
    }
   ],
   "source": [
    "jet = tx_tr[:, 22]\n",
    "\n",
    "values = []\n",
    "\n",
    "for x in jet: \n",
    "    if x not in values: \n",
    "        values.append(x) \n",
    "\n",
    "print(values) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495a314e-ef65-46e5-93b3-e01f401ab78f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
