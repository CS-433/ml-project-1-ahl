{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "882a9018-5319-4a82-87e7-2b0d420475bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from implementations import *\n",
    "from helpers import *\n",
    "from helper_functions import *\n",
    "from optimization import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d56f0e3-9ee7-448f-a9dd-dd349209cc8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading the training dataset\n",
      "training dataset loaded\n"
     ]
    }
   ],
   "source": [
    "# load the training dataset\n",
    "print(\"loading the training dataset\")\n",
    "DATA_TRAIN_PATH = 'train.csv'\n",
    "y_tr, tx_tr, ids_tr = load_csv_data(DATA_TRAIN_PATH)\n",
    "print(\"training dataset loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af90503c-e3fa-4e1b-a222-63ef11b39a80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading the test dataset\n",
      "test dataset loaded\n"
     ]
    }
   ],
   "source": [
    "# load the test dataset\n",
    "print(\"loading the test dataset\")\n",
    "DATA_TEST_PATH = 'test.csv'\n",
    "y_te, tx_te, ids_te = load_csv_data(DATA_TEST_PATH)\n",
    "print(\"test dataset loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa8da103-df5b-4f1e-b595-9ae0cd91750e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shape of the training y is  (250000,)  and the training tx is  (250000, 30)\n",
      "the shape of the test y is  (568238,)  and the test tx is  (568238, 30)\n"
     ]
    }
   ],
   "source": [
    "# printing the shapes of the arrays\n",
    "print(\"the shape of the training y is \",y_tr.shape,\" and the training tx is \", tx_tr.shape)\n",
    "print(\"the shape of the test y is \",y_te.shape,\" and the test tx is \",tx_te.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6884129-6892-4ec8-bce6-fe61104b9ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing the mean square gd\n",
    "initial_w = np.zeros(tx_tr.shape[1])\n",
    "max_iters = 500\n",
    "gamma = 0.00008\n",
    "\n",
    "#w, loss = mean_squared_error_gd(y_tr, tx_tr, initial_w, max_iters, gamma)\n",
    "\n",
    "#print(\"end of the mean_squared_error_gd with w=\",w,\" and loss=\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3abc58ab-10d3-4a01-b62a-6e3445c65b5b",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "dataClean() missing 1 required positional argument: 'y'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# test least square\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m txOpti \u001b[38;5;241m=\u001b[39m \u001b[43mdataClean\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtx_tr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m w, loss \u001b[38;5;241m=\u001b[39m least_squares(y_tr, txOpti)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend of the least_squares with w=\u001b[39m\u001b[38;5;124m\"\u001b[39m,w,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m and loss=\u001b[39m\u001b[38;5;124m\"\u001b[39m, loss)\n",
      "\u001b[0;31mTypeError\u001b[0m: dataClean() missing 1 required positional argument: 'y'"
     ]
    }
   ],
   "source": [
    "# test least square\n",
    "txOpti = dataClean(tx_tr)\n",
    "w, loss = least_squares(y_tr, txOpti)\n",
    "print(\"end of the least_squares with w=\",w,\" and loss=\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1f09a81-21c9-4587-ba44-9bb03ab90776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test mean squared error sgd\n",
    "\n",
    "initial_w = np.zeros(tx_tr.shape[1])\n",
    "max_iters = 100\n",
    "gamma = 0.00001\n",
    "\n",
    "#w, loss = mean_squared_error_sgd(y_tr, tx_tr, initial_w, max_iters, gamma)\n",
    "\n",
    "#print(\"end of the mean_squared_error_sgd with w=\",w,\" and loss=\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34c8ae3d-d431-4939-8e10-d0915a0fc194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of txOpti  (250000, 23)\n",
      "Current lambda=1e-06, the loss=0.35012063246180924\n",
      "Current lambda=1.820689655172414e-05, the loss=0.3501206518793699\n",
      "Current lambda=3.541379310344827e-05, the loss=0.3501207027717671\n",
      "Current lambda=5.2620689655172415e-05, the loss=0.35012078407937985\n",
      "Current lambda=6.982758620689655e-05, the loss=0.3501208943399873\n",
      "Current lambda=8.703448275862068e-05, the loss=0.350121032126924\n",
      "Current lambda=0.00010424137931034483, the loss=0.3501211960819501\n",
      "Current lambda=0.00012144827586206897, the loss=0.3501213849177364\n",
      "Current lambda=0.0001386551724137931, the loss=0.35012159741548876\n",
      "Current lambda=0.00015586206896551724, the loss=0.35012183242154193\n",
      "Current lambda=0.00017306896551724137, the loss=0.3501220888437942\n",
      "Current lambda=0.0001902758620689655, the loss=0.3501223656482302\n",
      "Current lambda=0.00020748275862068967, the loss=0.35012266185560537\n",
      "Current lambda=0.0002246896551724138, the loss=0.3501229765383166\n",
      "Current lambda=0.00024189655172413794, the loss=0.35012330881745757\n",
      "Current lambda=0.00025910344827586207, the loss=0.3501236578600525\n",
      "Current lambda=0.00027631034482758623, the loss=0.35012402287646216\n",
      "Current lambda=0.0002935172413793104, the loss=0.350124403117947\n",
      "Current lambda=0.0003107241379310345, the loss=0.35012479787438394\n",
      "Current lambda=0.00032793103448275866, the loss=0.35012520647211903\n",
      "Current lambda=0.00034513793103448277, the loss=0.35012562827195565\n",
      "Current lambda=0.00036234482758620693, the loss=0.35012606266726143\n",
      "Current lambda=0.00037955172413793104, the loss=0.3501265090821912\n",
      "Current lambda=0.0003967586206896552, the loss=0.35012696697001744\n",
      "Current lambda=0.00041396551724137936, the loss=0.35012743581155753\n",
      "Current lambda=0.00043117241379310347, the loss=0.35012791511369745\n",
      "Current lambda=0.00044837931034482763, the loss=0.35012840440800086\n",
      "Current lambda=0.00046558620689655174, the loss=0.3501289032493984\n",
      "Current lambda=0.0004827931034482759, the loss=0.35012941121495667\n",
      "Current lambda=0.0005, the loss=0.35012992790271524\n",
      "best lambda is  1e-06\n",
      "end of the ridge_regression with w= [-6.39472599e-01 -3.06548796e-01  7.21159032e-02 -1.63641568e-02\n",
      "  1.22515895e-01 -8.31319174e-02  3.05435989e-02 -1.02545421e-01\n",
      "  1.35997850e-01  2.44018911e-01 -1.71358026e-03 -1.69917173e-03\n",
      "  9.38150191e-02 -3.50573132e-04  2.17345682e-03  7.71334399e-02\n",
      "  5.45846163e-04  5.82734292e-03 -7.46010005e-02  2.46202321e-01\n",
      " -1.29631022e-01 -1.08994335e-01  1.05212777e-01]  and loss= 0.35012063246180924\n"
     ]
    }
   ],
   "source": [
    "# test ridge regression \n",
    "\n",
    "txOpti = dataClean(tx_tr)\n",
    "print(\"shape of txOpti \", txOpti.shape)\n",
    "\n",
    "lambda_ = best_lambda(y_tr, txOpti, 0.000001, 0.0005, 30)\n",
    "print(\"best lambda is \",lambda_)\n",
    "w, loss = ridge_regression(y_tr, txOpti, lambda_)\n",
    "\n",
    "#y_pred = predict(w, tx_te)\n",
    "#y_pred[y_pred==0] = -1\n",
    "\n",
    "#OUTPUT_PATH = 'sample-submission'\n",
    "#create_csv_submission(ids_te, y_pred, OUTPUT_PATH)\n",
    "\n",
    "print(\"end of the ridge_regression with w=\",w,\" and loss=\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5f5c7de-0c97-4975-93cb-71f4916eb96e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ml-project-1-ahl/optimization.py:28: RuntimeWarning: invalid value encountered in true_divide\n",
      "  standardize_dataset = (dataset - mean)/std\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of txOpti  (99913, 20)\n",
      "shape of yOpti  (99913, 20)\n",
      "end of the logistic_regression with w= [            nan             nan             nan             nan\n",
      "             nan             nan             nan             nan\n",
      "             nan             nan             nan             nan\n",
      "             nan             nan             nan             nan\n",
      "             nan             nan             nan             nan\n",
      " -2.20216529e+01 -8.90837356e-02 -9.71201484e+00  2.39131652e+00\n",
      "  1.22062735e+00  3.17727480e+00  8.06233589e-01  1.76831964e+00\n",
      " -4.14612204e+00  7.47326497e+00  4.32873868e+00 -4.70646320e-02\n",
      " -2.00705675e-01 -5.23762154e-01 -2.76219476e-01  2.01579973e-02\n",
      " -1.26157926e+00  2.46825128e-01  1.34874504e+00  1.31249739e+00\n",
      " -3.65928336e-02 -8.78085453e-02  1.31249769e+00 -1.42210262e+01\n",
      "  3.41739337e-01 -6.39694330e+00  4.15496098e+00  2.61553710e+00\n",
      "  4.10803161e+00  2.62587228e+00 -3.07700810e+00  1.67921390e+00\n",
      " -3.22844434e+00  1.00224940e+00 -2.76880396e+00  6.24517071e+00\n",
      "  5.73427989e+00  3.79960332e+00 -1.90991517e-01 -9.98874468e-02\n",
      " -2.21495392e-01 -2.57655540e-01  4.22807754e-02  6.89603811e-01\n",
      "  2.76868286e-01 -8.99479141e-01  5.86738696e-01  1.18936254e-01\n",
      "  1.72281079e-01  1.66318061e+00  1.15051380e-01 -1.84587189e-01\n",
      "  9.91082112e-01 -2.36866857e+01  5.46792603e-01 -6.94445725e+00\n",
      "  2.54365421e+00  4.19438661e+00  1.11412633e+00  1.46806096e-01\n",
      " -1.37722570e+00 -6.95148771e-01 -1.34340325e+00 -6.78800293e-01\n",
      " -3.08746564e+00  6.82745695e+00  3.93938066e+00  3.59843043e+00\n",
      " -4.64526927e-01  4.38656354e-03 -2.71167258e-02 -3.84003036e-01\n",
      "  6.92276302e-02  1.45832025e+00  2.80584717e-01 -8.94069013e-01\n",
      " -5.15021310e-01 -8.56609795e-02 -1.58948555e-01 -7.48795489e-01\n",
      "  2.09035755e-01  3.35695287e-02 -1.18511190e+00]  and loss= nan\n",
      "the accurcy on the train set is  0.36205762174507644\n"
     ]
    }
   ],
   "source": [
    "# test logistic regression\n",
    "\n",
    "txOpti, yOpti = dataClean(tx_tr, y_tr)\n",
    "print(\"shape of txOpti \", txOpti[0].shape)\n",
    "print(\"shape of yOpti \", txOpti[0].shape)\n",
    "#initial_w = np.zeros(txOpti.shape[1])\n",
    "# initial_w = w\n",
    "max_iters = 100\n",
    "gamma = 0.5\n",
    "ws = []\n",
    "losses = []\n",
    "y_pred = []\n",
    "accArray = []\n",
    "\n",
    "for i in range(4): \n",
    "    initial_w = np.zeros(txOpti[i].shape[1])\n",
    "    w, loss = logistic_regression(yOpti[i], txOpti[i], initial_w, max_iters, gamma)\n",
    "    label = predict(w, txOpti[i])\n",
    "    acc = calculate_accuracy(yOpti[i], label)\n",
    "    \n",
    "    ws.append(w)\n",
    "    losses.append(loss)\n",
    "    y_pred.append(label)\n",
    "    accArray.append(acc)\n",
    "\n",
    "tot_acc = accArray[0] + accArray[1] + accArray[2] + accArray[3]\n",
    "weights = np.array(list(ws[0])+list(ws[1])+list(ws[2])+list(ws[3]))\n",
    "loss = np.mean(losses)\n",
    "    \n",
    "#teOpti = dataClean(tx_te)\n",
    "#print(\"shape of teOpti \", teOpti.shape)\n",
    "\n",
    "#y_pred = predict(w, teOpti)\n",
    "\n",
    "#OUTPUT_PATH = 'sample-submission LR 2'\n",
    "#create_csv_submission(ids_te, y_pred, OUTPUT_PATH)\n",
    "\n",
    "print(\"end of the logistic_regression with w=\",weights,\" and loss=\", loss)\n",
    "print(\"the accurcy on the train set is \", tot_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1c71eed-4b5e-41af-b0b6-57d490347155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of txOpti  (250000, 24)\n",
      "shape of yOpti  (250000,)\n",
      "Preiteration, the loss=1.062475479906441, the grad=1.0756792916943188\n",
      "Current iteration=0, the loss=0.7256354092804485, the grad=1.0756792916943188\n",
      "Current iteration=1, the loss=0.4705976133937675, the grad=0.8419744841458995\n",
      "Current iteration=2, the loss=0.24799236227236826, the grad=0.7537390016028716\n",
      "Current iteration=3, the loss=0.042029399144002194, the grad=0.700778868460077\n",
      "Current iteration=4, the loss=-0.15389257918377303, the grad=0.665921703048163\n",
      "Current iteration=5, the loss=-0.3432155802387365, the grad=0.6420985296487979\n",
      "Current iteration=6, the loss=-0.5279525230316716, the grad=0.6253187245788955\n",
      "Current iteration=7, the loss=-0.7093609414006918, the grad=0.6131624568123972\n",
      "Current iteration=8, the loss=-0.8882637522758007, the grad=0.604118106655686\n",
      "Current iteration=9, the loss=-1.0652205812557705, the grad=0.597219921899926\n",
      "Current iteration=10, the loss=-1.2406254032124715, the grad=0.5918377448840968\n",
      "Current iteration=11, the loss=-1.4147645274609904, the grad=0.5875516630959421\n",
      "Current iteration=12, the loss=-1.587852124378616, the grad=0.5840758861946834\n",
      "Current iteration=13, the loss=-1.760052597917662, the grad=0.5812117674030524\n",
      "Current iteration=14, the loss=-1.931495048925011, the grad=0.5788183502995784\n",
      "Current iteration=15, the loss=-2.102282869830661, the grad=0.5767936015267373\n",
      "Current iteration=16, the loss=-2.27250027343772, the grad=0.5750622502799619\n",
      "Current iteration=17, the loss=-2.4422168467361036, the grad=0.573567772233021\n",
      "Current iteration=18, the loss=-2.611490803235224, the grad=0.5722670130216022\n",
      "Current iteration=19, the loss=-2.780371358043835, the grad=0.5711265194233666\n",
      "Current iteration=20, the loss=-2.9489004983981078, the grad=0.5701199931394805\n",
      "Current iteration=21, the loss=-3.1171143285540586, the grad=0.569226494462086\n",
      "Current iteration=22, the loss=-3.2850441088427407, the grad=0.5684291548601395\n",
      "Current iteration=23, the loss=-3.4527170707250168, the grad=0.567714240348636\n",
      "Current iteration=24, the loss=-3.6201570648483714, the grad=0.567070460309083\n",
      "Current iteration=25, the loss=-3.7873850825580293, the grad=0.5664884505638568\n",
      "Current iteration=26, the loss=-3.9544196800822378, the grad=0.565960381885279\n",
      "Current iteration=27, the loss=-4.121277326851625, the grad=0.5654796599962635\n",
      "Current iteration=28, the loss=-4.2879726939587055, the grad=0.5650406931441153\n",
      "Current iteration=29, the loss=-4.454518894867569, the grad=0.5646387101750806\n",
      "Current iteration=30, the loss=-4.620927687656707, the grad=0.5642696167732144\n",
      "Current iteration=31, the loss=-4.787209645996769, the grad=0.5639298808444562\n",
      "Current iteration=32, the loss=-4.953374304511737, the grad=0.563616440378415\n",
      "Current iteration=33, the loss=-5.1194302829981675, the grad=0.5633266288066694\n",
      "Current iteration=34, the loss=-5.285385393079467, the grad=0.563058114098893\n",
      "Current iteration=35, the loss=-5.451246730178386, the grad=0.5628088487337625\n",
      "Current iteration=36, the loss=-5.617020753149325, the grad=0.5625770283443088\n",
      "Current iteration=37, the loss=-5.782713353485437, the grad=0.5623610573323994\n",
      "Current iteration=38, the loss=-5.94832991567654, the grad=0.5621595201201898\n",
      "Current iteration=39, the loss=-6.113875370022611, the grad=0.5619711569900117\n",
      "Current iteration=40, the loss=-6.27935423898889, the grad=0.5617948436815736\n",
      "Current iteration=41, the loss=-6.444770678011085, the grad=0.5616295740832425\n",
      "Current iteration=42, the loss=-6.610128511514278, the grad=0.5614744454848044\n",
      "Current iteration=43, the loss=-6.775431264790127, the grad=0.5613286459614174\n",
      "Current iteration=44, the loss=-6.940682192278726, the grad=0.5611914435391865\n",
      "Current iteration=45, the loss=-7.105884302719997, the grad=0.5610621768568026\n",
      "Current iteration=46, the loss=-7.27104038157164, the grad=0.5609402470888168\n",
      "Current iteration=47, the loss=-7.436153011033802, the grad=0.560825110937146\n",
      "Current iteration=48, the loss=-7.601224587972928, the grad=0.5607162745305252\n",
      "Current iteration=49, the loss=-7.766257339996951, the grad=0.5606132880984909\n",
      "Current iteration=50, the loss=-7.931253339899886, the grad=0.5605157413083697\n",
      "Current iteration=51, the loss=-8.096214518664988, the grad=0.560423259171698\n",
      "Current iteration=52, the loss=-8.261142677190872, the grad=0.5603354984412391\n",
      "Current iteration=53, the loss=-8.426039496883957, the grad=0.5602521444319556\n",
      "Current iteration=54, the loss=-8.590906549242508, the grad=0.5601729082094116\n",
      "Current iteration=55, the loss=-8.755745304541913, the grad=0.560097524097488\n",
      "Current iteration=56, the loss=-8.920557139717545, the grad=0.5600257474643331\n",
      "Current iteration=57, the loss=-9.085343345529907, the grad=0.5599573527513704\n",
      "Current iteration=58, the loss=-9.250105133086675, the grad=0.5598921317151504\n",
      "Current iteration=59, the loss=-9.414843639787616, the grad=0.5598298918560282\n",
      "Current iteration=60, the loss=-9.579559934750733, the grad=0.5597704550111907\n",
      "Current iteration=61, the loss=-9.744255023771325, the grad=0.5597136560925834\n",
      "Current iteration=62, the loss=-9.908929853859938, the grad=0.559659341952853\n",
      "Current iteration=63, the loss=-10.073585317400036, the grad=0.5596073703646113\n",
      "Current iteration=64, the loss=-10.238222255961825, the grad=0.5595576091002159\n",
      "Current iteration=65, the loss=-10.402841463804712, the grad=0.5595099351008687\n",
      "Current iteration=66, the loss=-10.567443691097495, the grad=0.5594642337252336\n",
      "Current iteration=67, the loss=-10.732029646882182, the grad=0.5594203980689639\n",
      "Current iteration=68, the loss=-10.8966000018049, the grad=0.5593783283475777\n",
      "Current iteration=69, the loss=-11.061155390634708, the grad=0.5593379313360147\n",
      "Current iteration=70, the loss=-11.225696414589178, the grad=0.5592991198589846\n",
      "Current iteration=71, the loss=-11.39022364348377, the grad=0.5592618123269145\n",
      "Current iteration=72, the loss=-11.554737617720049, the grad=0.5592259323128747\n",
      "Current iteration=73, the loss=-11.719238850126876, the grad=0.559191408166403\n",
      "Current iteration=74, the loss=-11.883727827666688, the grad=0.559158172660584\n",
      "Current iteration=75, the loss=-12.04820501301837, the grad=0.5591261626691512\n",
      "Current iteration=76, the loss=-12.212670846046882, the grad=0.559095318870728\n",
      "Current iteration=77, the loss=-12.377125745168842, the grad=0.5590655854776269\n",
      "Current iteration=78, the loss=-12.541570108622555, the grad=0.559036909986907\n",
      "Current iteration=79, the loss=-12.706004315650116, the grad=0.5590092429516227\n",
      "Current iteration=80, the loss=-12.870428727598544, the grad=0.558982537770418\n",
      "Current iteration=81, the loss=-13.034843688946255, the grad=0.5589567504938001\n",
      "Current iteration=82, the loss=-13.199249528260745, the grad=0.5589318396456012\n",
      "Current iteration=83, the loss=-13.363646559092633, the grad=0.5589077660582825\n",
      "Current iteration=84, the loss=-13.528035080811057, the grad=0.5588844927208695\n",
      "Current iteration=85, the loss=-13.692415379384617, the grad=0.5588619846384237\n",
      "Current iteration=86, the loss=-13.856787728112138, the grad=0.5588402087020599\n",
      "Current iteration=87, the loss=-14.021152388306794, the grad=0.5588191335686182\n",
      "Current iteration=88, the loss=-14.185509609937004, the grad=0.558798729549181\n",
      "Current iteration=89, the loss=-14.349859632227336, the grad=0.558778968505696\n",
      "Current iteration=90, the loss=-14.514202684222099, the grad=0.5587598237550447\n",
      "Current iteration=91, the loss=-14.678538985314422, the grad=0.5587412699799467\n",
      "Current iteration=92, the loss=-14.842868745743159, the grad=0.5587232831461527\n",
      "Current iteration=93, the loss=-15.00719216705985, the grad=0.5587058404254212\n",
      "Current iteration=94, the loss=-15.171509442567853, the grad=0.5586889201238256\n",
      "Current iteration=95, the loss=-15.335820757735467, the grad=0.5586725016149733\n",
      "Current iteration=96, the loss=-15.500126290584866, the grad=0.5586565652777558\n",
      "Current iteration=97, the loss=-15.664426212058482, the grad=0.558641092438285\n",
      "Current iteration=98, the loss=-15.828720686364184, the grad=0.5586260653156935\n",
      "Current iteration=99, the loss=-15.993009871300869, the grad=0.5586114669715153\n",
      "Current iteration=100, the loss=-16.157293918565557, the grad=0.5585972812623676\n",
      "Current iteration=101, the loss=-16.321572974043296, the grad=0.5585834927957052\n",
      "Current iteration=102, the loss=-16.485847178080896, the grad=0.5585700868884098\n",
      "Current iteration=103, the loss=-16.650116665745585, the grad=0.5585570495280159\n",
      "Current iteration=104, the loss=-16.814381567069535, the grad=0.5585443673363788\n",
      "Current iteration=105, the loss=-16.978642007281056, the grad=0.5585320275356161\n",
      "Current iteration=106, the loss=-17.1428981070234, the grad=0.5585200179161564\n",
      "Current iteration=107, the loss=-17.30714998256184, the grad=0.5585083268067516\n",
      "Current iteration=108, the loss=-17.47139774597982, the grad=0.5584969430463141\n",
      "Current iteration=109, the loss=-17.635641505364774, the grad=0.5584858559574565\n",
      "Current iteration=110, the loss=-17.79988136498423, the grad=0.5584750553216107\n",
      "Current iteration=111, the loss=-17.964117425452848, the grad=0.5584645313556291\n",
      "Current iteration=112, the loss=-18.12834978389086, the grad=0.5584542746897595\n",
      "Current iteration=113, the loss=-18.292578534074423, the grad=0.5584442763469083\n",
      "Current iteration=114, the loss=-18.4568037665784, the grad=0.5584345277231031\n",
      "Current iteration=115, the loss=-18.621025568911943, the grad=0.5584250205690775\n",
      "Current iteration=116, the loss=-18.785244025647362, the grad=0.5584157469729084\n",
      "Current iteration=117, the loss=-18.94945921854255, the grad=0.5584066993436291\n",
      "Current iteration=118, the loss=-19.11367122665751, the grad=0.5583978703957672\n",
      "Current iteration=119, the loss=-19.277880126465067, the grad=0.5583892531347383\n",
      "Current iteration=120, the loss=-19.44208599195637, the grad=0.5583808408430512\n",
      "Current iteration=121, the loss=-19.606288894741173, the grad=0.5583726270672629\n",
      "Current iteration=122, the loss=-19.770488904143527, the grad=0.558364605605647\n",
      "Current iteration=123, the loss=-19.93468608729269, the grad=0.5583567704965272\n",
      "Current iteration=124, the loss=-20.09888050921002, the grad=0.5583491160072329\n",
      "Current iteration=125, the loss=-20.26307223289162, the grad=0.5583416366236467\n",
      "Current iteration=126, the loss=-20.427261319387274, the grad=0.5583343270402957\n",
      "Current iteration=127, the loss=-20.591447827875662, the grad=0.5583271821509698\n",
      "Current iteration=128, the loss=-20.755631815736184, the grad=0.558320197039824\n",
      "Current iteration=129, the loss=-20.919813338617516, the grad=0.5583133669729378\n",
      "Current iteration=130, the loss=-21.083992450503082, the grad=0.5583066873903096\n",
      "Current iteration=131, the loss=-21.248169203773532, the grad=0.5583001538982612\n",
      "Current iteration=132, the loss=-21.41234364926653, the grad=0.5582937622622175\n",
      "Current iteration=133, the loss=-21.576515836333865, the grad=0.5582875083998589\n",
      "Current iteration=134, the loss=-21.74068581289601, the grad=0.5582813883746044\n",
      "Current iteration=135, the loss=-21.904853625494315, the grad=0.5582753983894252\n",
      "Current iteration=136, the loss=-22.069019319341063, the grad=0.5582695347809569\n",
      "Current iteration=137, the loss=-22.23318293836714, the grad=0.5582637940139017\n",
      "Current iteration=138, the loss=-22.397344525267872, the grad=0.5582581726757005\n",
      "Current iteration=139, the loss=-22.561504121546825, the grad=0.5582526674714616\n",
      "Current iteration=140, the loss=-22.725661767557746, the grad=0.5582472752191309\n",
      "Current iteration=141, the loss=-22.889817502544766, the grad=0.5582419928448936\n",
      "Current iteration=142, the loss=-23.05397136468095, the grad=0.5582368173787912\n",
      "Current iteration=143, the loss=-23.218123391105195, the grad=0.5582317459505431\n",
      "Current iteration=144, the loss=-23.382273617957694, the grad=0.5582267757855629\n",
      "Current iteration=145, the loss=-23.546422080413805, the grad=0.5582219042011626\n",
      "Current iteration=146, the loss=-23.710568812716755, the grad=0.5582171286029238\n",
      "Current iteration=147, the loss=-23.87471384820879, the grad=0.5582124464812419\n",
      "Current iteration=148, the loss=-24.038857219361258, the grad=0.5582078554080233\n",
      "Current iteration=149, the loss=-24.20299895780342, the grad=0.5582033530335306\n",
      "Current iteration=150, the loss=-24.367139094350073, the grad=0.5581989370833713\n",
      "Current iteration=151, the loss=-24.531277659028202, the grad=0.5581946053556209\n",
      "Current iteration=152, the loss=-24.695414681102466, the grad=0.5581903557180691\n",
      "Current iteration=153, the loss=-24.859550189099792, the grad=0.558186186105592\n",
      "Current iteration=154, the loss=-25.023684210832936, the grad=0.558182094517637\n",
      "Current iteration=155, the loss=-25.187816773423197, the grad=0.5581780790158156\n",
      "Current iteration=156, the loss=-25.351947903322234, the grad=0.5581741377216015\n",
      "Current iteration=157, the loss=-25.516077626333082, the grad=0.5581702688141276\n",
      "Current iteration=158, the loss=-25.680205967630343, the grad=0.5581664705280764\n",
      "Current iteration=159, the loss=-25.844332951779617, the grad=0.558162741151659\n",
      "Current iteration=160, the loss=-26.00845860275626, the grad=0.5581590790246812\n",
      "Current iteration=161, the loss=-26.17258294396339, the grad=0.558155482536689\n",
      "Current iteration=162, the loss=-26.33670599824925, the grad=0.558151950125191\n",
      "Current iteration=163, the loss=-26.500827787923956, the grad=0.5581484802739561\n",
      "Current iteration=164, the loss=-26.664948334775605, the grad=0.5581450715113786\n",
      "Current iteration=165, the loss=-26.829067660085794, the grad=0.5581417224089147\n",
      "Current iteration=166, the loss=-26.993185784644616, the grad=0.5581384315795754\n",
      "Current iteration=167, the loss=-27.15730272876514, the grad=0.5581351976764894\n",
      "Current iteration=168, the loss=-27.32141851229725, the grad=0.5581320193915147\n",
      "Current iteration=169, the loss=-27.485533154641182, the grad=0.5581288954539121\n",
      "Current iteration=170, the loss=-27.6496466747604, the grad=0.5581258246290693\n",
      "Current iteration=171, the loss=-27.813759091194175, the grad=0.5581228057172749\n",
      "Current iteration=172, the loss=-27.977870422069632, the grad=0.5581198375525416\n",
      "Current iteration=173, the loss=-28.14198068511342, the grad=0.558116919001474\n",
      "Current iteration=174, the loss=-28.306089897662957, the grad=0.5581140489621829\n",
      "Current iteration=175, the loss=-28.470198076677327, the grad=0.5581112263632382\n",
      "Current iteration=176, the loss=-28.634305238747785, the grad=0.558108450162666\n",
      "Current iteration=177, the loss=-28.798411400107902, the grad=0.5581057193469802\n",
      "Current iteration=178, the loss=-28.962516576643367, the grad=0.5581030329302532\n",
      "Current iteration=179, the loss=-29.12662078390152, the grad=0.5581003899532225\n",
      "Current iteration=180, the loss=-29.290724037100453, the grad=0.5580977894824277\n",
      "Current iteration=181, the loss=-29.45482635113794, the grad=0.5580952306093838\n",
      "Current iteration=182, the loss=-29.618927740599993, the grad=0.5580927124497814\n",
      "Current iteration=183, the loss=-29.783028219769143, the grad=0.5580902341427205\n",
      "Current iteration=184, the loss=-29.947127802632508, the grad=0.5580877948499693\n",
      "Current iteration=185, the loss=-30.11122650288953, the grad=0.5580853937552509\n",
      "Current iteration=186, the loss=-30.275324333959517, the grad=0.5580830300635574\n",
      "Current iteration=187, the loss=-30.439421308988916, the grad=0.5580807030004884\n",
      "Current iteration=188, the loss=-30.60351744085837, the grad=0.5580784118116119\n",
      "Current iteration=189, the loss=-30.76761274218955, the grad=0.5580761557618493\n",
      "Current iteration=190, the loss=-30.93170722535176, the grad=0.5580739341348828\n",
      "Current iteration=191, the loss=-31.09580090246839, the grad=0.5580717462325838\n",
      "Current iteration=192, the loss=-31.25989378542302, the grad=0.5580695913744599\n",
      "Current iteration=193, the loss=-31.423985885865605, the grad=0.5580674688971234\n",
      "Current iteration=194, the loss=-31.588077215218167, the grad=0.5580653781537774\n",
      "Current iteration=195, the loss=-31.752167784680474, the grad=0.5580633185137197\n",
      "Current iteration=196, the loss=-31.91625760523561, the grad=0.5580612893618652\n",
      "Current iteration=197, the loss=-32.08034668765526, the grad=0.5580592900982818\n",
      "Current iteration=198, the loss=-32.244435042504776, the grad=0.5580573201377471\n",
      "Current iteration=199, the loss=-32.40852268014832, the grad=0.5580553789093149\n",
      "end of the logistic_regression with w= [-47.87480055  -0.54365897 -21.3432314    5.90477912   3.98159609\n",
      "   6.93776275  -2.41089241   3.13402422  -9.32387889  11.40457558\n",
      "   9.41252324  -0.19817511  -0.1227285   -1.08174286  -0.08473689\n",
      "   0.28977193  -4.21708423   0.43087411   2.48761858   1.59060884\n",
      "   3.37932527   1.36963403   1.5340857    3.34509628]  and loss= -32.40852268014832\n",
      "the accuracy on the train set is  0.704748\n"
     ]
    }
   ],
   "source": [
    "# test logistic regression without splitting \n",
    "txOpti = dataClean_without_splitting(tx_tr)\n",
    "print(\"shape of txOpti \", txOpti.shape)\n",
    "print(\"shape of yOpti \", y_tr.shape)\n",
    "initial_w = np.zeros(txOpti.shape[1])\n",
    "max_iters = 200\n",
    "gamma = 0.5\n",
    "\n",
    "w, loss = logistic_regression(y_tr, txOpti, initial_w, max_iters, gamma)\n",
    "\n",
    "label = predict_logistic(w, txOpti)\n",
    "acc = calculate_accuracy(y_tr, label)\n",
    "\n",
    "print(\"end of the logistic_regression with w=\",w,\" and loss=\", loss)\n",
    "print(\"the accuracy on the train set is \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0b54129f-2e9e-489f-989e-cffe093f018d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[105.012 46.524 73.752 38.468 -999.000 -999.000 -999.000 2.492 12.316\n",
      " 120.665 1.280 -0.356 -999.000 31.804 -0.023 -0.033 40.516 -0.045 0.086\n",
      " 34.802 -0.024 179.739 1.000 38.960 -1.872 -2.093 -999.000 -999.000\n",
      " -999.000 40.513]\n",
      "[-49.023 49.240 81.182 57.896 -708.421 -601.237 -709.357 2.373 18.917\n",
      " 158.432 1.438 -0.128 -708.985 38.707 -0.011 -0.008 46.660 -0.020 0.044\n",
      " 41.717 -0.010 209.797 0.979 -348.330 -399.254 -399.260 -692.381 -709.122\n",
      " -709.119 73.065]\n",
      "(array([ 4,  5,  6, 12, 26, 27, 28]),)\n",
      "index : 4\n",
      "177457\n",
      "index : 5\n",
      "177457\n",
      "index : 6\n",
      "177457\n",
      "index : 12\n",
      "177457\n",
      "index : 26\n",
      "177457\n",
      "index : 27\n",
      "177457\n",
      "index : 28\n",
      "177457\n",
      "counts of each column  [177457, 177457, 177457, 177457, 177457, 177457, 177457]\n"
     ]
    }
   ],
   "source": [
    "#clean column , remove column that has only -999 elements. \n",
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.3f}\".format(x)})\n",
    "\n",
    "median = np.median(tx_tr, axis=0) \n",
    "mean = np.mean(tx_tr, axis=0) \n",
    "\n",
    "print(median)\n",
    "print(mean)\n",
    "\n",
    "indx = np.where(median == -999)\n",
    "print(indx)\n",
    "\n",
    "counts = []\n",
    "count = 0\n",
    "\n",
    "for i in indx[0]: \n",
    "    count = np.count_nonzero( tx_tr[:,i] == -999)\n",
    "    print(\"index :\", i)\n",
    "    print(count)\n",
    "    counts.append(count)\n",
    "\n",
    "print(\"counts of each column \", counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cd948ae1-5376-46f3-b929-6d5d233f0bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7,)\n",
      "(250000, 30)\n",
      "(250000, 23)\n"
     ]
    }
   ],
   "source": [
    "median = np.median(tx_tr, axis=0) \n",
    "indx = np.where(median == -999)\n",
    "tx1 = np.delete(tx_tr, indx[0], 1)\n",
    "print(indx[0].shape)\n",
    "print(tx_tr.shape)\n",
    "print(tx1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f062a94a-ff3d-4d4d-88b2-66b0ca8bf65c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of txOpti  (250000, 23)\n",
      "end of the logistic_regression with w= [ 4.50300842e-03 -6.62757642e-03 -2.65297946e-04  3.59712971e-03\n",
      "  2.58058701e-04 -3.09882051e-04  2.85152955e-03 -3.69117823e-03\n",
      "  5.10551184e-03  4.42795209e-03 -1.83273093e-05 -8.34744891e-05\n",
      " -6.11568648e-04  2.75644738e-05  7.80817587e-05  4.03966168e-04\n",
      "  1.40670181e-04  2.51657143e-03  2.47748581e-03  2.93239003e-03\n",
      "  2.79926559e-03  2.79924277e-03  2.49137767e-03]  and loss= 1.0562025575989353\n"
     ]
    }
   ],
   "source": [
    "# test regularized logistic regression\n",
    "txOpti, yOpti = dataClean(tx_tr)\n",
    "print(\"shape of txOpti \", txOpti.shape)\n",
    "initial_w = np.zeros(txOpti.shape[1])\n",
    "# initial_w = w\n",
    "max_iters = 200\n",
    "gamma = 0.0001\n",
    "lambda_ = 0.0000001\n",
    "\n",
    "w, loss = reg_logistic_regression(y_tr, txOpti, lambda_, initial_w, max_iters, gamma)\n",
    "\n",
    "print(\"end of the logistic_regression with w=\",w,\" and loss=\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc9b9982-bb31-408a-86a5-c11433e7df5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.0, 1.0, 0.0, 3.0]\n"
     ]
    }
   ],
   "source": [
    "jet = tx_tr[:, 22]\n",
    "\n",
    "values = []\n",
    "\n",
    "for x in jet: \n",
    "    if x not in values: \n",
    "        values.append(x) \n",
    "\n",
    "print(values) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495a314e-ef65-46e5-93b3-e01f401ab78f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
