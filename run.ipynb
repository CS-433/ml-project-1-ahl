{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "882a9018-5319-4a82-87e7-2b0d420475bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from implementations import *\n",
    "from helpers import *\n",
    "from helper_functions import *\n",
    "from optimization import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d56f0e3-9ee7-448f-a9dd-dd349209cc8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading the training dataset\n",
      "training dataset loaded\n"
     ]
    }
   ],
   "source": [
    "# load the training dataset\n",
    "print(\"loading the training dataset\")\n",
    "DATA_TRAIN_PATH = 'train.csv'\n",
    "y_tr, tx_tr, ids_tr = load_csv_data(DATA_TRAIN_PATH)\n",
    "print(\"training dataset loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af90503c-e3fa-4e1b-a222-63ef11b39a80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading the test dataset\n",
      "test dataset loaded\n"
     ]
    }
   ],
   "source": [
    "# load the test dataset\n",
    "print(\"loading the test dataset\")\n",
    "DATA_TEST_PATH = 'test.csv'\n",
    "y_te, tx_te, ids_te = load_csv_data(DATA_TEST_PATH)\n",
    "print(\"test dataset loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa8da103-df5b-4f1e-b595-9ae0cd91750e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shape of the training y is  (250000,)  and the training tx is  (250000, 30)\n",
      "the shape of the test y is  (568238,)  and the test tx is  (568238, 30)\n"
     ]
    }
   ],
   "source": [
    "# printing the shapes of the arrays\n",
    "print(\"the shape of the training y is \",y_tr.shape,\" and the training tx is \", tx_tr.shape)\n",
    "print(\"the shape of the test y is \",y_te.shape,\" and the test tx is \",tx_te.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6884129-6892-4ec8-bce6-fe61104b9ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing the mean square gd\n",
    "initial_w = np.zeros(tx_tr.shape[1])\n",
    "max_iters = 500\n",
    "gamma = 0.00008\n",
    "\n",
    "#w, loss = mean_squared_error_gd(y_tr, tx_tr, initial_w, max_iters, gamma)\n",
    "\n",
    "#print(\"end of the mean_squared_error_gd with w=\",w,\" and loss=\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3abc58ab-10d3-4a01-b62a-6e3445c65b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end of the least_squares with w= [ 8.03494312e-05 -7.20202273e-03 -6.05417273e-03 -5.47559065e-04\n",
      " -1.93874700e-02  4.73451621e-04 -2.60379054e-02  3.25106300e-01\n",
      " -3.80780282e-05 -2.72724919e+00 -2.21220140e-01  9.50794091e-02\n",
      "  6.40351613e-02  2.73550887e+00 -3.31801241e-04 -9.54325120e-04\n",
      "  2.74026561e+00 -5.34164891e-04  9.73498581e-04  3.69225052e-03\n",
      "  3.54487449e-04 -5.43344598e-04 -3.30448035e-01 -1.40800498e-03\n",
      "  8.31432888e-04  1.02117272e-03 -1.68047416e-03 -5.83664818e-03\n",
      " -1.11087997e-02  2.72770912e+00]  and loss= 0.3396868094770935\n"
     ]
    }
   ],
   "source": [
    "# test least square\n",
    "w, loss = least_squares(y_tr, tx_tr)\n",
    "print(\"end of the least_squares with w=\",w,\" and loss=\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1f09a81-21c9-4587-ba44-9bb03ab90776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test mean squared error sgd\n",
    "\n",
    "initial_w = np.zeros(tx_tr.shape[1])\n",
    "max_iters = 100\n",
    "gamma = 0.00001\n",
    "\n",
    "#w, loss = mean_squared_error_sgd(y_tr, tx_tr, initial_w, max_iters, gamma)\n",
    "\n",
    "#print(\"end of the mean_squared_error_sgd with w=\",w,\" and loss=\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34c8ae3d-d431-4939-8e10-d0915a0fc194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current lambda=0.0001, the loss=0.33968798676609097\n",
      "Current lambda=0.00013738237958832623, the loss=0.33968803954866844\n",
      "Current lambda=0.00018873918221350977, the loss=0.33968813859041375\n",
      "Current lambda=0.0002592943797404667, the loss=0.3396883245147402\n",
      "Current lambda=0.0003562247890262444, the loss=0.3396886731920951\n",
      "Current lambda=0.0004893900918477494, the loss=0.3396893258059021\n",
      "Current lambda=0.0006723357536499335, the loss=0.33969054373021085\n",
      "Current lambda=0.0009236708571873865, the loss=0.33969280743924646\n",
      "Current lambda=0.0012689610031679222, the loss=0.3396969916896993\n",
      "Current lambda=0.0017433288221999873, the loss=0.33970466833847507\n",
      "Current lambda=0.002395026619987486, the loss=0.3397186122395501\n",
      "Current lambda=0.0032903445623126675, the loss=0.33974360637308687\n",
      "Current lambda=0.004520353656360241, the loss=0.33978763536530615\n",
      "Current lambda=0.006210169418915616, the loss=0.3398634706893294\n",
      "Current lambda=0.008531678524172805, the loss=0.33999040783666146\n",
      "Current lambda=0.011721022975334805, the loss=0.34019545283472974\n",
      "Current lambda=0.01610262027560939, the loss=0.3405126532289999\n",
      "Current lambda=0.02212216291070448, the loss=0.3409789456162246\n",
      "Current lambda=0.03039195382313198, the loss=0.34162564114876065\n",
      "Current lambda=0.041753189365604, the loss=0.34246709117086294\n",
      "Current lambda=0.05736152510448681, the loss=0.34349132395827114\n",
      "Current lambda=0.07880462815669913, the loss=0.34465867526889005\n",
      "Current lambda=0.1082636733874054, the loss=0.34591102937226975\n",
      "Current lambda=0.14873521072935117, the loss=0.34718767370275866\n",
      "Current lambda=0.20433597178569418, the loss=0.34843939327798257\n",
      "Current lambda=0.2807216203941176, the loss=0.3496342050611894\n",
      "Current lambda=0.38566204211634725, the loss=0.35075424601787725\n",
      "Current lambda=0.5298316906283708, the loss=0.351788603311994\n",
      "Current lambda=0.7278953843983146, the loss=0.35272780782090674\n",
      "Current lambda=1.0, the loss=0.3535627339505689\n",
      "best lambda is  0.0001\n",
      "end of the ridge_regression with w= [ 8.06960854e-05 -7.20578084e-03 -6.04517159e-03 -5.52739531e-04\n",
      " -1.94754484e-02  4.73682819e-04 -2.60450820e-02  3.24395228e-01\n",
      " -3.82847694e-05  4.40588717e-03 -2.20862099e-01  9.50768388e-02\n",
      "  6.40870833e-02  3.85465642e-03 -3.32608132e-04 -9.55589083e-04\n",
      "  8.59668812e-03 -5.32284362e-04  9.71536530e-04  3.69559157e-03\n",
      "  3.55703473e-04 -5.43892997e-04 -3.29573140e-01 -1.39974366e-03\n",
      "  8.26957837e-04  1.01646507e-03 -1.67549174e-03 -5.82354748e-03\n",
      " -1.10844940e-02 -3.95386302e-03]  and loss= 0.33968798676609097\n"
     ]
    }
   ],
   "source": [
    "# test ridge regression \n",
    "\n",
    "#initial_w = np.zeros(tx_tr.shape[1])\n",
    "#max_iters = 100\n",
    "#lambda_ = 0.0005\n",
    "\n",
    "#w, loss = ridge_regression(y_tr, tx_tr, lambda_)\n",
    "lambda_ = ridge_regression_best_lambda(y_tr, tx_tr)\n",
    "print(\"best lambda is \",lambda_)\n",
    "w, loss = ridge_regression(y_tr, tx_tr, lambda_)\n",
    "\n",
    "#y_pred = predict(w, tx_te)\n",
    "#y_pred[y_pred==0] = -1\n",
    "\n",
    "#OUTPUT_PATH = 'sample-submission'\n",
    "#create_csv_submission(ids_te, y_pred, OUTPUT_PATH)\n",
    "\n",
    "print(\"end of the ridge_regression with w=\",w,\" and loss=\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5f5c7de-0c97-4975-93cb-71f4916eb96e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preiteration, the loss=265618.8699766104, the grad=-170.99750063180275\n",
      "Current iteration=0, the loss=-131622.40316858073, the grad=1821.1184651771323\n",
      "Current iteration=1, the loss=-304224.27345602674, the grad=834.2998806025661\n",
      "Current iteration=2, the loss=-473819.023226236, the grad=822.0329349913256\n",
      "Current iteration=3, the loss=-642832.2535794436, the grad=819.95442066089\n",
      "Current iteration=4, the loss=-811580.6865605841, the grad=819.1344796372504\n",
      "Current iteration=5, the loss=-980135.6385100734, the grad=818.555115367976\n",
      "Current iteration=6, the loss=-1148531.4801186854, the grad=818.0741678461544\n",
      "Current iteration=7, the loss=-1316792.7929266673, the grad=817.6605291273098\n",
      "Current iteration=8, the loss=-1484939.014645783, the grad=817.3002315478669\n",
      "Current iteration=9, the loss=-1652985.9149293667, the grad=816.9837174833692\n",
      "Current iteration=10, the loss=-1820946.4446302052, the grad=816.7036473347346\n",
      "Current iteration=11, the loss=-1988831.3445494724, the grad=816.4542210787805\n",
      "Current iteration=12, the loss=-2156649.604153131, the grad=816.2307968581225\n",
      "Current iteration=13, the loss=-2324408.8119551395, the grad=816.0296215170065\n",
      "Current iteration=14, the loss=-2492115.4253289737, the grad=815.8476304559043\n",
      "Current iteration=15, the loss=-2659774.979936004, the grad=815.6822972755272\n",
      "Current iteration=16, the loss=-2827392.2536577308, the grad=815.5315201341305\n",
      "Current iteration=17, the loss=-2994971.3960384983, the grad=815.3935352899941\n",
      "Current iteration=18, the loss=-3162516.0313992966, the grad=815.2668508287702\n",
      "Current iteration=19, the loss=-3330029.341699011, the grad=815.1501954401904\n",
      "Current iteration=20, the loss=-3497514.1336923423, the grad=815.0424784780788\n",
      "Current iteration=21, the loss=-3664972.8938117526, the grad=814.9427585360762\n",
      "Current iteration=22, the loss=-3832407.83337325, the grad=814.8502184976678\n",
      "Current iteration=23, the loss=-3999820.9260923923, the grad=814.7641455473015\n",
      "Current iteration=24, the loss=-4167213.9394395263, the grad=814.6839150144096\n",
      "Current iteration=25, the loss=-4334588.461020196, the grad=814.6089772037569\n",
      "Current iteration=26, the loss=-4501945.920907663, the grad=814.5388465724706\n",
      "Current iteration=27, the loss=-4669287.610657588, the grad=814.4730927669107\n",
      "Current iteration=28, the loss=-4836614.699584214, the grad=814.4113331460098\n",
      "Current iteration=29, the loss=-5003928.248761313, the grad=814.3532265025071\n",
      "Current iteration=30, the loss=-5171229.223120919, the grad=814.2984677572441\n",
      "Current iteration=31, the loss=-5338518.5019524535, the grad=814.24678344996\n",
      "Current iteration=32, the loss=-5505796.888049287, the grad=814.1979278868146\n",
      "Current iteration=33, the loss=-5673065.11570585, the grad=814.1516798331112\n",
      "Current iteration=34, the loss=-5840323.857733158, the grad=814.1078396615528\n",
      "Current iteration=35, the loss=-6007573.731632501, the grad=814.0662268833901\n",
      "Current iteration=36, the loss=-6174815.305044082, the grad=814.0266780031792\n",
      "Current iteration=37, the loss=-6342049.100568864, the grad=813.9890446484688\n",
      "Current iteration=38, the loss=-6509275.600046621, the grad=813.9531919341315\n",
      "Current iteration=39, the loss=-6676495.248360609, the grad=813.9189970278516\n",
      "Current iteration=40, the loss=-6843708.456828911, the grad=813.8863478887203\n",
      "Current iteration=41, the loss=-7010915.606233841, the grad=813.8551421553663\n",
      "Current iteration=42, the loss=-7178117.049533516, the grad=813.8252861636753\n",
      "Current iteration=43, the loss=-7345313.114293667, the grad=813.7966940771634\n",
      "Current iteration=44, the loss=-7512504.10487252, the grad=813.7692871155639\n",
      "Current iteration=45, the loss=-7679690.304387404, the grad=813.74299286926\n",
      "Current iteration=46, the loss=-7846871.976487734, the grad=813.7177446889299\n",
      "Current iteration=47, the loss=-8014049.366956169, the grad=813.6934811412463\n",
      "Current iteration=48, the loss=-8181222.70515679, the grad=813.6701455227028\n",
      "Current iteration=49, the loss=-8348392.20534694, the grad=813.6476854246947\n",
      "Current iteration=50, the loss=-8515558.067867227, the grad=813.626052343876\n",
      "Current iteration=51, the loss=-8682720.480222648, the grad=813.6052013325832\n",
      "Current iteration=52, the loss=-8849879.618066031, the grad=813.5850906847777\n",
      "Current iteration=53, the loss=-9017035.646093855, the grad=813.5656816535143\n",
      "Current iteration=54, the loss=-9184188.718863338, the grad=813.5469381964479\n",
      "Current iteration=55, the loss=-9351338.981538625, the grad=813.5288267463019\n",
      "Current iteration=56, the loss=-9518486.57057302, the grad=813.5113160035925\n",
      "Current iteration=57, the loss=-9685631.614333594, the grad=813.4943767492263\n",
      "Current iteration=58, the loss=-9852774.233673573, the grad=813.4779816748643\n",
      "Current iteration=59, the loss=-10019914.54245756, the grad=813.4621052291853\n",
      "Current iteration=60, the loss=-10187052.648043996, the grad=813.4467234783998\n",
      "Current iteration=61, the loss=-10354188.651728772, the grad=813.4318139795528\n",
      "Current iteration=62, the loss=-10521322.649153596, the grad=813.4173556653075\n",
      "Current iteration=63, the loss=-10688454.730682306, the grad=813.4033287390627\n",
      "Current iteration=64, the loss=-10855584.981747953, the grad=813.3897145793636\n",
      "Current iteration=65, the loss=-11022713.48317328, the grad=813.3764956526971\n",
      "Current iteration=66, the loss=-11189840.311466916, the grad=813.3636554338431\n",
      "Current iteration=67, the loss=-11356965.539097345, the grad=813.3511783330599\n",
      "Current iteration=68, the loss=-11524089.234746616, the grad=813.3390496294367\n",
      "Current iteration=69, the loss=-11691211.46354546, the grad=813.3272554098371\n",
      "Current iteration=70, the loss=-11858332.287291331, the grad=813.3157825129034\n",
      "Current iteration=71, the loss=-12025451.764650924, the grad=813.3046184776464\n",
      "Current iteration=72, the loss=-12192569.951348262, the grad=813.2937514962076\n",
      "Current iteration=73, the loss=-12359686.900339635, the grad=813.2831703703982\n",
      "Current iteration=74, the loss=-12526802.661976436, the grad=813.272864471688\n",
      "Current iteration=75, the loss=-12693917.284156788, the grad=813.2628237043208\n",
      "Current iteration=76, the loss=-12861030.812466906, the grad=813.2530384712898\n",
      "Current iteration=77, the loss=-13028143.290312974, the grad=813.2434996429132\n",
      "Current iteration=78, the loss=-13195254.759044226, the grad=813.2341985277886\n",
      "Current iteration=79, the loss=-13362365.258067995, the grad=813.2251268459192\n",
      "Current iteration=80, the loss=-13529474.824957203, the grad=813.2162767038234\n",
      "Current iteration=81, the loss=-13696583.495550958, the grad=813.2076405714661\n",
      "Current iteration=82, the loss=-13863691.304048734, the grad=813.1992112608526\n",
      "Current iteration=83, the loss=-14030798.283098584, the grad=813.1909819061511\n",
      "Current iteration=84, the loss=-14197904.46387988, the grad=813.1829459452167\n",
      "Current iteration=85, the loss=-14365009.876180835, the grad=813.1750971024027\n",
      "Current iteration=86, the loss=-14532114.548471386, the grad=813.167429372557\n",
      "Current iteration=87, the loss=-14699218.507971553, the grad=813.1599370061065\n",
      "Current iteration=88, the loss=-14866321.780715706, the grad=813.1526144951458\n",
      "Current iteration=89, the loss=-15033424.391613025, the grad=813.1454565604504\n",
      "Current iteration=90, the loss=-15200526.364504391, the grad=813.1384581393436\n",
      "Current iteration=91, the loss=-15367627.722215857, the grad=813.1316143743505\n",
      "Current iteration=92, the loss=-15534728.486609142, the grad=813.1249206025826\n",
      "Current iteration=93, the loss=-15701828.67862912, the grad=813.1183723457909\n",
      "Current iteration=94, the loss=-15868928.31834866, the grad=813.1119653010493\n",
      "Current iteration=95, the loss=-16036027.42501089, the grad=813.1056953320086\n",
      "Current iteration=96, the loss=-16203126.017069114, the grad=813.0995584606908\n",
      "Current iteration=97, the loss=-16370224.112224523, the grad=813.0935508597784\n",
      "Current iteration=98, the loss=-16537321.7274618, the grad=813.0876688453649\n",
      "Current iteration=99, the loss=-16704418.879082775, the grad=813.081908870135\n",
      "Current iteration=100, the loss=-16871515.582738303, the grad=813.0762675169416\n",
      "Current iteration=101, the loss=-17038611.853458367, the grad=813.0707414927591\n",
      "Current iteration=102, the loss=-17205707.70568064, the grad=813.0653276229756\n",
      "Current iteration=103, the loss=-17372803.15327746, the grad=813.0600228460124\n",
      "Current iteration=104, the loss=-17539898.209581457, the grad=813.0548242082435\n",
      "Current iteration=105, the loss=-17706992.887409814, the grad=813.049728859194\n",
      "Current iteration=106, the loss=-17874087.199087285, the grad=813.0447340470032\n",
      "Current iteration=107, the loss=-18041181.156468, the grad=813.0398371141321\n",
      "Current iteration=108, the loss=-18208274.770956226, the grad=813.035035493299\n",
      "Current iteration=109, the loss=-18375368.05352599, the grad=813.0303267036331\n",
      "Current iteration=110, the loss=-18542461.014739834, the grad=813.0257083470268\n",
      "Current iteration=111, the loss=-18709553.664766487, the grad=813.0211781046787\n",
      "Current iteration=112, the loss=-18876646.01339783, the grad=813.0167337338102\n",
      "Current iteration=113, the loss=-19043738.070064865, the grad=813.0123730645546\n",
      "Current iteration=114, the loss=-19210829.843853053, the grad=813.0080939969972\n",
      "Current iteration=115, the loss=-19377921.343516804, the grad=813.0038944983629\n",
      "Current iteration=116, the loss=-19545012.577493317, the grad=812.9997726003453\n",
      "Current iteration=117, the loss=-19712103.55391575, the grad=812.9957263965609\n",
      "Current iteration=118, the loss=-19879194.28062579, the grad=812.991754040133\n",
      "Current iteration=119, the loss=-20046284.765185542, the grad=812.9878537413816\n",
      "Current iteration=120, the loss=-20213375.01488902, the grad=812.9840237656297\n",
      "Current iteration=121, the loss=-20380465.036772933, the grad=812.9802624311105\n",
      "Current iteration=122, the loss=-20547554.837627098, the grad=812.9765681069697\n",
      "Current iteration=123, the loss=-20714644.424004328, the grad=812.9729392113612\n",
      "Current iteration=124, the loss=-20881733.802229837, the grad=812.969374209629\n",
      "Current iteration=125, the loss=-21048822.978410255, the grad=812.9658716125708\n",
      "Current iteration=126, the loss=-21215911.958442323, the grad=812.9624299747794\n",
      "Current iteration=127, the loss=-21383000.74802095, the grad=812.9590478930575\n",
      "Current iteration=128, the loss=-21550089.35264724, the grad=812.9557240049008\n",
      "Current iteration=129, the loss=-21717177.777635876, the grad=812.9524569870508\n",
      "Current iteration=130, the loss=-21884266.02812235, the grad=812.9492455541033\n",
      "Current iteration=131, the loss=-22051354.10906983, the grad=812.9460884571841\n",
      "Current iteration=132, the loss=-22218442.025275726, the grad=812.9429844826745\n",
      "Current iteration=133, the loss=-22385529.78137794, the grad=812.939932450994\n",
      "Current iteration=134, the loss=-22552617.38186095, the grad=812.9369312154316\n",
      "Current iteration=135, the loss=-22719704.831061464, the grad=812.9339796610279\n",
      "Current iteration=136, the loss=-22886792.13317404, the grad=812.9310767034991\n",
      "Current iteration=137, the loss=-23053879.292256277, the grad=812.9282212882096\n",
      "Current iteration=138, the loss=-23220966.312233962, the grad=812.9254123891809\n",
      "Current iteration=139, the loss=-23388053.196905825, the grad=812.9226490081445\n",
      "Current iteration=140, the loss=-23555139.949948255, the grad=812.919930173631\n",
      "Current iteration=141, the loss=-23722226.574919723, the grad=812.917254940092\n",
      "Current iteration=142, the loss=-23889313.075265057, the grad=812.9146223870628\n",
      "Current iteration=143, the loss=-24056399.454319563, the grad=812.9120316183504\n",
      "Current iteration=144, the loss=-24223485.715312887, the grad=812.909481761259\n",
      "Current iteration=145, the loss=-24390571.861372896, the grad=812.9069719658413\n",
      "Current iteration=146, the loss=-24557657.895529196, the grad=812.9045014041781\n",
      "Current iteration=147, the loss=-24724743.82071664, the grad=812.9020692696878\n",
      "Current iteration=148, the loss=-24891829.63977873, the grad=812.8996747764608\n",
      "Current iteration=149, the loss=-25058915.35547069, the grad=812.8973171586161\n",
      "Current iteration=150, the loss=-25226000.970462687, the grad=812.8949956696849\n",
      "Current iteration=151, the loss=-25393086.487342678, the grad=812.8927095820155\n",
      "Current iteration=152, the loss=-25560171.90861931, the grad=812.8904581861995\n",
      "Current iteration=153, the loss=-25727257.236724645, the grad=812.8882407905201\n",
      "Current iteration=154, the loss=-25894342.47401677, the grad=812.8860567204173\n",
      "Current iteration=155, the loss=-26061427.62278232, the grad=812.883905317977\n",
      "Current iteration=156, the loss=-26228512.685238954, the grad=812.8817859414339\n",
      "Current iteration=157, the loss=-26395597.66353763, the grad=812.8796979646933\n",
      "Current iteration=158, the loss=-26562682.55976487, the grad=812.8776407768709\n",
      "Current iteration=159, the loss=-26729767.375944965, the grad=812.8756137818461\n",
      "Current iteration=160, the loss=-26896852.114041988, the grad=812.8736163978344\n",
      "Current iteration=161, the loss=-27063936.775961846, the grad=812.8716480569702\n",
      "Current iteration=162, the loss=-27231021.363554213, the grad=812.8697082049075\n",
      "Current iteration=163, the loss=-27398105.87861435, the grad=812.8677963004327\n",
      "Current iteration=164, the loss=-27565190.322884906, the grad=812.8659118150886\n",
      "Current iteration=165, the loss=-27732274.698057685, the grad=812.8640542328166\n",
      "Current iteration=166, the loss=-27899359.005775224, the grad=812.8622230496037\n",
      "Current iteration=167, the loss=-28066443.24763248, the grad=812.8604177731464\n",
      "Current iteration=168, the loss=-28233527.425178315, the grad=812.858637922524\n",
      "Current iteration=169, the loss=-28400611.539917, the grad=812.856883027882\n",
      "Current iteration=170, the loss=-28567695.593309645, the grad=812.8551526301281\n",
      "Current iteration=171, the loss=-28734779.586775567, the grad=812.8534462806333\n",
      "Current iteration=172, the loss=-28901863.52169368, the grad=812.8517635409506\n",
      "Current iteration=173, the loss=-29068947.39940369, the grad=812.850103982533\n",
      "Current iteration=174, the loss=-29236031.221207425, the grad=812.8484671864693\n",
      "Current iteration=175, the loss=-29403114.98836993, the grad=812.8468527432224\n",
      "Current iteration=176, the loss=-29570198.70212073, the grad=812.8452602523784\n",
      "Current iteration=177, the loss=-29737282.363654874, the grad=812.8436893224044\n",
      "Current iteration=178, the loss=-29904365.974134013, the grad=812.8421395704107\n",
      "Current iteration=179, the loss=-30071449.534687467, the grad=812.8406106219243\n",
      "Current iteration=180, the loss=-30238533.04641322, the grad=812.8391021106651\n",
      "Current iteration=181, the loss=-30405616.510378864, the grad=812.8376136783348\n",
      "Current iteration=182, the loss=-30572699.92762254, the grad=812.836144974405\n",
      "Current iteration=183, the loss=-30739783.29915387, the grad=812.8346956559188\n",
      "Current iteration=184, the loss=-30906866.625954814, the grad=812.8332653872941\n",
      "Current iteration=185, the loss=-31073949.908980455, the grad=812.8318538401329\n",
      "Current iteration=186, the loss=-31241033.1491599, the grad=812.8304606930395\n",
      "Current iteration=187, the loss=-31408116.347397063, the grad=812.8290856314403\n",
      "Current iteration=188, the loss=-31575199.50457132, the grad=812.8277283474118\n",
      "Current iteration=189, the loss=-31742282.62153835, the grad=812.826388539513\n",
      "Current iteration=190, the loss=-31909365.699130822, the grad=812.8250659126211\n",
      "Current iteration=191, the loss=-32076448.73815906, the grad=812.823760177774\n",
      "Current iteration=192, the loss=-32243531.73941173, the grad=812.8224710520174\n",
      "Current iteration=193, the loss=-32410614.703656424, the grad=812.8211982582538\n",
      "Current iteration=194, the loss=-32577697.631640386, the grad=812.8199415251014\n",
      "Current iteration=195, the loss=-32744780.524091024, the grad=812.8187005867485\n",
      "Current iteration=196, the loss=-32911863.381716516, the grad=812.8174751828211\n",
      "Current iteration=197, the loss=-33078946.205206413, the grad=812.8162650582486\n",
      "Current iteration=198, the loss=-33246028.995232105, the grad=812.8150699631349\n",
      "Current iteration=199, the loss=-33413111.752447437, the grad=812.8138896526334\n",
      "Current iteration=200, the loss=-33580194.47748913, the grad=812.8127238868252\n",
      "Current iteration=201, the loss=-33747277.17097736, the grad=812.8115724306007\n",
      "Current iteration=202, the loss=-33914359.83351616, the grad=812.8104350535455\n",
      "Current iteration=203, the loss=-34081442.46569396, the grad=812.8093115298276\n",
      "Current iteration=204, the loss=-34248525.068083934, the grad=812.808201638089\n",
      "Current iteration=205, the loss=-34415607.64124457, the grad=812.8071051613407\n",
      "Current iteration=206, the loss=-34582690.18571995, the grad=812.8060218868593\n",
      "Current iteration=207, the loss=-34749772.702040225, the grad=812.8049516060865\n",
      "Current iteration=208, the loss=-34916855.19072206, the grad=812.8038941145343\n",
      "Current iteration=209, the loss=-35083937.65226895, the grad=812.8028492116869\n",
      "Current iteration=210, the loss=-35251020.08717154, the grad=812.8018167009117\n",
      "Current iteration=211, the loss=-35418102.49590811, the grad=812.8007963893682\n",
      "Current iteration=212, the loss=-35585184.878944844, the grad=812.7997880879218\n",
      "Current iteration=213, the loss=-35752267.23673618, the grad=812.7987916110575\n",
      "Current iteration=214, the loss=-35919349.569725156, the grad=812.7978067768003\n",
      "Current iteration=215, the loss=-36086431.87834367, the grad=812.7968334066309\n",
      "Current iteration=216, the loss=-36253514.163012885, the grad=812.7958713254109\n",
      "Current iteration=217, the loss=-36420596.424143456, the grad=812.7949203613039\n",
      "Current iteration=218, the loss=-36587678.66213579, the grad=812.7939803457043\n",
      "Current iteration=219, the loss=-36754760.87738044, the grad=812.7930511131625\n",
      "Current iteration=220, the loss=-36921843.070258245, the grad=812.7921325013159\n",
      "Current iteration=221, the loss=-37088925.24114068, the grad=812.7912243508194\n",
      "Current iteration=222, the loss=-37256007.390390135, the grad=812.7903265052797\n",
      "Current iteration=223, the loss=-37423089.51836005, the grad=812.7894388111896\n",
      "Current iteration=224, the loss=-37590171.62539524, the grad=812.7885611178649\n",
      "Current iteration=225, the loss=-37757253.71183213, the grad=812.787693277382\n",
      "Current iteration=226, the loss=-37924335.77799897, the grad=812.7868351445195\n",
      "Current iteration=227, the loss=-38091417.82421602, the grad=812.7859865766965\n",
      "Current iteration=228, the loss=-38258499.8507958, the grad=812.7851474339187\n",
      "Current iteration=229, the loss=-38425581.85804329, the grad=812.7843175787195\n",
      "Current iteration=230, the loss=-38592663.84625616, the grad=812.7834968761093\n",
      "Current iteration=231, the loss=-38759745.815724894, the grad=812.7826851935187\n",
      "Current iteration=232, the loss=-38926827.766733065, the grad=812.7818824007492\n",
      "Current iteration=233, the loss=-39093909.69955743, the grad=812.7810883699223\n",
      "Current iteration=234, the loss=-39260991.61446819, the grad=812.7803029754302\n",
      "Current iteration=235, the loss=-39428073.51172914, the grad=812.779526093887\n",
      "Current iteration=236, the loss=-39595155.39159779, the grad=812.7787576040844\n",
      "Current iteration=237, the loss=-39762237.25432561, the grad=812.7779973869433\n",
      "Current iteration=238, the loss=-39929319.1001581, the grad=812.7772453254705\n",
      "Current iteration=239, the loss=-40096400.92933503, the grad=812.7765013047157\n",
      "Current iteration=240, the loss=-40263482.742090546, the grad=812.7757652117282\n",
      "Current iteration=241, the loss=-40430564.53865332, the grad=812.7750369355158\n",
      "Current iteration=242, the loss=-40597646.31924669, the grad=812.7743163670048\n",
      "Current iteration=243, the loss=-40764728.08408877, the grad=812.7736033990009\n",
      "Current iteration=244, the loss=-40931809.8333927, the grad=812.7728979261491\n",
      "Current iteration=245, the loss=-41098891.567366645, the grad=812.7721998448977\n",
      "Current iteration=246, the loss=-41265973.286213934, the grad=812.7715090534618\n",
      "Current iteration=247, the loss=-41433054.99013327, the grad=812.7708254517853\n",
      "Current iteration=248, the loss=-41600136.67931881, the grad=812.7701489415092\n",
      "Current iteration=249, the loss=-41767218.35396012, the grad=812.7694794259343\n",
      "Current iteration=250, the loss=-41934300.014244564, the grad=812.7688168099903\n",
      "Current iteration=251, the loss=-42101381.66036376, the grad=812.7681610002018\n",
      "Current iteration=252, the loss=-42268463.292952016, the grad=812.7675119046568\n",
      "Current iteration=253, the loss=-42435544.91965723, the grad=812.7668694329757\n",
      "Current iteration=254, the loss=-42602626.56938612, the grad=812.7662334962812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ml-project-1-ahl/helper_functions.py:61: RuntimeWarning: divide by zero encountered in log\n",
      "  return -np.sum(y * np.log(sigmoid(v)) + (1 - y) * np.log(1-sigmoid(v)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=255, the loss=-inf, the grad=812.7656040071687\n",
      "Current iteration=256, the loss=-inf, the grad=812.7649808796759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ml-project-1-ahl/implementations.py:180: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=257, the loss=-inf, the grad=812.764364029257\n",
      "Current iteration=258, the loss=-inf, the grad=812.7637533727523\n",
      "Current iteration=259, the loss=-inf, the grad=812.763148828363\n",
      "Current iteration=260, the loss=-inf, the grad=812.7625503156247\n",
      "Current iteration=261, the loss=-inf, the grad=812.7619577553808\n",
      "Current iteration=262, the loss=-inf, the grad=812.761371069756\n",
      "Current iteration=263, the loss=-inf, the grad=812.7607901821353\n",
      "Current iteration=264, the loss=-inf, the grad=812.7602150171369\n",
      "Current iteration=265, the loss=-inf, the grad=812.7596455005887\n",
      "Current iteration=266, the loss=-inf, the grad=812.7590815595062\n",
      "Current iteration=267, the loss=-inf, the grad=812.7585231220698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:86: RuntimeWarning: invalid value encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=268, the loss=nan, the grad=812.7579701176023\n",
      "Current iteration=269, the loss=nan, the grad=812.7574224765474\n",
      "Current iteration=270, the loss=nan, the grad=812.7568801304486\n",
      "Current iteration=271, the loss=nan, the grad=812.7563430119293\n",
      "Current iteration=272, the loss=nan, the grad=812.7558110546707\n",
      "Current iteration=273, the loss=nan, the grad=812.7552841933941\n",
      "Current iteration=274, the loss=nan, the grad=812.7547623638405\n",
      "Current iteration=275, the loss=nan, the grad=812.7542455027514\n",
      "Current iteration=276, the loss=nan, the grad=812.7537335478511\n",
      "Current iteration=277, the loss=nan, the grad=812.7532264378285\n",
      "Current iteration=278, the loss=nan, the grad=812.752724112318\n",
      "Current iteration=279, the loss=nan, the grad=812.7522265118845\n",
      "Current iteration=280, the loss=nan, the grad=812.7517335780036\n",
      "Current iteration=281, the loss=nan, the grad=812.7512452530484\n",
      "Current iteration=282, the loss=nan, the grad=812.7507614802694\n",
      "Current iteration=283, the loss=nan, the grad=812.750282203781\n",
      "Current iteration=284, the loss=nan, the grad=812.7498073685465\n",
      "Current iteration=285, the loss=nan, the grad=812.7493369203598\n",
      "Current iteration=286, the loss=nan, the grad=812.7488708058337\n",
      "Current iteration=287, the loss=nan, the grad=812.7484089723835\n",
      "Current iteration=288, the loss=nan, the grad=812.747951368212\n",
      "Current iteration=289, the loss=nan, the grad=812.7474979422984\n",
      "Current iteration=290, the loss=nan, the grad=812.7470486443813\n",
      "Current iteration=291, the loss=nan, the grad=812.7466034249461\n",
      "Current iteration=292, the loss=nan, the grad=812.7461622352139\n",
      "Current iteration=293, the loss=nan, the grad=812.7457250271251\n",
      "Current iteration=294, the loss=nan, the grad=812.7452917533299\n",
      "Current iteration=295, the loss=nan, the grad=812.7448623671742\n",
      "Current iteration=296, the loss=nan, the grad=812.7444368226867\n",
      "Current iteration=297, the loss=nan, the grad=812.7440150745693\n",
      "Current iteration=298, the loss=nan, the grad=812.7435970781846\n",
      "Current iteration=299, the loss=nan, the grad=812.7431827895426\n",
      "end of the logistic_regression with w= [ 2.62145655e-02 -1.01009089e-02 -1.22025027e-02 -7.93889576e-03\n",
      "  8.53920323e-02  6.50449528e-02  8.55639999e-02 -3.36707764e-04\n",
      " -3.28994370e-03 -2.55438937e-02 -2.63159878e-04  6.79451948e-05\n",
      "  8.55129653e-02 -4.63297803e-03  8.45922210e-07 -1.14124597e-06\n",
      " -7.41368280e-03  2.58243394e-06 -4.00304991e-06 -6.77575986e-03\n",
      "  4.72875074e-06 -3.31429943e-02 -1.81224811e-04  5.08413060e-02\n",
      "  5.88572681e-02  5.88578141e-02  8.15929534e-02  8.55389074e-02\n",
      "  8.55370279e-02 -1.34972326e-02]  and loss= nan\n"
     ]
    }
   ],
   "source": [
    "# test logistic regression\n",
    "\n",
    "initial_w = np.zeros(tx_tr.shape[1])\n",
    "# initial_w = w\n",
    "max_iters = 200\n",
    "gamma = 0.000001\n",
    "\n",
    "w, loss = logistic_regression(y_tr, tx_tr, initial_w, max_iters, gamma)\n",
    "\n",
    "print(\"end of the logistic_regression with w=\",w,\" and loss=\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b54129f-2e9e-489f-989e-cffe093f018d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
