{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "882a9018-5319-4a82-87e7-2b0d420475bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from implementations import *\n",
    "from helpers import *\n",
    "from helper_functions import *\n",
    "from optimization import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d56f0e3-9ee7-448f-a9dd-dd349209cc8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading the training dataset\n",
      "training dataset loaded\n"
     ]
    }
   ],
   "source": [
    "# load the training dataset\n",
    "print(\"loading the training dataset\")\n",
    "DATA_TRAIN_PATH = 'train.csv'\n",
    "y_tr, tx_tr, ids_tr = load_csv_data(DATA_TRAIN_PATH)\n",
    "print(\"training dataset loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af90503c-e3fa-4e1b-a222-63ef11b39a80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading the test dataset\n",
      "test dataset loaded\n"
     ]
    }
   ],
   "source": [
    "# load the test dataset\n",
    "print(\"loading the test dataset\")\n",
    "DATA_TEST_PATH = 'test.csv'\n",
    "y_te, tx_te, ids_te = load_csv_data(DATA_TEST_PATH)\n",
    "print(\"test dataset loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa8da103-df5b-4f1e-b595-9ae0cd91750e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shape of the training y is  (250000,)  and the training tx is  (250000, 30)\n",
      "the shape of the test y is  (568238,)  and the test tx is  (568238, 30)\n"
     ]
    }
   ],
   "source": [
    "# printing the shapes of the arrays\n",
    "print(\"the shape of the training y is \",y_tr.shape,\" and the training tx is \", tx_tr.shape)\n",
    "print(\"the shape of the test y is \",y_te.shape,\" and the test tx is \",tx_te.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6884129-6892-4ec8-bce6-fe61104b9ff5",
   "metadata": {},
<<<<<<< HEAD
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, the loss=0.4486458187302863, the grad=-83.42721036786665\n",
      "Current iteration=1, the loss=0.4377103722046582, the grad=33.98783214577216\n",
      "Current iteration=2, the loss=0.43449433074074234, the grad=-16.60357059571331\n",
      "Current iteration=3, the loss=0.4328322302044497, the grad=5.629515266934457\n",
      "Current iteration=4, the loss=0.4315514189916519, the grad=-3.7473774149053196\n",
      "Current iteration=5, the loss=0.4304204692525385, the grad=0.5577518598071749\n",
      "Current iteration=6, the loss=0.4293867701908119, the grad=-1.0934697641288125\n",
      "Current iteration=7, the loss=0.42843278516364114, the grad=-0.18221172512174794\n",
      "Current iteration=8, the loss=0.42754873444983027, the grad=-0.400517308847955\n",
      "Current iteration=9, the loss=0.42672723509157223, the grad=-0.14535207634151168\n",
      "Current iteration=10, the loss=0.4259621180594481, the grad=-0.11015833020600423\n",
      "Current iteration=11, the loss=0.42524806823948674, the grad=0.007306161636387198\n",
      "Current iteration=12, the loss=0.42458044802821815, the grad=0.07732206680594446\n",
      "Current iteration=13, the loss=0.42395517787287845, the grad=0.15733890981941295\n",
      "Current iteration=14, the loss=0.4233686449591719, the grad=0.22350310182679245\n",
      "Current iteration=15, the loss=0.42281763078895, the grad=0.28709906619895914\n",
      "Current iteration=16, the loss=0.42229925293889053, the grad=0.3440679623542085\n",
      "Current iteration=17, the loss=0.4218109178197018, the grad=0.39691180631538026\n",
      "Current iteration=18, the loss=0.42135028206116887, the grad=0.4452056006422906\n",
      "Current iteration=19, the loss=0.42091522069931736, the grad=0.48972149454009284\n",
      "end of the mean_squared_error_gd with w= [ 2.67514040e-04 -5.72615115e-05 -2.09496651e-05  5.34768152e-06\n",
      "  2.86867308e-05  1.19844098e-04  2.71679875e-05 -2.72606761e-07\n",
      " -1.15919469e-05 -3.63832852e-05 -1.12366383e-06  9.00290573e-07\n",
      "  2.84706382e-05  1.13694466e-05 -1.18417000e-08 -3.91224329e-08\n",
      " -1.78378608e-05 -7.20677489e-09  2.63648932e-08 -1.33707458e-05\n",
      "  5.15849415e-08 -5.20441569e-05 -5.35842877e-07  4.71168458e-05\n",
      "  5.26067999e-05  5.26036811e-05  1.61636422e-05  2.83966001e-05\n",
      "  2.83750807e-05 -2.99148650e-05]  and loss= 0.42091522069931736\n",
      "Current iteration=0, the loss=0.457316555397448, the grad=-83.42721036786665\n",
      "Current iteration=1, the loss=0.44200621657627187, the grad=45.729336397136045\n",
      "Current iteration=2, the loss=0.4359182357448747, the grad=-28.401915596561647\n",
      "Current iteration=3, the loss=0.4329936250497594, the grad=14.725006617712037\n",
      "Current iteration=4, the loss=0.4312024086797886, the grad=-9.846997609005745\n",
      "Current iteration=5, the loss=0.4298584072487603, the grad=4.611251850340437\n",
      "Current iteration=6, the loss=0.4287246670438193, the grad=-3.481407449596053\n",
      "Current iteration=7, the loss=0.42771521836182497, the grad=1.4123032543752447\n",
      "Current iteration=8, the loss=0.42679547791209166, the grad=-1.2103805586933516\n",
      "Current iteration=9, the loss=0.4259488607630753, the grad=0.4837857220237211\n",
      "Current iteration=10, the loss=0.42516548976207896, the grad=-0.3309204964172562\n",
      "Current iteration=11, the loss=0.4244383021448915, the grad=0.2861383869180834\n",
      "Current iteration=12, the loss=0.42376164910797703, the grad=0.06290406350177905\n",
      "Current iteration=13, the loss=0.4231307481868747, the grad=0.31196473692412074\n",
      "Current iteration=14, the loss=0.4225414368850391, the grad=0.2772910263457175\n",
      "Current iteration=15, the loss=0.42199003980750127, the grad=0.39632565755620236\n",
      "Current iteration=16, the loss=0.4214732837004787, the grad=0.41770417512829183\n",
      "Current iteration=17, the loss=0.4209882362272007, the grad=0.4873927295085933\n",
      "Current iteration=18, the loss=0.42053225867366856, the grad=0.521977929316899\n",
      "Current iteration=19, the loss=0.420102967987544, the grad=0.5703074095555821\n",
      "end of the mean_squared_error_gd with w= [ 2.84031310e-04 -6.23640596e-05 -2.28654317e-05  5.81931961e-06\n",
      "  2.73258040e-05  1.26607103e-04  2.56670770e-05 -2.94815760e-07\n",
      " -1.26769827e-05 -3.98650946e-05 -1.22952138e-06  9.78331549e-07\n",
      "  2.70919088e-05  1.25992561e-05 -1.31599221e-08 -4.27896898e-08\n",
      " -1.94678178e-05 -8.36380527e-09  2.91945746e-08 -1.43334461e-05\n",
      "  5.63547546e-08 -5.69639308e-05 -5.90872505e-07  4.63421460e-05\n",
      "  5.24832401e-05  5.24798991e-05  1.35456176e-05  2.70111584e-05\n",
      "  2.69875146e-05 -3.29965264e-05]  and loss= 0.420102967987544\n",
      "Current iteration=0, the loss=0.4684975020271792, the grad=-83.42721036786665\n",
      "Current iteration=1, the loss=0.4513712748460005, the grad=57.47084064849991\n",
      "Current iteration=2, the loss=0.44178360296424807, the grad=-43.56038950251244\n",
      "Current iteration=3, the loss=0.43617479340076165, the grad=29.634952887893135\n",
      "Current iteration=4, the loss=0.43268849485123373, the grad=-22.729614001422586\n",
      "Current iteration=5, the loss=0.4303539841023341, the grad=15.315259096391403\n",
      "Current iteration=6, the loss=0.4286614226647361, the grad=-11.8067735215727\n",
      "Current iteration=7, the loss=0.4273411475179239, the grad=7.984549042057253\n",
      "Current iteration=8, the loss=0.4262489193466734, the grad=-6.048032125698609\n",
      "Current iteration=9, the loss=0.4253062112549615, the grad=4.261249734588027\n",
      "Current iteration=10, the loss=0.4244691000973839, the grad=-2.986527485289154\n",
      "Current iteration=11, the loss=0.42371203005243574, the grad=2.3947844917048813\n",
      "Current iteration=12, the loss=0.42301931548757576, the grad=-1.3383102126144695\n",
      "Current iteration=13, the loss=0.4223806759656577, the grad=1.4799234723976127\n",
      "Current iteration=14, the loss=0.4217888755503783, the grad=-0.43423796096158623\n",
      "Current iteration=15, the loss=0.4212384630175911, the grad=1.0493464324630073\n",
      "Current iteration=16, the loss=0.42072509010349285, the grad=0.07509110540316791\n",
      "Current iteration=17, the loss=0.4202451348565581, the grad=0.8624059779866046\n",
      "Current iteration=18, the loss=0.4197954872779463, the grad=0.3726429743660339\n",
      "Current iteration=19, the loss=0.4193734222616614, the grad=0.7956553535672023\n",
      "end of the mean_squared_error_gd with w= [ 2.99303128e-04 -6.74121694e-05 -2.47936774e-05  6.29185568e-06\n",
      "  2.59492690e-05  1.33228981e-04  2.41522150e-05 -3.17665630e-07\n",
      " -1.37558013e-05 -4.33295151e-05 -1.33511550e-06  1.05539491e-06\n",
      "  2.56980156e-05  1.38282081e-05 -1.44811869e-08 -4.64149369e-08\n",
      " -2.10940736e-05 -9.54703223e-09  3.20294756e-08 -1.52562711e-05\n",
      "  6.10874385e-08 -6.18692921e-05 -6.45817775e-07  4.54554009e-05\n",
      "  5.22440217e-05  5.22404697e-05  1.09174774e-05  2.56106206e-05\n",
      "  2.55848560e-05 -3.60636423e-05]  and loss= 0.4193734222616614\n",
      "Current iteration=0, the loss=0.4821886586194801, the grad=-83.42721036786665\n",
      "Current iteration=1, the loss=0.46860585690459233, the grad=69.2123448998638\n",
      "Current iteration=2, the loss=0.4581982565934716, the grad=-62.07899231356577\n",
      "Current iteration=3, the loss=0.4501791146011084, the grad=51.80433968063336\n",
      "Current iteration=4, the loss=0.44396036370956915, the grad=-46.14460924735117\n",
      "Current iteration=5, the loss=0.4391020524505416, the grad=38.82485784870273\n",
      "Current iteration=6, the loss=0.43527469843105515, the grad=-34.24753475619125\n",
      "Current iteration=7, the loss=0.43223124983630795, the grad=29.150561452745343\n",
      "Current iteration=8, the loss=0.42978619726701733, the grad=-25.361892668173176\n",
      "Current iteration=9, the loss=0.42780000787273786, the grad=21.942463138596235\n",
      "Current iteration=10, the loss=0.42616752190857976, the grad=-18.72302285177043\n",
      "Current iteration=11, the loss=0.42480929992374106, the grad=16.574048644656003\n",
      "Current iteration=12, the loss=0.42366516763748774, the grad=-13.76088037412461\n",
      "Current iteration=13, the loss=0.4226893980829994, the grad=12.577547713882488\n",
      "Current iteration=14, the loss=0.4218471138206511, the grad=-10.050432798082106\n",
      "Current iteration=15, the loss=0.4211115985844365, the grad=9.603776219531271\n",
      "Current iteration=16, the loss=0.42046228702505306, the grad=-7.274713169969255\n",
      "Current iteration=17, the loss=0.41988326023516365, the grad=7.392125309493045\n",
      "Current iteration=18, the loss=0.4193621186789887, the grad=-5.1972967005627355\n",
      "Current iteration=19, the loss=0.4188891368607531, the grad=5.748136342425167\n",
      "end of the mean_squared_error_gd with w= [ 3.13157165e-04 -7.22747054e-05 -2.65292899e-05  6.84724851e-06\n",
      "  2.20410507e-05  1.37160381e-04  2.01076388e-05 -3.34791000e-07\n",
      " -1.47919067e-05 -4.65226753e-05 -1.43694985e-06  1.13035253e-06\n",
      "  2.17730147e-05  1.51449606e-05 -1.58447488e-08 -5.00320823e-08\n",
      " -2.26074081e-05 -1.08280509e-08  3.49772044e-08 -1.60560247e-05\n",
      "  6.57421887e-08 -6.63789547e-05 -6.99776220e-07  4.29928327e-05\n",
      "  5.03658770e-05  5.03621103e-05  5.76054327e-06  2.16790696e-05\n",
      "  2.16511868e-05 -3.90602199e-05]  and loss= 0.4188891368607531\n",
      "Current iteration=0, the loss=0.49839002517435055, the grad=-83.42721036786665\n",
      "Current iteration=1, the loss=0.49712916702472487, the grad=80.95384915122766\n",
      "Current iteration=2, the loss=0.49615134971816305, the grad=-83.95772402972155\n",
      "Current iteration=3, the loss=0.49540731213947303, the grad=82.6781525990882\n",
      "Current iteration=4, the loss=0.49485973377372255, the grad=-84.67000296440553\n",
      "Current iteration=5, the loss=0.4944798525022932, the grad=84.27983045459091\n",
      "Current iteration=6, the loss=0.49424510863290133, the grad=-85.51726186796863\n",
      "Current iteration=7, the loss=0.49413749582288047, the grad=85.79837452682933\n",
      "Current iteration=8, the loss=0.4941423998017128, the grad=-86.4669427736661\n",
      "Current iteration=9, the loss=0.49424777448218005, the grad=87.2614708512579\n",
      "Current iteration=10, the loss=0.49444355211753077, the grad=-87.49629999629353\n",
      "Current iteration=11, the loss=0.49472121642631833, the grad=88.68865753024191\n",
      "Current iteration=12, the loss=0.4950734897310871, the grad=-88.58936928828858\n",
      "Current iteration=13, the loss=0.4954941003357032, the grad=90.09383670405104\n",
      "Current iteration=14, the loss=0.49597760678581554, the grad=-89.73488562366303\n",
      "Current iteration=15, the loss=0.4965192628154739, the grad=91.48700113089821\n",
      "Current iteration=16, the loss=0.49711491170604316, the grad=-90.92485127987145\n",
      "Current iteration=17, the loss=0.4977609021738314, the grad=92.87542227833092\n",
      "Current iteration=18, the loss=0.49845402024155405, the grad=-92.15354987167741\n",
      "Current iteration=19, the loss=0.49919143316562337, the grad=94.2644690562147\n",
      "end of the mean_squared_error_gd with w= [ 3.20294824e-04 -7.42452392e-05 -2.39869353e-05  9.06038972e-06\n",
      " -3.51198787e-05  8.68758041e-05 -3.71800570e-05 -2.19166776e-07\n",
      " -1.50570839e-05 -4.44673125e-05 -1.46480780e-06  1.17833153e-06\n",
      " -3.54008655e-05  1.83143487e-05 -1.80590686e-08 -5.42741597e-08\n",
      " -2.18267447e-05 -1.37188983e-08  4.02029128e-08 -1.50310816e-05\n",
      "  6.94359151e-08 -6.29804155e-05 -7.35787024e-07  8.65704037e-06\n",
      "  1.54312785e-05  1.54270186e-05 -5.27123110e-05 -3.55005824e-05\n",
      " -3.55305909e-05 -4.09549081e-05]  and loss= 0.49919143316562337\n",
      "[0.42091522069931736, 0.420102967987544, 0.4193734222616614, 0.4188891368607531, 0.49919143316562337]\n"
     ]
    }
   ],
=======
   "outputs": [],
>>>>>>> 518c9d40bcb72cf733cc121260794e88145a7722
   "source": [
    "# testing the mean square gd\n",
    "#initial_w = w\n",
    "max_iters = 20\n",
    "# gamma = 0.0000002\n",
    "gamma = np.linspace(0.00000025, 0.00000035, 5)\n",
    "losses = []\n",
    "\n",
<<<<<<< HEAD
    "for g in gamma:\n",
    "    initial_w = np.zeros(tx_tr.shape[1])\n",
    "    w, loss = mean_squared_error_gd(y_tr, tx_tr, initial_w, max_iters, g)\n",
    "    print(\"end of the mean_squared_error_gd with w=\",w,\" and loss=\", loss)\n",
    "    losses.append(loss)\n",
    "\n",
    "    \n",
    "print(losses)"
=======
    "#w, loss = mean_squared_error_gd(y_tr, tx_tr, initial_w, max_iters, gamma)\n",
    "\n",
    "#print(\"end of the mean_squared_error_gd with w=\",w,\" and loss=\", loss)"
>>>>>>> 518c9d40bcb72cf733cc121260794e88145a7722
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3abc58ab-10d3-4a01-b62a-6e3445c65b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end of the least_squares with w= [ 8.03494312e-05 -7.20202273e-03 -6.05417273e-03 -5.47559065e-04\n",
      " -1.93874700e-02  4.73451621e-04 -2.60379054e-02  3.25106300e-01\n",
      " -3.80780282e-05 -2.72724919e+00 -2.21220140e-01  9.50794091e-02\n",
      "  6.40351613e-02  2.73550887e+00 -3.31801241e-04 -9.54325120e-04\n",
      "  2.74026561e+00 -5.34164891e-04  9.73498581e-04  3.69225052e-03\n",
      "  3.54487449e-04 -5.43344598e-04 -3.30448035e-01 -1.40800498e-03\n",
      "  8.31432888e-04  1.02117272e-03 -1.68047416e-03 -5.83664818e-03\n",
      " -1.11087997e-02  2.72770912e+00]  and loss= 0.3396868094770935\n"
     ]
    }
   ],
   "source": [
    "# test least square\n",
    "w, loss = least_squares(y_tr, tx_tr)\n",
    "print(\"end of the least_squares with w=\",w,\" and loss=\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1f09a81-21c9-4587-ba44-9bb03ab90776",
   "metadata": {},
<<<<<<< HEAD
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, the loss=4936.0381532943165, the grad=-314.5195333333333\n",
      "Current iteration=1, the loss=14436640.242115596, the grad=16724.252848176577\n",
      "Current iteration=2, the loss=10.46562710329236, the grad=-3649.696678971624\n",
      "Current iteration=3, the loss=94832167159.25461, the grad=-1368980.7726393084\n",
      "Current iteration=4, the loss=1106237497009050.5, the grad=147548022.78783157\n",
      "Current iteration=5, the loss=8.786359462626438e+18, the grad=-13474669323.545958\n",
      "Current iteration=6, the loss=8.715869095178757e+22, the grad=1295811616599.5022\n",
      "Current iteration=7, the loss=8.527130641061444e+26, the grad=-132064272440744.6\n",
      "Current iteration=8, the loss=2.057196970796375e+30, the grad=5474234700356821.0\n",
      "Current iteration=9, the loss=2.153450955824883e+34, the grad=-6.722558672265441e+17\n",
      "Current iteration=10, the loss=1.7104679614023277e+38, the grad=5.71936909009453e+19\n",
      "Current iteration=11, the loss=9.058877671353836e+34, the grad=-1.683261720329536e+19\n",
      "Current iteration=12, the loss=1.736011639565544e+35, the grad=1.979250200126561e+19\n",
      "Current iteration=13, the loss=5.165084235715988e+35, the grad=-3.390940218851804e+19\n",
      "Current iteration=14, the loss=1.366945196606594e+42, the grad=-5.223409959712707e+21\n",
      "Current iteration=15, the loss=3.1983279301307837e+45, the grad=2.4526099060979365e+23\n",
      "Current iteration=16, the loss=3.0693534640949937e+49, the grad=-2.483814279759361e+25\n",
      "Current iteration=17, the loss=3.0221397196685567e+53, the grad=2.470143582169776e+27\n",
      "Current iteration=18, the loss=2.9595171387870196e+57, the grad=-2.4824990836749502e+29\n",
      "Current iteration=19, the loss=2.9035971318493575e+61, the grad=2.4479596404051336e+31\n",
      "Current iteration=20, the loss=6.850724143703867e+64, the grad=-1.098795242338876e+33\n",
      "Current iteration=21, the loss=4.0998604076756137e+68, the grad=6.048874668618549e+34\n",
      "Current iteration=22, the loss=3.272671305484412e+72, the grad=-8.27569029835629e+36\n",
      "Current iteration=23, the loss=7.784230866974903e+75, the grad=3.244032839667059e+38\n",
      "Current iteration=24, the loss=3.63845632866704e+79, the grad=-2.55391394685218e+40\n",
      "Current iteration=25, the loss=2.362639073333269e+78, the grad=8.774444811116658e+38\n",
      "Current iteration=26, the loss=3.1479127922761617e+83, the grad=2.446633768905853e+42\n",
      "Current iteration=27, the loss=3.0852509776242255e+87, the grad=-2.489848985373629e+44\n",
      "Current iteration=28, the loss=7.204235506460764e+90, the grad=1.1287283144423571e+46\n",
      "Current iteration=29, the loss=3.496262675810314e+94, the grad=-7.805002454136248e+47\n",
      "Current iteration=30, the loss=3.3277680974790764e+98, the grad=8.319172163222481e+49\n",
      "Current iteration=31, the loss=3.2768316779092244e+102, the grad=-8.140411569872762e+51\n",
      "Current iteration=32, the loss=1.2932604269726122e+102, the grad=1.6583531647560716e+52\n",
      "Current iteration=33, the loss=8.381535264889931e+105, the grad=4.145651848199155e+53\n",
      "Current iteration=34, the loss=9.818439970897681e+109, the grad=-4.507029573672725e+55\n",
      "Current iteration=35, the loss=3.2789158122154384e+113, the grad=2.3859086046029186e+57\n",
      "Current iteration=36, the loss=2.2935074875536205e+117, the grad=-2.164074451412585e+59\n",
      "Current iteration=37, the loss=2.063603563883872e+117, the grad=4.071821836625587e+59\n",
      "Current iteration=38, the loss=1.8258255314134626e+117, the grad=-1.1562578880489076e+60\n",
      "Current iteration=39, the loss=1.2475593376486079e+120, the grad=-4.5079956608778514e+60\n",
      "Current iteration=40, the loss=1.2629351481795902e+124, the grad=5.102354651643046e+62\n",
      "Current iteration=41, the loss=1.2527658708047927e+128, the grad=-4.9073308329404e+64\n",
      "Current iteration=42, the loss=2.6338291438680047e+124, the grad=8.978929128150616e+63\n",
      "Current iteration=43, the loss=1.3496715423247367e+132, the grad=5.2549110835908845e+66\n",
      "Current iteration=44, the loss=5.094085699751513e+133, the grad=-3.1922889542509885e+67\n",
      "Current iteration=45, the loss=2.400794625996939e+131, the grad=4.9092365832895395e+67\n",
      "Current iteration=46, the loss=2.0869804603714827e+136, the grad=5.984868534218628e+68\n",
      "Current iteration=47, the loss=2.7552341254805294e+133, the grad=-2.492868025596093e+68\n",
      "Current iteration=48, the loss=1.7017154309174856e+140, the grad=-5.772629843260845e+70\n",
      "Current iteration=49, the loss=1.9606145291204224e+144, the grad=6.2985476425737344e+72\n",
      "Current iteration=50, the loss=3.6596439386269886e+147, the grad=-2.572577462616934e+74\n",
      "Current iteration=51, the loss=1.0578784492685164e+142, the grad=1.5640097281203196e+73\n",
      "Current iteration=52, the loss=1.9571418761413585e+151, the grad=1.5725090603261704e+76\n",
      "Current iteration=53, the loss=8.895749604454304e+154, the grad=-1.2740820400667573e+78\n",
      "Current iteration=54, the loss=5.434356345415678e+158, the grad=1.0095781591221617e+80\n",
      "Current iteration=55, the loss=1.5232579554960954e+158, the grad=-1.9100255488642425e+80\n",
      "Current iteration=56, the loss=4.2027324678450975e+159, the grad=1.132130385816158e+81\n",
      "Current iteration=57, the loss=6.640225899736005e+163, the grad=-3.694689089637644e+82\n",
      "Current iteration=58, the loss=6.560961928026137e+167, the grad=3.604419051498885e+84\n",
      "Current iteration=59, the loss=1.989383888730068e+171, the grad=-1.876764599033678e+86\n",
      "Current iteration=60, the loss=2.3059523064273212e+175, the grad=2.16283057057859e+88\n",
      "Current iteration=61, the loss=1.8470428376302407e+179, the grad=-1.8687639094657397e+90\n",
      "Current iteration=62, the loss=5.406983310477497e+182, the grad=1.0136448054854828e+92\n",
      "Current iteration=63, the loss=3.825864836823266e+186, the grad=-8.853756343403246e+93\n",
      "Current iteration=64, the loss=8.954326272555702e+189, the grad=3.997814774511053e+95\n",
      "Current iteration=65, the loss=8.343405396606317e+180, the grad=-4.3479962337793976e+94\n",
      "Current iteration=66, the loss=4.275710559284777e+193, the grad=-2.7727300185487304e+97\n",
      "Current iteration=67, the loss=1.8028411036261992e+191, the grad=1.445689940420221e+97\n",
      "Current iteration=68, the loss=3.614100280571303e+197, the grad=2.6998424627765866e+99\n",
      "Current iteration=69, the loss=2.8852211318364506e+201, the grad=-2.442507137958459e+101\n",
      "Current iteration=70, the loss=2.828018815101145e+205, the grad=2.4126986268340908e+103\n",
      "Current iteration=71, the loss=1.211389791435559e+205, the grad=-3.917037944837498e+103\n",
      "Current iteration=72, the loss=1.6929002467096877e+209, the grad=-1.8721857219699568e+105\n",
      "Current iteration=73, the loss=1.6606109518521155e+213, the grad=1.857767335817276e+107\n",
      "Current iteration=74, the loss=1.9375924651707594e+217, the grad=-2.0138230572089104e+109\n",
      "Current iteration=75, the loss=4.9048660843070135e+213, the grad=5.222415454475067e+108\n",
      "Current iteration=76, the loss=3.648432618071435e+220, the grad=7.902437665521567e+110\n",
      "Current iteration=77, the loss=2.7539700066210487e+217, the grad=-1.7564867644541602e+110\n",
      "Current iteration=78, the loss=3.401092108717815e+224, the grad=-8.068610449321328e+112\n",
      "Current iteration=79, the loss=3.336577534841572e+228, the grad=8.200277656552001e+114\n",
      "Current iteration=80, the loss=7.82814602494794e+231, the grad=-3.6378795262720724e+116\n",
      "Current iteration=81, the loss=1.2191824221701236e+228, the grad=-1.6938674383789885e+115\n",
      "Current iteration=82, the loss=7.407596612564993e+235, the grad=3.823749603509933e+118\n",
      "Current iteration=83, the loss=1.7326810276968221e+239, the grad=-1.7495801601107314e+120\n",
      "Current iteration=84, the loss=8.289535597352822e+242, the grad=1.2254764143619295e+122\n",
      "Current iteration=85, the loss=5.1771978863362325e+241, the grad=-1.4785564070629956e+122\n",
      "Current iteration=86, the loss=2.6315707342833613e+246, the grad=-6.909782588147479e+123\n",
      "Current iteration=87, the loss=2.564047166953532e+250, the grad=7.171924043228093e+125\n",
      "Current iteration=88, the loss=2.5179428402712725e+254, the grad=-7.139083130653126e+127\n",
      "Current iteration=89, the loss=2.9278095009116157e+258, the grad=7.68480130443255e+129\n",
      "Current iteration=90, the loss=4.233016060616936e+256, the grad=-5.37671883864216e+129\n",
      "Current iteration=91, the loss=5.099963578935108e+261, the grad=-2.900189868440087e+131\n",
      "Current iteration=92, the loss=4.789050251115602e+265, the grad=3.110107412503646e+133\n",
      "Current iteration=93, the loss=4.722562555168627e+269, the grad=-3.0677440814685035e+135\n",
      "Current iteration=94, the loss=7.085774067626324e+268, the grad=4.579047881523666e+135\n",
      "Current iteration=95, the loss=1.843004894876618e+272, the grad=5.80038532807272e+136\n",
      "Current iteration=96, the loss=4.8984194733143767e+269, the grad=-2.2835928107505801e+136\n",
      "Current iteration=97, the loss=1.755367906007267e+276, the grad=-6.045033420806215e+138\n",
      "Current iteration=98, the loss=5.776035786861441e+279, the grad=3.3346660853208563e+140\n",
      "Current iteration=99, the loss=2.136417185384856e+283, the grad=-1.8307759891143303e+142\n",
      "end of the mean_squared_error_sgd with w= [-1.18480866e+137 -4.47843078e+136 -6.70449526e+136 -8.47038705e+136\n",
      "  9.15533267e+137  9.15533307e+137  9.15533277e+137 -1.70961481e+135\n",
      " -2.04222670e+136 -1.66782982e+137 -7.44580906e+134 -1.24403486e+135\n",
      "  9.15533268e+137 -4.57411735e+136  1.52756482e+135  9.66636654e+134\n",
      " -3.74269754e+136  1.79657056e+135 -7.57374678e+134 -3.41299784e+136\n",
      "  5.56516694e+134 -2.25713026e+137 -9.16267197e+134 -8.34427046e+136\n",
      "  7.49756897e+134 -2.58083623e+135  9.15533488e+137  9.15533263e+137\n",
      "  9.15533281e+137 -8.36139033e+136]  and loss= 2.136417185384856e+283\n"
     ]
    }
   ],
=======
   "outputs": [],
>>>>>>> 518c9d40bcb72cf733cc121260794e88145a7722
   "source": [
    "# test mean squared error sgd\n",
    "\n",
    "initial_w = np.zeros(tx_tr.shape[1])\n",
    "max_iters = 100\n",
    "gamma = 0.00001\n",
    "\n",
    "w, loss = mean_squared_error_sgd(y_tr, tx_tr, initial_w, max_iters, gamma)\n",
    "\n",
    "#print(\"end of the mean_squared_error_sgd with w=\",w,\" and loss=\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34c8ae3d-d431-4939-8e10-d0915a0fc194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current lambda=0.0001, the loss=0.33968798676609097\n",
      "Current lambda=0.00013738237958832623, the loss=0.33968803954866844\n",
      "Current lambda=0.00018873918221350977, the loss=0.33968813859041375\n",
      "Current lambda=0.0002592943797404667, the loss=0.3396883245147402\n",
      "Current lambda=0.0003562247890262444, the loss=0.3396886731920951\n",
      "Current lambda=0.0004893900918477494, the loss=0.3396893258059021\n",
      "Current lambda=0.0006723357536499335, the loss=0.33969054373021085\n",
      "Current lambda=0.0009236708571873865, the loss=0.33969280743924646\n",
      "Current lambda=0.0012689610031679222, the loss=0.3396969916896993\n",
      "Current lambda=0.0017433288221999873, the loss=0.33970466833847507\n",
      "Current lambda=0.002395026619987486, the loss=0.3397186122395501\n",
      "Current lambda=0.0032903445623126675, the loss=0.33974360637308687\n",
      "Current lambda=0.004520353656360241, the loss=0.33978763536530615\n",
      "Current lambda=0.006210169418915616, the loss=0.3398634706893294\n",
      "Current lambda=0.008531678524172805, the loss=0.33999040783666146\n",
      "Current lambda=0.011721022975334805, the loss=0.34019545283472974\n",
      "Current lambda=0.01610262027560939, the loss=0.3405126532289999\n",
      "Current lambda=0.02212216291070448, the loss=0.3409789456162246\n",
      "Current lambda=0.03039195382313198, the loss=0.34162564114876065\n",
      "Current lambda=0.041753189365604, the loss=0.34246709117086294\n",
      "Current lambda=0.05736152510448681, the loss=0.34349132395827114\n",
      "Current lambda=0.07880462815669913, the loss=0.34465867526889005\n",
      "Current lambda=0.1082636733874054, the loss=0.34591102937226975\n",
      "Current lambda=0.14873521072935117, the loss=0.34718767370275866\n",
      "Current lambda=0.20433597178569418, the loss=0.34843939327798257\n",
      "Current lambda=0.2807216203941176, the loss=0.3496342050611894\n",
      "Current lambda=0.38566204211634725, the loss=0.35075424601787725\n",
      "Current lambda=0.5298316906283708, the loss=0.351788603311994\n",
      "Current lambda=0.7278953843983146, the loss=0.35272780782090674\n",
      "Current lambda=1.0, the loss=0.3535627339505689\n",
      "best lambda is  0.0001\n",
      "end of the ridge_regression with w= [ 8.06960854e-05 -7.20578084e-03 -6.04517159e-03 -5.52739531e-04\n",
      " -1.94754484e-02  4.73682819e-04 -2.60450820e-02  3.24395228e-01\n",
      " -3.82847694e-05  4.40588717e-03 -2.20862099e-01  9.50768388e-02\n",
      "  6.40870833e-02  3.85465642e-03 -3.32608132e-04 -9.55589083e-04\n",
      "  8.59668812e-03 -5.32284362e-04  9.71536530e-04  3.69559157e-03\n",
      "  3.55703473e-04 -5.43892997e-04 -3.29573140e-01 -1.39974366e-03\n",
      "  8.26957837e-04  1.01646507e-03 -1.67549174e-03 -5.82354748e-03\n",
      " -1.10844940e-02 -3.95386302e-03]  and loss= 0.33968798676609097\n"
     ]
    }
   ],
   "source": [
    "# test ridge regression \n",
    "\n",
    "#initial_w = np.zeros(tx_tr.shape[1])\n",
    "#max_iters = 100\n",
    "#lambda_ = 0.0005\n",
    "\n",
    "#w, loss = ridge_regression(y_tr, tx_tr, lambda_)\n",
    "lambda_ = ridge_regression_best_lambda(y_tr, tx_tr)\n",
    "print(\"best lambda is \",lambda_)\n",
    "w, loss = ridge_regression(y_tr, tx_tr, lambda_)\n",
    "\n",
    "#y_pred = predict(w, tx_te)\n",
    "#y_pred[y_pred==0] = -1\n",
    "\n",
    "#OUTPUT_PATH = 'sample-submission'\n",
    "#create_csv_submission(ids_te, y_pred, OUTPUT_PATH)\n",
    "\n",
    "print(\"end of the ridge_regression with w=\",w,\" and loss=\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f5c7de-0c97-4975-93cb-71f4916eb96e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preiteration, the loss=265618.8699766104, the grad=-170.99750063180275\n",
      "Current iteration=0, the loss=nan, the grad=1821.1184651771323\n",
      "Current iteration=1, the loss=nan, the grad=815.7386220748331\n",
      "Current iteration=2, the loss=nan, the grad=813.6054342608572\n",
      "Current iteration=3, the loss=nan, the grad=813.0003288530146\n",
      "Current iteration=4, the loss=nan, the grad=812.8209466089727\n",
      "Current iteration=5, the loss=nan, the grad=812.7506651517383\n",
      "Current iteration=6, the loss=nan, the grad=812.7188287533983\n",
      "Current iteration=7, the loss=nan, the grad=812.7003201401884\n",
      "Current iteration=8, the loss=nan, the grad=812.6915794953517\n",
      "Current iteration=9, the loss=nan, the grad=812.6865340279271\n",
      "Current iteration=10, the loss=nan, the grad=812.6832479751233\n",
      "Current iteration=11, the loss=nan, the grad=812.6810870823336\n",
      "Current iteration=12, the loss=nan, the grad=812.6797985431555\n",
      "Current iteration=13, the loss=nan, the grad=812.678601503909\n",
      "Current iteration=14, the loss=nan, the grad=812.6778269222888\n",
      "Current iteration=15, the loss=nan, the grad=812.6772109091302\n",
      "Current iteration=16, the loss=nan, the grad=812.6766505546568\n",
      "Current iteration=17, the loss=nan, the grad=812.6763346266384\n",
      "Current iteration=18, the loss=nan, the grad=812.6760103848716\n",
      "Current iteration=19, the loss=nan, the grad=812.6758206561819\n",
      "Current iteration=20, the loss=nan, the grad=812.6756441394302\n",
      "Current iteration=21, the loss=nan, the grad=812.675497701842\n",
      "Current iteration=22, the loss=nan, the grad=812.6753979617931\n",
      "Current iteration=23, the loss=nan, the grad=812.6753242157214\n",
      "Current iteration=24, the loss=nan, the grad=812.6751998294986\n",
      "Current iteration=25, the loss=nan, the grad=812.6751460123052\n",
      "Current iteration=26, the loss=nan, the grad=812.6750851224252\n",
      "Current iteration=27, the loss=nan, the grad=812.6750280310421\n",
      "Current iteration=28, the loss=nan, the grad=812.6749722932477\n",
      "Current iteration=29, the loss=nan, the grad=812.6749093386751\n",
      "Current iteration=30, the loss=nan, the grad=812.6748634339243\n",
      "Current iteration=31, the loss=nan, the grad=812.67483150469\n",
      "Current iteration=32, the loss=nan, the grad=812.6748111484064\n",
      "Current iteration=33, the loss=nan, the grad=812.6748028599512\n",
      "Current iteration=34, the loss=nan, the grad=812.6747660918696\n",
      "Current iteration=35, the loss=nan, the grad=812.6747214931867\n",
      "Current iteration=36, the loss=nan, the grad=812.6747016225576\n",
      "Current iteration=37, the loss=nan, the grad=812.6746682074291\n",
      "Current iteration=38, the loss=nan, the grad=812.674623777697\n",
      "Current iteration=39, the loss=nan, the grad=812.6745662869901\n",
      "Current iteration=40, the loss=nan, the grad=812.6745400937206\n",
      "Current iteration=41, the loss=nan, the grad=812.6745314292214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test logistic regression\n",
    "\n",
    "initial_w = np.zeros(tx_tr.shape[1])\n",
    "# initial_w = w\n",
    "max_iters = 200\n",
    "gamma = 0.01\n",
    "\n",
    "w, loss = logistic_regression(y_tr, tx_tr, initial_w, max_iters, gamma)\n",
    "\n",
    "print(\"end of the logistic_regression with w=\",w,\" and loss=\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0b54129f-2e9e-489f-989e-cffe093f018d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[105.012 46.524 73.752 38.468 -999.000 -999.000 -999.000 2.492 12.316\n",
      " 120.665 1.280 -0.356 -999.000 31.804 -0.023 -0.033 40.516 -0.045 0.086\n",
      " 34.802 -0.024 179.739 1.000 38.960 -1.872 -2.093 -999.000 -999.000\n",
      " -999.000 40.513]\n",
      "[-49.023 49.240 81.182 57.896 -708.421 -601.237 -709.357 2.373 18.917\n",
      " 158.432 1.438 -0.128 -708.985 38.707 -0.011 -0.008 46.660 -0.020 0.044\n",
      " 41.717 -0.010 209.797 0.979 -348.330 -399.254 -399.260 -692.381 -709.122\n",
      " -709.119 73.065]\n",
      "(array([ 4,  5,  6, 12, 26, 27, 28]),)\n",
      "index : 4\n",
      "177457\n",
      "index : 5\n",
      "177457\n",
      "index : 6\n",
      "177457\n",
      "index : 12\n",
      "177457\n",
      "index : 26\n",
      "177457\n",
      "index : 27\n",
      "177457\n",
      "index : 28\n",
      "177457\n",
      "counts of each column  [177457, 177457, 177457, 177457, 177457, 177457, 177457]\n"
     ]
    }
   ],
   "source": [
    "#clean column , remove column that has only -999 elements. \n",
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.3f}\".format(x)})\n",
    "\n",
    "median = np.median(tx_tr, axis=0) \n",
    "mean = np.mean(tx_tr, axis=0) \n",
    "\n",
    "print(median)\n",
    "print(mean)\n",
    "\n",
    "indx = np.where(median == -999)\n",
    "print(indx)\n",
    "\n",
    "counts = []\n",
    "count = 0\n",
    "\n",
    "for i in indx[0]: \n",
    "    count = np.count_nonzero( tx_tr[:,i] == -999)\n",
    "    print(\"index :\", i)\n",
    "    print(count)\n",
    "    counts.append(count)\n",
    "\n",
    "print(\"counts of each column \", counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cd948ae1-5376-46f3-b929-6d5d233f0bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7,)\n",
      "(250000, 30)\n",
      "(250000, 23)\n"
     ]
    }
   ],
   "source": [
    "median = np.median(tx_tr, axis=0) \n",
    "indx = np.where(median == -999)\n",
    "tx1 = np.delete(tx_tr, indx[0], 1)\n",
    "print(indx[0].shape)\n",
    "print(tx_tr.shape)\n",
    "print(tx1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f062a94a-ff3d-4d4d-88b2-66b0ca8bf65c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
