{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "882a9018-5319-4a82-87e7-2b0d420475bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from implementations import *\n",
    "from helpers import *\n",
    "from helper_functions import *\n",
    "from optimization import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d56f0e3-9ee7-448f-a9dd-dd349209cc8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading the training dataset\n",
      "training dataset loaded\n"
     ]
    }
   ],
   "source": [
    "# load the training dataset\n",
    "print(\"loading the training dataset\")\n",
    "DATA_TRAIN_PATH = 'train.csv'\n",
    "y_tr, tx_tr, ids_tr = load_csv_data(DATA_TRAIN_PATH)\n",
    "print(\"training dataset loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af90503c-e3fa-4e1b-a222-63ef11b39a80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading the test dataset\n",
      "test dataset loaded\n"
     ]
    }
   ],
   "source": [
    "# load the test dataset\n",
    "print(\"loading the test dataset\")\n",
    "DATA_TEST_PATH = 'test.csv'\n",
    "y_te, tx_te, ids_te = load_csv_data(DATA_TEST_PATH)\n",
    "print(\"test dataset loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa8da103-df5b-4f1e-b595-9ae0cd91750e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shape of the training y is  (250000,)  and the training tx is  (250000, 30)\n",
      "the shape of the test y is  (568238,)  and the test tx is  (568238, 30)\n"
     ]
    }
   ],
   "source": [
    "# printing the shapes of the arrays\n",
    "print(\"the shape of the training y is \",y_tr.shape,\" and the training tx is \", tx_tr.shape)\n",
    "print(\"the shape of the test y is \",y_te.shape,\" and the test tx is \",tx_te.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6884129-6892-4ec8-bce6-fe61104b9ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing the mean square gd\n",
    "initial_w = np.zeros(tx_tr.shape[1])\n",
    "max_iters = 500\n",
    "gamma = 0.00008\n",
    "\n",
    "#w, loss = mean_squared_error_gd(y_tr, tx_tr, initial_w, max_iters, gamma)\n",
    "\n",
    "#print(\"end of the mean_squared_error_gd with w=\",w,\" and loss=\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3abc58ab-10d3-4a01-b62a-6e3445c65b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end of the least_squares with w= [ 8.03494312e-05 -7.20202273e-03 -6.05417273e-03 -5.47559065e-04\n",
      " -1.93874700e-02  4.73451621e-04 -2.60379054e-02  3.25106300e-01\n",
      " -3.80780282e-05 -2.72724919e+00 -2.21220140e-01  9.50794091e-02\n",
      "  6.40351613e-02  2.73550887e+00 -3.31801241e-04 -9.54325120e-04\n",
      "  2.74026561e+00 -5.34164891e-04  9.73498581e-04  3.69225052e-03\n",
      "  3.54487449e-04 -5.43344598e-04 -3.30448035e-01 -1.40800498e-03\n",
      "  8.31432888e-04  1.02117272e-03 -1.68047416e-03 -5.83664818e-03\n",
      " -1.11087997e-02  2.72770912e+00]  and loss= 0.3396868094770935\n"
     ]
    }
   ],
   "source": [
    "# test least square\n",
    "w, loss = least_squares(y_tr, tx_tr)\n",
    "print(\"end of the least_squares with w=\",w,\" and loss=\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1f09a81-21c9-4587-ba44-9bb03ab90776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test mean squared error sgd\n",
    "\n",
    "initial_w = np.zeros(tx_tr.shape[1])\n",
    "max_iters = 100\n",
    "gamma = 0.00001\n",
    "\n",
    "#w, loss = mean_squared_error_sgd(y_tr, tx_tr, initial_w, max_iters, gamma)\n",
    "\n",
    "#print(\"end of the mean_squared_error_sgd with w=\",w,\" and loss=\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34c8ae3d-d431-4939-8e10-d0915a0fc194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current lambda=0.0001, the loss=0.33968798676609097\n",
      "Current lambda=0.00013738237958832623, the loss=0.33968803954866844\n",
      "Current lambda=0.00018873918221350977, the loss=0.33968813859041375\n",
      "Current lambda=0.0002592943797404667, the loss=0.3396883245147402\n",
      "Current lambda=0.0003562247890262444, the loss=0.3396886731920951\n",
      "Current lambda=0.0004893900918477494, the loss=0.3396893258059021\n",
      "Current lambda=0.0006723357536499335, the loss=0.33969054373021085\n",
      "Current lambda=0.0009236708571873865, the loss=0.33969280743924646\n",
      "Current lambda=0.0012689610031679222, the loss=0.3396969916896993\n",
      "Current lambda=0.0017433288221999873, the loss=0.33970466833847507\n",
      "Current lambda=0.002395026619987486, the loss=0.3397186122395501\n",
      "Current lambda=0.0032903445623126675, the loss=0.33974360637308687\n",
      "Current lambda=0.004520353656360241, the loss=0.33978763536530615\n",
      "Current lambda=0.006210169418915616, the loss=0.3398634706893294\n",
      "Current lambda=0.008531678524172805, the loss=0.33999040783666146\n",
      "Current lambda=0.011721022975334805, the loss=0.34019545283472974\n",
      "Current lambda=0.01610262027560939, the loss=0.3405126532289999\n",
      "Current lambda=0.02212216291070448, the loss=0.3409789456162246\n",
      "Current lambda=0.03039195382313198, the loss=0.34162564114876065\n",
      "Current lambda=0.041753189365604, the loss=0.34246709117086294\n",
      "Current lambda=0.05736152510448681, the loss=0.34349132395827114\n",
      "Current lambda=0.07880462815669913, the loss=0.34465867526889005\n",
      "Current lambda=0.1082636733874054, the loss=0.34591102937226975\n",
      "Current lambda=0.14873521072935117, the loss=0.34718767370275866\n",
      "Current lambda=0.20433597178569418, the loss=0.34843939327798257\n",
      "Current lambda=0.2807216203941176, the loss=0.3496342050611894\n",
      "Current lambda=0.38566204211634725, the loss=0.35075424601787725\n",
      "Current lambda=0.5298316906283708, the loss=0.351788603311994\n",
      "Current lambda=0.7278953843983146, the loss=0.35272780782090674\n",
      "Current lambda=1.0, the loss=0.3535627339505689\n",
      "best lambda is  0.0001\n",
      "end of the ridge_regression with w= [ 8.06960854e-05 -7.20578084e-03 -6.04517159e-03 -5.52739531e-04\n",
      " -1.94754484e-02  4.73682819e-04 -2.60450820e-02  3.24395228e-01\n",
      " -3.82847694e-05  4.40588717e-03 -2.20862099e-01  9.50768388e-02\n",
      "  6.40870833e-02  3.85465642e-03 -3.32608132e-04 -9.55589083e-04\n",
      "  8.59668812e-03 -5.32284362e-04  9.71536530e-04  3.69559157e-03\n",
      "  3.55703473e-04 -5.43892997e-04 -3.29573140e-01 -1.39974366e-03\n",
      "  8.26957837e-04  1.01646507e-03 -1.67549174e-03 -5.82354748e-03\n",
      " -1.10844940e-02 -3.95386302e-03]  and loss= 0.33968798676609097\n"
     ]
    }
   ],
   "source": [
    "# test ridge regression \n",
    "\n",
    "#initial_w = np.zeros(tx_tr.shape[1])\n",
    "#max_iters = 100\n",
    "#lambda_ = 0.0005\n",
    "\n",
    "#w, loss = ridge_regression(y_tr, tx_tr, lambda_)\n",
    "lambda_ = ridge_regression_best_lambda(y_tr, tx_tr)\n",
    "print(\"best lambda is \",lambda_)\n",
    "w, loss = ridge_regression(y_tr, tx_tr, lambda_)\n",
    "\n",
    "#y_pred = predict(w, tx_te)\n",
    "#y_pred[y_pred==0] = -1\n",
    "\n",
    "#OUTPUT_PATH = 'sample-submission'\n",
    "#create_csv_submission(ids_te, y_pred, OUTPUT_PATH)\n",
    "\n",
    "print(\"end of the ridge_regression with w=\",w,\" and loss=\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d5f5c7de-0c97-4975-93cb-71f4916eb96e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preiteration, the loss=1.0624754799064415, the grad=-170.99750063180275\n",
      "Current iteration=0, the loss=-149.7720868305088, the grad=1821.1184651771323\n",
      "Current iteration=1, the loss=nan, the grad=815.5788169178264\n",
      "Current iteration=2, the loss=nan, the grad=813.6500927338228\n",
      "Current iteration=3, the loss=nan, the grad=813.0452435608873\n",
      "Current iteration=4, the loss=nan, the grad=812.8492027018372\n",
      "Current iteration=5, the loss=nan, the grad=812.7707192121253\n",
      "Current iteration=6, the loss=nan, the grad=812.7326595790746\n",
      "Current iteration=7, the loss=nan, the grad=812.7123415409455\n",
      "Current iteration=8, the loss=nan, the grad=812.7006958341075\n",
      "Current iteration=9, the loss=nan, the grad=812.6935745765511\n",
      "Current iteration=10, the loss=nan, the grad=812.6890129468179\n",
      "Current iteration=11, the loss=nan, the grad=812.6859442946064\n",
      "Current iteration=12, the loss=nan, the grad=812.683747698587\n",
      "Current iteration=13, the loss=nan, the grad=812.6821070083046\n",
      "Current iteration=14, the loss=nan, the grad=812.6808574461556\n",
      "Current iteration=15, the loss=nan, the grad=812.679884891601\n",
      "Current iteration=16, the loss=nan, the grad=812.6791105962295\n",
      "Current iteration=17, the loss=nan, the grad=812.6784892873648\n",
      "Current iteration=18, the loss=nan, the grad=812.6779897049339\n",
      "Current iteration=19, the loss=nan, the grad=812.677584716923\n",
      "Current iteration=20, the loss=nan, the grad=812.6772514877405\n",
      "Current iteration=21, the loss=nan, the grad=812.6769716250969\n",
      "Current iteration=22, the loss=nan, the grad=812.6767317161062\n",
      "Current iteration=23, the loss=nan, the grad=812.6765226752393\n",
      "Current iteration=24, the loss=nan, the grad=812.6763385315076\n",
      "Current iteration=25, the loss=nan, the grad=812.6761753516431\n",
      "Current iteration=26, the loss=nan, the grad=812.6760303142065\n",
      "Current iteration=27, the loss=nan, the grad=812.6759011122765\n",
      "Current iteration=28, the loss=nan, the grad=812.6757857472659\n",
      "Current iteration=29, the loss=nan, the grad=812.6756825102402\n",
      "Current iteration=30, the loss=nan, the grad=812.6755899270647\n",
      "Current iteration=31, the loss=nan, the grad=812.6755066179464\n",
      "Current iteration=32, the loss=nan, the grad=812.6754311738458\n",
      "Current iteration=33, the loss=nan, the grad=812.6753621618232\n",
      "Current iteration=34, the loss=nan, the grad=812.6752982654014\n",
      "Current iteration=35, the loss=nan, the grad=812.6752384781541\n",
      "Current iteration=36, the loss=nan, the grad=812.6751822627864\n",
      "Current iteration=37, the loss=nan, the grad=812.6751295839613\n",
      "Current iteration=38, the loss=nan, the grad=812.6750807315738\n",
      "Current iteration=39, the loss=nan, the grad=812.6750360046588\n",
      "Current iteration=40, the loss=nan, the grad=812.6749954789572\n",
      "Current iteration=41, the loss=nan, the grad=812.6749589701142\n",
      "Current iteration=42, the loss=nan, the grad=812.6749261122488\n",
      "Current iteration=43, the loss=nan, the grad=812.6748964500686\n",
      "Current iteration=44, the loss=nan, the grad=812.6748695096574\n",
      "Current iteration=45, the loss=nan, the grad=812.6748448479026\n",
      "Current iteration=46, the loss=nan, the grad=812.6748220833792\n",
      "Current iteration=47, the loss=nan, the grad=812.6748009099603\n",
      "Current iteration=48, the loss=nan, the grad=812.6747810960069\n",
      "Current iteration=49, the loss=nan, the grad=812.6747624742187\n",
      "Current iteration=50, the loss=nan, the grad=812.6747449276979\n",
      "Current iteration=51, the loss=nan, the grad=812.6747283761708\n",
      "Current iteration=52, the loss=nan, the grad=812.6747127639458\n",
      "Current iteration=53, the loss=nan, the grad=812.6746980496296\n",
      "Current iteration=54, the loss=nan, the grad=812.674684197353\n",
      "Current iteration=55, the loss=nan, the grad=812.6746711697098\n",
      "Current iteration=56, the loss=nan, the grad=812.6746589229535\n",
      "Current iteration=57, the loss=nan, the grad=812.6746474048889\n",
      "Current iteration=58, the loss=nan, the grad=812.6746365554371\n",
      "Current iteration=59, the loss=nan, the grad=812.6746263093145\n",
      "Current iteration=60, the loss=nan, the grad=812.6746165999268\n",
      "Current iteration=61, the loss=nan, the grad=812.6746073635177\n",
      "Current iteration=62, the loss=nan, the grad=812.674598542782\n",
      "Current iteration=63, the loss=nan, the grad=812.6745900894251\n",
      "Current iteration=64, the loss=nan, the grad=812.6745819654207\n",
      "Current iteration=65, the loss=nan, the grad=812.6745741429597\n",
      "Current iteration=66, the loss=nan, the grad=812.6745666032909\n",
      "Current iteration=67, the loss=nan, the grad=812.674559334801\n",
      "Current iteration=68, the loss=nan, the grad=812.6745523307617\n",
      "Current iteration=69, the loss=nan, the grad=812.6745455871463\n",
      "Current iteration=70, the loss=nan, the grad=812.6745391008059\n",
      "Current iteration=71, the loss=nan, the grad=812.6745328681424\n",
      "Current iteration=72, the loss=nan, the grad=812.6745268842933\n",
      "Current iteration=73, the loss=nan, the grad=812.674521142762\n",
      "Current iteration=74, the loss=nan, the grad=812.6745156354004\n",
      "Current iteration=75, the loss=nan, the grad=812.6745103526574\n",
      "Current iteration=76, the loss=nan, the grad=812.6745052840067\n",
      "Current iteration=77, the loss=nan, the grad=812.6745004184755\n",
      "Current iteration=78, the loss=nan, the grad=812.6744957451905\n",
      "Current iteration=79, the loss=nan, the grad=812.6744912538675\n",
      "Current iteration=80, the loss=nan, the grad=812.6744869351794\n",
      "Current iteration=81, the loss=nan, the grad=812.6744827809651\n",
      "Current iteration=82, the loss=nan, the grad=812.6744787842729\n",
      "Current iteration=83, the loss=nan, the grad=812.6744749392495\n",
      "Current iteration=84, the loss=nan, the grad=812.6744712409221\n",
      "Current iteration=85, the loss=nan, the grad=812.6744676849214\n",
      "Current iteration=86, the loss=nan, the grad=812.6744642671975\n",
      "Current iteration=87, the loss=nan, the grad=812.6744609837733\n",
      "Current iteration=88, the loss=nan, the grad=812.6744578305663\n",
      "Current iteration=89, the loss=nan, the grad=812.6744548032877\n",
      "Current iteration=90, the loss=nan, the grad=812.6744518974168\n",
      "Current iteration=91, the loss=nan, the grad=812.6744491082369\n",
      "Current iteration=92, the loss=nan, the grad=812.67444643091\n",
      "Current iteration=93, the loss=nan, the grad=812.6744438605702\n",
      "Current iteration=94, the loss=nan, the grad=812.674441392415\n",
      "Current iteration=95, the loss=nan, the grad=812.6744390217837\n",
      "Current iteration=96, the loss=nan, the grad=812.6744367442116\n",
      "Current iteration=97, the loss=nan, the grad=812.6744345554605\n",
      "Current iteration=98, the loss=nan, the grad=812.6744324515266\n",
      "Current iteration=99, the loss=nan, the grad=812.6744304286287\n",
      "end of the logistic_regression with w= [ 8.71911594e-01 -3.37605863e-01 -4.09224714e-01 -2.63735683e-01\n",
      "  2.86970248e+00  2.17496312e+00  2.87572845e+00 -1.13304566e-02\n",
      " -1.09530505e-01 -8.55305012e-01 -8.80458934e-03  2.27321874e-03\n",
      "  2.87385412e+00 -1.54960087e-01  2.82967198e-05 -2.83725966e-05\n",
      " -2.47744869e-01  8.53146884e-05 -1.33348078e-04 -2.26117569e-01\n",
      "  1.53647055e-04 -1.10796894e+00 -6.05511250e-03  1.70565134e+00\n",
      "  1.97515827e+00  1.97517798e+00  2.74219455e+00  2.87473868e+00\n",
      "  2.87467764e+00 -4.52600048e-01]  and loss= nan\n"
     ]
    }
   ],
   "source": [
    "# test logistic regression\n",
    "\n",
    "initial_w = np.zeros(tx_tr.shape[1])\n",
    "# initial_w = w\n",
    "max_iters = 100\n",
    "gamma = 0.0001\n",
    "\n",
    "w, loss = logistic_regression(y_tr, tx_tr, initial_w, max_iters, gamma)\n",
    "\n",
    "print(\"end of the logistic_regression with w=\",w,\" and loss=\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b54129f-2e9e-489f-989e-cffe093f018d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
