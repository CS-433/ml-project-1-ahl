{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "882a9018-5319-4a82-87e7-2b0d420475bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from implementations import *\n",
    "from helpers import *\n",
    "from helper_functions import *\n",
    "from optimization import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d56f0e3-9ee7-448f-a9dd-dd349209cc8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading the training dataset\n",
      "training dataset loaded\n"
     ]
    }
   ],
   "source": [
    "# load the training dataset\n",
    "print(\"loading the training dataset\")\n",
    "DATA_TRAIN_PATH = 'train.csv'\n",
    "y_tr, tx_tr, ids_tr = load_csv_data(DATA_TRAIN_PATH)\n",
    "print(\"training dataset loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af90503c-e3fa-4e1b-a222-63ef11b39a80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading the test dataset\n",
      "test dataset loaded\n"
     ]
    }
   ],
   "source": [
    "# load the test dataset\n",
    "print(\"loading the test dataset\")\n",
    "DATA_TEST_PATH = 'test.csv'\n",
    "y_te, tx_te, ids_te = load_csv_data(DATA_TEST_PATH)\n",
    "print(\"test dataset loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa8da103-df5b-4f1e-b595-9ae0cd91750e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shape of the training y is  (250000,)  and the training tx is  (250000, 30)\n",
      "the shape of the test y is  (568238,)  and the test tx is  (568238, 30)\n"
     ]
    }
   ],
   "source": [
    "# printing the shapes of the arrays\n",
    "print(\"the shape of the training y is \",y_tr.shape,\" and the training tx is \", tx_tr.shape)\n",
    "print(\"the shape of the test y is \",y_te.shape,\" and the test tx is \",tx_te.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6884129-6892-4ec8-bce6-fe61104b9ff5",
   "metadata": {},
<<<<<<< HEAD
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, the loss=0.4486458187302863, the grad=-83.42721036786665\n",
      "Current iteration=1, the loss=0.4377103722046582, the grad=33.98783214577216\n",
      "Current iteration=2, the loss=0.43449433074074234, the grad=-16.60357059571331\n",
      "Current iteration=3, the loss=0.4328322302044497, the grad=5.629515266934457\n",
      "Current iteration=4, the loss=0.4315514189916519, the grad=-3.7473774149053196\n",
      "Current iteration=5, the loss=0.4304204692525385, the grad=0.5577518598071749\n",
      "Current iteration=6, the loss=0.4293867701908119, the grad=-1.0934697641288125\n",
      "Current iteration=7, the loss=0.42843278516364114, the grad=-0.18221172512174794\n",
      "Current iteration=8, the loss=0.42754873444983027, the grad=-0.400517308847955\n",
      "Current iteration=9, the loss=0.42672723509157223, the grad=-0.14535207634151168\n",
      "Current iteration=10, the loss=0.4259621180594481, the grad=-0.11015833020600423\n",
      "Current iteration=11, the loss=0.42524806823948674, the grad=0.007306161636387198\n",
      "Current iteration=12, the loss=0.42458044802821815, the grad=0.07732206680594446\n",
      "Current iteration=13, the loss=0.42395517787287845, the grad=0.15733890981941295\n",
      "Current iteration=14, the loss=0.4233686449591719, the grad=0.22350310182679245\n",
      "Current iteration=15, the loss=0.42281763078895, the grad=0.28709906619895914\n",
      "Current iteration=16, the loss=0.42229925293889053, the grad=0.3440679623542085\n",
      "Current iteration=17, the loss=0.4218109178197018, the grad=0.39691180631538026\n",
      "Current iteration=18, the loss=0.42135028206116887, the grad=0.4452056006422906\n",
      "Current iteration=19, the loss=0.42091522069931736, the grad=0.48972149454009284\n",
      "end of the mean_squared_error_gd with w= [ 2.67514040e-04 -5.72615115e-05 -2.09496651e-05  5.34768152e-06\n",
      "  2.86867308e-05  1.19844098e-04  2.71679875e-05 -2.72606761e-07\n",
      " -1.15919469e-05 -3.63832852e-05 -1.12366383e-06  9.00290573e-07\n",
      "  2.84706382e-05  1.13694466e-05 -1.18417000e-08 -3.91224329e-08\n",
      " -1.78378608e-05 -7.20677489e-09  2.63648932e-08 -1.33707458e-05\n",
      "  5.15849415e-08 -5.20441569e-05 -5.35842877e-07  4.71168458e-05\n",
      "  5.26067999e-05  5.26036811e-05  1.61636422e-05  2.83966001e-05\n",
      "  2.83750807e-05 -2.99148650e-05]  and loss= 0.42091522069931736\n",
      "Current iteration=0, the loss=0.457316555397448, the grad=-83.42721036786665\n",
      "Current iteration=1, the loss=0.44200621657627187, the grad=45.729336397136045\n",
      "Current iteration=2, the loss=0.4359182357448747, the grad=-28.401915596561647\n",
      "Current iteration=3, the loss=0.4329936250497594, the grad=14.725006617712037\n",
      "Current iteration=4, the loss=0.4312024086797886, the grad=-9.846997609005745\n",
      "Current iteration=5, the loss=0.4298584072487603, the grad=4.611251850340437\n",
      "Current iteration=6, the loss=0.4287246670438193, the grad=-3.481407449596053\n",
      "Current iteration=7, the loss=0.42771521836182497, the grad=1.4123032543752447\n",
      "Current iteration=8, the loss=0.42679547791209166, the grad=-1.2103805586933516\n",
      "Current iteration=9, the loss=0.4259488607630753, the grad=0.4837857220237211\n",
      "Current iteration=10, the loss=0.42516548976207896, the grad=-0.3309204964172562\n",
      "Current iteration=11, the loss=0.4244383021448915, the grad=0.2861383869180834\n",
      "Current iteration=12, the loss=0.42376164910797703, the grad=0.06290406350177905\n",
      "Current iteration=13, the loss=0.4231307481868747, the grad=0.31196473692412074\n",
      "Current iteration=14, the loss=0.4225414368850391, the grad=0.2772910263457175\n",
      "Current iteration=15, the loss=0.42199003980750127, the grad=0.39632565755620236\n",
      "Current iteration=16, the loss=0.4214732837004787, the grad=0.41770417512829183\n",
      "Current iteration=17, the loss=0.4209882362272007, the grad=0.4873927295085933\n",
      "Current iteration=18, the loss=0.42053225867366856, the grad=0.521977929316899\n",
      "Current iteration=19, the loss=0.420102967987544, the grad=0.5703074095555821\n",
      "end of the mean_squared_error_gd with w= [ 2.84031310e-04 -6.23640596e-05 -2.28654317e-05  5.81931961e-06\n",
      "  2.73258040e-05  1.26607103e-04  2.56670770e-05 -2.94815760e-07\n",
      " -1.26769827e-05 -3.98650946e-05 -1.22952138e-06  9.78331549e-07\n",
      "  2.70919088e-05  1.25992561e-05 -1.31599221e-08 -4.27896898e-08\n",
      " -1.94678178e-05 -8.36380527e-09  2.91945746e-08 -1.43334461e-05\n",
      "  5.63547546e-08 -5.69639308e-05 -5.90872505e-07  4.63421460e-05\n",
      "  5.24832401e-05  5.24798991e-05  1.35456176e-05  2.70111584e-05\n",
      "  2.69875146e-05 -3.29965264e-05]  and loss= 0.420102967987544\n",
      "Current iteration=0, the loss=0.4684975020271792, the grad=-83.42721036786665\n",
      "Current iteration=1, the loss=0.4513712748460005, the grad=57.47084064849991\n",
      "Current iteration=2, the loss=0.44178360296424807, the grad=-43.56038950251244\n",
      "Current iteration=3, the loss=0.43617479340076165, the grad=29.634952887893135\n",
      "Current iteration=4, the loss=0.43268849485123373, the grad=-22.729614001422586\n",
      "Current iteration=5, the loss=0.4303539841023341, the grad=15.315259096391403\n",
      "Current iteration=6, the loss=0.4286614226647361, the grad=-11.8067735215727\n",
      "Current iteration=7, the loss=0.4273411475179239, the grad=7.984549042057253\n",
      "Current iteration=8, the loss=0.4262489193466734, the grad=-6.048032125698609\n",
      "Current iteration=9, the loss=0.4253062112549615, the grad=4.261249734588027\n",
      "Current iteration=10, the loss=0.4244691000973839, the grad=-2.986527485289154\n",
      "Current iteration=11, the loss=0.42371203005243574, the grad=2.3947844917048813\n",
      "Current iteration=12, the loss=0.42301931548757576, the grad=-1.3383102126144695\n",
      "Current iteration=13, the loss=0.4223806759656577, the grad=1.4799234723976127\n",
      "Current iteration=14, the loss=0.4217888755503783, the grad=-0.43423796096158623\n",
      "Current iteration=15, the loss=0.4212384630175911, the grad=1.0493464324630073\n",
      "Current iteration=16, the loss=0.42072509010349285, the grad=0.07509110540316791\n",
      "Current iteration=17, the loss=0.4202451348565581, the grad=0.8624059779866046\n",
      "Current iteration=18, the loss=0.4197954872779463, the grad=0.3726429743660339\n",
      "Current iteration=19, the loss=0.4193734222616614, the grad=0.7956553535672023\n",
      "end of the mean_squared_error_gd with w= [ 2.99303128e-04 -6.74121694e-05 -2.47936774e-05  6.29185568e-06\n",
      "  2.59492690e-05  1.33228981e-04  2.41522150e-05 -3.17665630e-07\n",
      " -1.37558013e-05 -4.33295151e-05 -1.33511550e-06  1.05539491e-06\n",
      "  2.56980156e-05  1.38282081e-05 -1.44811869e-08 -4.64149369e-08\n",
      " -2.10940736e-05 -9.54703223e-09  3.20294756e-08 -1.52562711e-05\n",
      "  6.10874385e-08 -6.18692921e-05 -6.45817775e-07  4.54554009e-05\n",
      "  5.22440217e-05  5.22404697e-05  1.09174774e-05  2.56106206e-05\n",
      "  2.55848560e-05 -3.60636423e-05]  and loss= 0.4193734222616614\n",
      "Current iteration=0, the loss=0.4821886586194801, the grad=-83.42721036786665\n",
      "Current iteration=1, the loss=0.46860585690459233, the grad=69.2123448998638\n",
      "Current iteration=2, the loss=0.4581982565934716, the grad=-62.07899231356577\n",
      "Current iteration=3, the loss=0.4501791146011084, the grad=51.80433968063336\n",
      "Current iteration=4, the loss=0.44396036370956915, the grad=-46.14460924735117\n",
      "Current iteration=5, the loss=0.4391020524505416, the grad=38.82485784870273\n",
      "Current iteration=6, the loss=0.43527469843105515, the grad=-34.24753475619125\n",
      "Current iteration=7, the loss=0.43223124983630795, the grad=29.150561452745343\n",
      "Current iteration=8, the loss=0.42978619726701733, the grad=-25.361892668173176\n",
      "Current iteration=9, the loss=0.42780000787273786, the grad=21.942463138596235\n",
      "Current iteration=10, the loss=0.42616752190857976, the grad=-18.72302285177043\n",
      "Current iteration=11, the loss=0.42480929992374106, the grad=16.574048644656003\n",
      "Current iteration=12, the loss=0.42366516763748774, the grad=-13.76088037412461\n",
      "Current iteration=13, the loss=0.4226893980829994, the grad=12.577547713882488\n",
      "Current iteration=14, the loss=0.4218471138206511, the grad=-10.050432798082106\n",
      "Current iteration=15, the loss=0.4211115985844365, the grad=9.603776219531271\n",
      "Current iteration=16, the loss=0.42046228702505306, the grad=-7.274713169969255\n",
      "Current iteration=17, the loss=0.41988326023516365, the grad=7.392125309493045\n",
      "Current iteration=18, the loss=0.4193621186789887, the grad=-5.1972967005627355\n",
      "Current iteration=19, the loss=0.4188891368607531, the grad=5.748136342425167\n",
      "end of the mean_squared_error_gd with w= [ 3.13157165e-04 -7.22747054e-05 -2.65292899e-05  6.84724851e-06\n",
      "  2.20410507e-05  1.37160381e-04  2.01076388e-05 -3.34791000e-07\n",
      " -1.47919067e-05 -4.65226753e-05 -1.43694985e-06  1.13035253e-06\n",
      "  2.17730147e-05  1.51449606e-05 -1.58447488e-08 -5.00320823e-08\n",
      " -2.26074081e-05 -1.08280509e-08  3.49772044e-08 -1.60560247e-05\n",
      "  6.57421887e-08 -6.63789547e-05 -6.99776220e-07  4.29928327e-05\n",
      "  5.03658770e-05  5.03621103e-05  5.76054327e-06  2.16790696e-05\n",
      "  2.16511868e-05 -3.90602199e-05]  and loss= 0.4188891368607531\n",
      "Current iteration=0, the loss=0.49839002517435055, the grad=-83.42721036786665\n",
      "Current iteration=1, the loss=0.49712916702472487, the grad=80.95384915122766\n",
      "Current iteration=2, the loss=0.49615134971816305, the grad=-83.95772402972155\n",
      "Current iteration=3, the loss=0.49540731213947303, the grad=82.6781525990882\n",
      "Current iteration=4, the loss=0.49485973377372255, the grad=-84.67000296440553\n",
      "Current iteration=5, the loss=0.4944798525022932, the grad=84.27983045459091\n",
      "Current iteration=6, the loss=0.49424510863290133, the grad=-85.51726186796863\n",
      "Current iteration=7, the loss=0.49413749582288047, the grad=85.79837452682933\n",
      "Current iteration=8, the loss=0.4941423998017128, the grad=-86.4669427736661\n",
      "Current iteration=9, the loss=0.49424777448218005, the grad=87.2614708512579\n",
      "Current iteration=10, the loss=0.49444355211753077, the grad=-87.49629999629353\n",
      "Current iteration=11, the loss=0.49472121642631833, the grad=88.68865753024191\n",
      "Current iteration=12, the loss=0.4950734897310871, the grad=-88.58936928828858\n",
      "Current iteration=13, the loss=0.4954941003357032, the grad=90.09383670405104\n",
      "Current iteration=14, the loss=0.49597760678581554, the grad=-89.73488562366303\n",
      "Current iteration=15, the loss=0.4965192628154739, the grad=91.48700113089821\n",
      "Current iteration=16, the loss=0.49711491170604316, the grad=-90.92485127987145\n",
      "Current iteration=17, the loss=0.4977609021738314, the grad=92.87542227833092\n",
      "Current iteration=18, the loss=0.49845402024155405, the grad=-92.15354987167741\n",
      "Current iteration=19, the loss=0.49919143316562337, the grad=94.2644690562147\n",
      "end of the mean_squared_error_gd with w= [ 3.20294824e-04 -7.42452392e-05 -2.39869353e-05  9.06038972e-06\n",
      " -3.51198787e-05  8.68758041e-05 -3.71800570e-05 -2.19166776e-07\n",
      " -1.50570839e-05 -4.44673125e-05 -1.46480780e-06  1.17833153e-06\n",
      " -3.54008655e-05  1.83143487e-05 -1.80590686e-08 -5.42741597e-08\n",
      " -2.18267447e-05 -1.37188983e-08  4.02029128e-08 -1.50310816e-05\n",
      "  6.94359151e-08 -6.29804155e-05 -7.35787024e-07  8.65704037e-06\n",
      "  1.54312785e-05  1.54270186e-05 -5.27123110e-05 -3.55005824e-05\n",
      " -3.55305909e-05 -4.09549081e-05]  and loss= 0.49919143316562337\n",
      "[0.42091522069931736, 0.420102967987544, 0.4193734222616614, 0.4188891368607531, 0.49919143316562337]\n"
     ]
    }
   ],
=======
   "outputs": [],
>>>>>>> 518c9d40bcb72cf733cc121260794e88145a7722
   "source": [
    "# testing the mean square gd\n",
    "#initial_w = w\n",
    "max_iters = 20\n",
    "# gamma = 0.0000002\n",
    "gamma = np.linspace(0.00000025, 0.00000035, 5)\n",
    "losses = []\n",
    "\n",
<<<<<<< HEAD
    "for g in gamma:\n",
    "    initial_w = np.zeros(tx_tr.shape[1])\n",
    "    w, loss = mean_squared_error_gd(y_tr, tx_tr, initial_w, max_iters, g)\n",
    "    print(\"end of the mean_squared_error_gd with w=\",w,\" and loss=\", loss)\n",
    "    losses.append(loss)\n",
    "\n",
    "    \n",
    "print(losses)"
=======
    "#w, loss = mean_squared_error_gd(y_tr, tx_tr, initial_w, max_iters, gamma)\n",
    "\n",
    "#print(\"end of the mean_squared_error_gd with w=\",w,\" and loss=\", loss)"
>>>>>>> 518c9d40bcb72cf733cc121260794e88145a7722
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3abc58ab-10d3-4a01-b62a-6e3445c65b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end of the least_squares with w= [ 2.90775824e-02 -2.58174342e-01 -2.58075231e-01 -3.15695998e-02\n",
      "  2.70148463e-01 -3.56244790e-02 -3.77596359e+02 -1.88764858e-01\n",
      "  1.26980572e-01  7.33360865e+01 -7.90497167e-04 -9.03464001e-04\n",
      "  7.22984805e+01 -8.65751516e-04  2.23848044e-03  1.25628556e-01\n",
      "  8.13634257e-04 -7.51316144e-02  6.61315259e-02  2.08838944e-01\n",
      " -1.01704341e-01 -6.91450673e-02  3.19799465e+02]  and loss= 0.4016547492014886\n"
     ]
    }
   ],
   "source": [
    "# test least square\n",
    "txOpti = dataClean(tx_tr)\n",
    "w, loss = least_squares(y_tr, txOpti)\n",
    "print(\"end of the least_squares with w=\",w,\" and loss=\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1f09a81-21c9-4587-ba44-9bb03ab90776",
   "metadata": {},
<<<<<<< HEAD
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, the loss=4936.0381532943165, the grad=-314.5195333333333\n",
      "Current iteration=1, the loss=14436640.242115596, the grad=16724.252848176577\n",
      "Current iteration=2, the loss=10.46562710329236, the grad=-3649.696678971624\n",
      "Current iteration=3, the loss=94832167159.25461, the grad=-1368980.7726393084\n",
      "Current iteration=4, the loss=1106237497009050.5, the grad=147548022.78783157\n",
      "Current iteration=5, the loss=8.786359462626438e+18, the grad=-13474669323.545958\n",
      "Current iteration=6, the loss=8.715869095178757e+22, the grad=1295811616599.5022\n",
      "Current iteration=7, the loss=8.527130641061444e+26, the grad=-132064272440744.6\n",
      "Current iteration=8, the loss=2.057196970796375e+30, the grad=5474234700356821.0\n",
      "Current iteration=9, the loss=2.153450955824883e+34, the grad=-6.722558672265441e+17\n",
      "Current iteration=10, the loss=1.7104679614023277e+38, the grad=5.71936909009453e+19\n",
      "Current iteration=11, the loss=9.058877671353836e+34, the grad=-1.683261720329536e+19\n",
      "Current iteration=12, the loss=1.736011639565544e+35, the grad=1.979250200126561e+19\n",
      "Current iteration=13, the loss=5.165084235715988e+35, the grad=-3.390940218851804e+19\n",
      "Current iteration=14, the loss=1.366945196606594e+42, the grad=-5.223409959712707e+21\n",
      "Current iteration=15, the loss=3.1983279301307837e+45, the grad=2.4526099060979365e+23\n",
      "Current iteration=16, the loss=3.0693534640949937e+49, the grad=-2.483814279759361e+25\n",
      "Current iteration=17, the loss=3.0221397196685567e+53, the grad=2.470143582169776e+27\n",
      "Current iteration=18, the loss=2.9595171387870196e+57, the grad=-2.4824990836749502e+29\n",
      "Current iteration=19, the loss=2.9035971318493575e+61, the grad=2.4479596404051336e+31\n",
      "Current iteration=20, the loss=6.850724143703867e+64, the grad=-1.098795242338876e+33\n",
      "Current iteration=21, the loss=4.0998604076756137e+68, the grad=6.048874668618549e+34\n",
      "Current iteration=22, the loss=3.272671305484412e+72, the grad=-8.27569029835629e+36\n",
      "Current iteration=23, the loss=7.784230866974903e+75, the grad=3.244032839667059e+38\n",
      "Current iteration=24, the loss=3.63845632866704e+79, the grad=-2.55391394685218e+40\n",
      "Current iteration=25, the loss=2.362639073333269e+78, the grad=8.774444811116658e+38\n",
      "Current iteration=26, the loss=3.1479127922761617e+83, the grad=2.446633768905853e+42\n",
      "Current iteration=27, the loss=3.0852509776242255e+87, the grad=-2.489848985373629e+44\n",
      "Current iteration=28, the loss=7.204235506460764e+90, the grad=1.1287283144423571e+46\n",
      "Current iteration=29, the loss=3.496262675810314e+94, the grad=-7.805002454136248e+47\n",
      "Current iteration=30, the loss=3.3277680974790764e+98, the grad=8.319172163222481e+49\n",
      "Current iteration=31, the loss=3.2768316779092244e+102, the grad=-8.140411569872762e+51\n",
      "Current iteration=32, the loss=1.2932604269726122e+102, the grad=1.6583531647560716e+52\n",
      "Current iteration=33, the loss=8.381535264889931e+105, the grad=4.145651848199155e+53\n",
      "Current iteration=34, the loss=9.818439970897681e+109, the grad=-4.507029573672725e+55\n",
      "Current iteration=35, the loss=3.2789158122154384e+113, the grad=2.3859086046029186e+57\n",
      "Current iteration=36, the loss=2.2935074875536205e+117, the grad=-2.164074451412585e+59\n",
      "Current iteration=37, the loss=2.063603563883872e+117, the grad=4.071821836625587e+59\n",
      "Current iteration=38, the loss=1.8258255314134626e+117, the grad=-1.1562578880489076e+60\n",
      "Current iteration=39, the loss=1.2475593376486079e+120, the grad=-4.5079956608778514e+60\n",
      "Current iteration=40, the loss=1.2629351481795902e+124, the grad=5.102354651643046e+62\n",
      "Current iteration=41, the loss=1.2527658708047927e+128, the grad=-4.9073308329404e+64\n",
      "Current iteration=42, the loss=2.6338291438680047e+124, the grad=8.978929128150616e+63\n",
      "Current iteration=43, the loss=1.3496715423247367e+132, the grad=5.2549110835908845e+66\n",
      "Current iteration=44, the loss=5.094085699751513e+133, the grad=-3.1922889542509885e+67\n",
      "Current iteration=45, the loss=2.400794625996939e+131, the grad=4.9092365832895395e+67\n",
      "Current iteration=46, the loss=2.0869804603714827e+136, the grad=5.984868534218628e+68\n",
      "Current iteration=47, the loss=2.7552341254805294e+133, the grad=-2.492868025596093e+68\n",
      "Current iteration=48, the loss=1.7017154309174856e+140, the grad=-5.772629843260845e+70\n",
      "Current iteration=49, the loss=1.9606145291204224e+144, the grad=6.2985476425737344e+72\n",
      "Current iteration=50, the loss=3.6596439386269886e+147, the grad=-2.572577462616934e+74\n",
      "Current iteration=51, the loss=1.0578784492685164e+142, the grad=1.5640097281203196e+73\n",
      "Current iteration=52, the loss=1.9571418761413585e+151, the grad=1.5725090603261704e+76\n",
      "Current iteration=53, the loss=8.895749604454304e+154, the grad=-1.2740820400667573e+78\n",
      "Current iteration=54, the loss=5.434356345415678e+158, the grad=1.0095781591221617e+80\n",
      "Current iteration=55, the loss=1.5232579554960954e+158, the grad=-1.9100255488642425e+80\n",
      "Current iteration=56, the loss=4.2027324678450975e+159, the grad=1.132130385816158e+81\n",
      "Current iteration=57, the loss=6.640225899736005e+163, the grad=-3.694689089637644e+82\n",
      "Current iteration=58, the loss=6.560961928026137e+167, the grad=3.604419051498885e+84\n",
      "Current iteration=59, the loss=1.989383888730068e+171, the grad=-1.876764599033678e+86\n",
      "Current iteration=60, the loss=2.3059523064273212e+175, the grad=2.16283057057859e+88\n",
      "Current iteration=61, the loss=1.8470428376302407e+179, the grad=-1.8687639094657397e+90\n",
      "Current iteration=62, the loss=5.406983310477497e+182, the grad=1.0136448054854828e+92\n",
      "Current iteration=63, the loss=3.825864836823266e+186, the grad=-8.853756343403246e+93\n",
      "Current iteration=64, the loss=8.954326272555702e+189, the grad=3.997814774511053e+95\n",
      "Current iteration=65, the loss=8.343405396606317e+180, the grad=-4.3479962337793976e+94\n",
      "Current iteration=66, the loss=4.275710559284777e+193, the grad=-2.7727300185487304e+97\n",
      "Current iteration=67, the loss=1.8028411036261992e+191, the grad=1.445689940420221e+97\n",
      "Current iteration=68, the loss=3.614100280571303e+197, the grad=2.6998424627765866e+99\n",
      "Current iteration=69, the loss=2.8852211318364506e+201, the grad=-2.442507137958459e+101\n",
      "Current iteration=70, the loss=2.828018815101145e+205, the grad=2.4126986268340908e+103\n",
      "Current iteration=71, the loss=1.211389791435559e+205, the grad=-3.917037944837498e+103\n",
      "Current iteration=72, the loss=1.6929002467096877e+209, the grad=-1.8721857219699568e+105\n",
      "Current iteration=73, the loss=1.6606109518521155e+213, the grad=1.857767335817276e+107\n",
      "Current iteration=74, the loss=1.9375924651707594e+217, the grad=-2.0138230572089104e+109\n",
      "Current iteration=75, the loss=4.9048660843070135e+213, the grad=5.222415454475067e+108\n",
      "Current iteration=76, the loss=3.648432618071435e+220, the grad=7.902437665521567e+110\n",
      "Current iteration=77, the loss=2.7539700066210487e+217, the grad=-1.7564867644541602e+110\n",
      "Current iteration=78, the loss=3.401092108717815e+224, the grad=-8.068610449321328e+112\n",
      "Current iteration=79, the loss=3.336577534841572e+228, the grad=8.200277656552001e+114\n",
      "Current iteration=80, the loss=7.82814602494794e+231, the grad=-3.6378795262720724e+116\n",
      "Current iteration=81, the loss=1.2191824221701236e+228, the grad=-1.6938674383789885e+115\n",
      "Current iteration=82, the loss=7.407596612564993e+235, the grad=3.823749603509933e+118\n",
      "Current iteration=83, the loss=1.7326810276968221e+239, the grad=-1.7495801601107314e+120\n",
      "Current iteration=84, the loss=8.289535597352822e+242, the grad=1.2254764143619295e+122\n",
      "Current iteration=85, the loss=5.1771978863362325e+241, the grad=-1.4785564070629956e+122\n",
      "Current iteration=86, the loss=2.6315707342833613e+246, the grad=-6.909782588147479e+123\n",
      "Current iteration=87, the loss=2.564047166953532e+250, the grad=7.171924043228093e+125\n",
      "Current iteration=88, the loss=2.5179428402712725e+254, the grad=-7.139083130653126e+127\n",
      "Current iteration=89, the loss=2.9278095009116157e+258, the grad=7.68480130443255e+129\n",
      "Current iteration=90, the loss=4.233016060616936e+256, the grad=-5.37671883864216e+129\n",
      "Current iteration=91, the loss=5.099963578935108e+261, the grad=-2.900189868440087e+131\n",
      "Current iteration=92, the loss=4.789050251115602e+265, the grad=3.110107412503646e+133\n",
      "Current iteration=93, the loss=4.722562555168627e+269, the grad=-3.0677440814685035e+135\n",
      "Current iteration=94, the loss=7.085774067626324e+268, the grad=4.579047881523666e+135\n",
      "Current iteration=95, the loss=1.843004894876618e+272, the grad=5.80038532807272e+136\n",
      "Current iteration=96, the loss=4.8984194733143767e+269, the grad=-2.2835928107505801e+136\n",
      "Current iteration=97, the loss=1.755367906007267e+276, the grad=-6.045033420806215e+138\n",
      "Current iteration=98, the loss=5.776035786861441e+279, the grad=3.3346660853208563e+140\n",
      "Current iteration=99, the loss=2.136417185384856e+283, the grad=-1.8307759891143303e+142\n",
      "end of the mean_squared_error_sgd with w= [-1.18480866e+137 -4.47843078e+136 -6.70449526e+136 -8.47038705e+136\n",
      "  9.15533267e+137  9.15533307e+137  9.15533277e+137 -1.70961481e+135\n",
      " -2.04222670e+136 -1.66782982e+137 -7.44580906e+134 -1.24403486e+135\n",
      "  9.15533268e+137 -4.57411735e+136  1.52756482e+135  9.66636654e+134\n",
      " -3.74269754e+136  1.79657056e+135 -7.57374678e+134 -3.41299784e+136\n",
      "  5.56516694e+134 -2.25713026e+137 -9.16267197e+134 -8.34427046e+136\n",
      "  7.49756897e+134 -2.58083623e+135  9.15533488e+137  9.15533263e+137\n",
      "  9.15533281e+137 -8.36139033e+136]  and loss= 2.136417185384856e+283\n"
     ]
    }
   ],
=======
   "outputs": [],
>>>>>>> 518c9d40bcb72cf733cc121260794e88145a7722
   "source": [
    "# test mean squared error sgd\n",
    "\n",
    "initial_w = np.zeros(tx_tr.shape[1])\n",
    "max_iters = 100\n",
    "gamma = 0.00001\n",
    "\n",
    "w, loss = mean_squared_error_sgd(y_tr, tx_tr, initial_w, max_iters, gamma)\n",
    "\n",
    "#print(\"end of the mean_squared_error_sgd with w=\",w,\" and loss=\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34c8ae3d-d431-4939-8e10-d0915a0fc194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of txOpti  (250000, 23)\n",
      "Current lambda=1e-07, the loss=0.4016563436270216\n",
      "Current lambda=1.0302040816326533e-05, the loss=0.40165637140640603\n",
      "Current lambda=2.0504081632653063e-05, the loss=0.4016564423195787\n",
      "Current lambda=3.07061224489796e-05, the loss=0.4016565470725474\n",
      "Current lambda=4.090816326530613e-05, the loss=0.40165667800708466\n",
      "Current lambda=5.111020408163266e-05, the loss=0.40165682906733735\n",
      "Current lambda=6.13122448979592e-05, the loss=0.40165699547523187\n",
      "Current lambda=7.151428571428572e-05, the loss=0.4016571734530141\n",
      "Current lambda=8.171632653061226e-05, the loss=0.40165736000674596\n",
      "Current lambda=9.19183673469388e-05, the loss=0.4016575527592416\n",
      "Current lambda=0.00010212040816326532, the loss=0.40165774982091546\n",
      "Current lambda=0.00011232244897959185, the loss=0.40165794968944485\n",
      "Current lambda=0.0001225244897959184, the loss=0.40165815117138287\n",
      "Current lambda=0.00013272653061224493, the loss=0.4016583533205778\n",
      "Current lambda=0.00014292857142857144, the loss=0.4016585553895404\n",
      "Current lambda=0.00015313061224489797, the loss=0.40165875679084\n",
      "Current lambda=0.0001633326530612245, the loss=0.4016589570663222\n",
      "Current lambda=0.00017353469387755105, the loss=0.40165915586245426\n",
      "Current lambda=0.00018373673469387758, the loss=0.40165935291049903\n",
      "Current lambda=0.0001939387755102041, the loss=0.4016595480105147\n",
      "Current lambda=0.00020414081632653063, the loss=0.4016597410183984\n",
      "Current lambda=0.00021434285714285717, the loss=0.401659931835361\n",
      "Current lambda=0.0002245448979591837, the loss=0.40166012039935334\n",
      "Current lambda=0.00023474693877551024, the loss=0.40166030667806785\n",
      "Current lambda=0.0002449489795918368, the loss=0.40166049066320914\n",
      "Current lambda=0.0002551510204081633, the loss=0.4016606723657988\n",
      "Current lambda=0.00026535306122448985, the loss=0.40166085181231925\n",
      "Current lambda=0.00027555510204081636, the loss=0.4016610290415424\n",
      "Current lambda=0.00028575714285714287, the loss=0.4016612041019227\n",
      "Current lambda=0.00029595918367346944, the loss=0.4016613770494485\n",
      "Current lambda=0.00030616122448979595, the loss=0.4016615479458734\n",
      "Current lambda=0.0003163632653061225, the loss=0.4016617168572607\n",
      "Current lambda=0.000326565306122449, the loss=0.40166188385278484\n",
      "Current lambda=0.00033676734693877553, the loss=0.40166204900374813\n",
      "Current lambda=0.0003469693877551021, the loss=0.4016622123827731\n",
      "Current lambda=0.0003571714285714286, the loss=0.4016623740631429\n",
      "Current lambda=0.00036737346938775517, the loss=0.40166253411826286\n",
      "Current lambda=0.0003775755102040817, the loss=0.40166269262122406\n",
      "Current lambda=0.0003877775510204082, the loss=0.40166284964445104\n",
      "Current lambda=0.00039797959183673475, the loss=0.4016630052594204\n",
      "Current lambda=0.00040818163265306126, the loss=0.4016631595364373\n",
      "Current lambda=0.0004183836734693878, the loss=0.40166331254445975\n",
      "Current lambda=0.00042858571428571433, the loss=0.4016634643509662\n",
      "Current lambda=0.00043878775510204084, the loss=0.4016636150218529\n",
      "Current lambda=0.0004489897959183674, the loss=0.40166376462136266\n",
      "Current lambda=0.0004591918367346939, the loss=0.4016639132120344\n",
      "Current lambda=0.0004693938775510205, the loss=0.40166406085467316\n",
      "Current lambda=0.000479595918367347, the loss=0.401664207608334\n",
      "Current lambda=0.0004897979591836735, the loss=0.4016643535303215\n",
      "Current lambda=0.0005, the loss=0.40166449867619763\n",
      "best lambda is  1e-07\n",
      "end of the ridge_regression with w= [ 0.02907017 -0.25818776 -0.25807243 -0.03156359  0.27015066 -0.03562999\n",
      " -0.0023772  -0.188761    0.12697698  0.19676287 -0.00079167 -0.00090381\n",
      "  0.29206947 -0.00086357  0.00223468  0.12563208  0.00081575 -0.07511911\n",
      "  0.0661325   0.20866507 -0.10087154 -0.06981304 -0.06369628]  and loss= 0.4016563436270216\n"
     ]
    }
   ],
   "source": [
    "# test ridge regression \n",
    "\n",
    "txOpti = dataClean(tx_tr)\n",
    "print(\"shape of txOpti \", txOpti.shape)\n",
    "\n",
    "lambda_ = ridge_regression_best_lambda(y_tr, txOpti)\n",
    "print(\"best lambda is \",lambda_)\n",
    "w, loss = ridge_regression(y_tr, txOpti, lambda_)\n",
    "\n",
    "#y_pred = predict(w, tx_te)\n",
    "#y_pred[y_pred==0] = -1\n",
    "\n",
    "#OUTPUT_PATH = 'sample-submission'\n",
    "#create_csv_submission(ids_te, y_pred, OUTPUT_PATH)\n",
    "\n",
    "print(\"end of the ridge_regression with w=\",w,\" and loss=\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5f5c7de-0c97-4975-93cb-71f4916eb96e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of txOpti  (250000, 23)\n",
      "Preiteration, the loss=1.0624754799064415, the grad=-0.0572895281497229\n",
      "Current iteration=0, the loss=1.062317401733531, the grad=0.6943905988992103\n",
      "Current iteration=1, the loss=1.062159385930291, the grad=0.69397148489038\n",
      "Current iteration=2, the loss=1.0620014325047544, the grad=0.693552790847094\n",
      "Current iteration=3, the loss=1.0618435414647618, the grad=0.6931345166924245\n",
      "Current iteration=4, the loss=1.0616857128179562, the grad=0.6927166623474981\n",
      "Current iteration=5, the loss=1.0615279465717868, the grad=0.6922992277315012\n",
      "Current iteration=6, the loss=1.061370242733509, the grad=0.6918822127616887\n",
      "Current iteration=7, the loss=1.061212601310186, the grad=0.6914656173533885\n",
      "Current iteration=8, the loss=1.061055022308688, the grad=0.6910494414200097\n",
      "Current iteration=9, the loss=1.060897505735696, the grad=0.6906336848730477\n",
      "Current iteration=10, the loss=1.0607400515976986, the grad=0.6902183476220917\n",
      "Current iteration=11, the loss=1.060582659900995, the grad=0.6898034295748311\n",
      "Current iteration=12, the loss=1.060425330651698, the grad=0.6893889306370623\n",
      "Current iteration=13, the loss=1.0602680638557305, the grad=0.6889748507126949\n",
      "Current iteration=14, the loss=1.060110859518828, the grad=0.6885611897037588\n",
      "Current iteration=15, the loss=1.0599537176465421, the grad=0.6881479475104102\n",
      "Current iteration=16, the loss=1.0597966382442356, the grad=0.6877351240309395\n",
      "Current iteration=17, the loss=1.05963962131709, the grad=0.6873227191617768\n",
      "Current iteration=18, the loss=1.059482666870101, the grad=0.686910732797499\n",
      "Current iteration=19, the loss=1.059325774908081, the grad=0.6864991648308367\n",
      "Current iteration=20, the loss=1.0591689454356623, the grad=0.6860880151526807\n",
      "Current iteration=21, the loss=1.0590121784572935, the grad=0.685677283652089\n",
      "Current iteration=22, the loss=1.058855473977243, the grad=0.6852669702162935\n",
      "Current iteration=23, the loss=1.0586988319996, the grad=0.6848570747307063\n",
      "Current iteration=24, the loss=1.058542252528274, the grad=0.6844475970789273\n",
      "Current iteration=25, the loss=1.0583857355669966, the grad=0.6840385371427504\n",
      "Current iteration=26, the loss=1.0582292811193221, the grad=0.6836298948021705\n",
      "Current iteration=27, the loss=1.0580728891886269, the grad=0.6832216699353899\n",
      "Current iteration=28, the loss=1.0579165597781126, the grad=0.6828138624188265\n",
      "Current iteration=29, the loss=1.057760292890805, the grad=0.682406472127119\n",
      "Current iteration=30, the loss=1.0576040885295555, the grad=0.6819994989331342\n",
      "Current iteration=31, the loss=1.057447946697042, the grad=0.6815929427079749\n",
      "Current iteration=32, the loss=1.057291867395769, the grad=0.6811868033209852\n",
      "Current iteration=33, the loss=1.0571358506280697, the grad=0.6807810806397584\n",
      "Current iteration=34, the loss=1.056979896396105, the grad=0.6803757745301441\n",
      "Current iteration=35, the loss=1.0568240047018658, the grad=0.6799708848562539\n",
      "Current iteration=36, the loss=1.0566681755471732, the grad=0.6795664114804699\n",
      "Current iteration=37, the loss=1.0565124089336786, the grad=0.6791623542634498\n",
      "Current iteration=38, the loss=1.0563567048628648, the grad=0.6787587130641356\n",
      "Current iteration=39, the loss=1.0562010633360484, the grad=0.6783554877397594\n",
      "Current iteration=40, the loss=1.056045484354377, the grad=0.6779526781458513\n",
      "Current iteration=41, the loss=1.0558899679188338, the grad=0.6775502841362443\n",
      "Current iteration=42, the loss=1.0557345140302363, the grad=0.6771483055630847\n",
      "Current iteration=43, the loss=1.0555791226892364, the grad=0.6767467422768352\n",
      "Current iteration=44, the loss=1.055423793896323, the grad=0.6763455941262853\n",
      "Current iteration=45, the loss=1.055268527651821, the grad=0.6759448609585557\n",
      "Current iteration=46, the loss=1.0551133239558939, the grad=0.6755445426191067\n",
      "Current iteration=47, the loss=1.0549581828085417, the grad=0.6751446389517447\n",
      "Current iteration=48, the loss=1.0548031042096053, the grad=0.67474514979863\n",
      "Current iteration=49, the loss=1.0546480881587643, the grad=0.674346075000282\n",
      "Current iteration=50, the loss=1.0544931346555386, the grad=0.673947414395588\n",
      "Current iteration=51, the loss=1.0543382436992899, the grad=0.6735491678218102\n",
      "Current iteration=52, the loss=1.0541834152892204, the grad=0.6731513351145907\n",
      "Current iteration=53, the loss=1.0540286494243762, the grad=0.6727539161079609\n",
      "Current iteration=54, the loss=1.0538739461036462, the grad=0.6723569106343469\n",
      "Current iteration=55, the loss=1.053719305325763, the grad=0.6719603185245778\n",
      "Current iteration=56, the loss=1.0535647270893043, the grad=0.6715641396078923\n",
      "Current iteration=57, the loss=1.0534102113926929, the grad=0.6711683737119448\n",
      "Current iteration=58, the loss=1.053255758234197, the grad=0.6707730206628137\n",
      "Current iteration=59, the loss=1.053101367611933, the grad=0.6703780802850078\n",
      "Current iteration=60, the loss=1.0529470395238631, the grad=0.6699835524014738\n",
      "Current iteration=61, the loss=1.0527927739678, the grad=0.6695894368336026\n",
      "Current iteration=62, the loss=1.0526385709414017, the grad=0.6691957334012375\n",
      "Current iteration=63, the loss=1.052484430442179, the grad=0.6688024419226799\n",
      "Current iteration=64, the loss=1.0523303524674912, the grad=0.6684095622146982\n",
      "Current iteration=65, the loss=1.0521763370145494, the grad=0.6680170940925321\n",
      "Current iteration=66, the loss=1.0520223840804144, the grad=0.667625037369903\n",
      "Current iteration=67, the loss=1.0518684936620017, the grad=0.6672333918590178\n",
      "Current iteration=68, the loss=1.0517146657560785, the grad=0.666842157370579\n",
      "Current iteration=69, the loss=1.051560900359265, the grad=0.6664513337137895\n",
      "Current iteration=70, the loss=1.051407197468036, the grad=0.6660609206963609\n",
      "Current iteration=71, the loss=1.0512535570787216, the grad=0.6656709181245196\n",
      "Current iteration=72, the loss=1.0510999791875077, the grad=0.6652813258030149\n",
      "Current iteration=73, the loss=1.050946463790436, the grad=0.6648921435351259\n",
      "Current iteration=74, the loss=1.050793010883404, the grad=0.6645033711226678\n",
      "Current iteration=75, the loss=1.0506396204621684, the grad=0.6641150083659996\n",
      "Current iteration=76, the loss=1.050486292522344, the grad=0.6637270550640311\n",
      "Current iteration=77, the loss=1.0503330270594031, the grad=0.6633395110142297\n",
      "Current iteration=78, the loss=1.0501798240686786, the grad=0.6629523760126281\n",
      "Current iteration=79, the loss=1.0500266835453629, the grad=0.6625656498538307\n",
      "Current iteration=80, the loss=1.04987360548451, the grad=0.662179332331021\n",
      "Current iteration=81, the loss=1.0497205898810336, the grad=0.6617934232359687\n",
      "Current iteration=82, the loss=1.0495676367297118, the grad=0.661407922359036\n",
      "Current iteration=83, the loss=1.049414746025184, the grad=0.6610228294891863\n",
      "Current iteration=84, the loss=1.0492619177619524, the grad=0.6606381444139898\n",
      "Current iteration=85, the loss=1.0491091519343845, the grad=0.660253866919631\n",
      "Current iteration=86, the loss=1.0489564485367113, the grad=0.6598699967909157\n",
      "Current iteration=87, the loss=1.0488038075630293, the grad=0.6594865338112783\n",
      "Current iteration=88, the loss=1.0486512290073011, the grad=0.6591034777627888\n",
      "Current iteration=89, the loss=1.0484987128633558, the grad=0.6587208284261596\n",
      "Current iteration=90, the loss=1.048346259124889, the grad=0.6583385855807524\n",
      "Current iteration=91, the loss=1.048193867785464, the grad=0.6579567490045864\n",
      "Current iteration=92, the loss=1.0480415388385136, the grad=0.6575753184743431\n",
      "Current iteration=93, the loss=1.047889272277338, the grad=0.6571942937653759\n",
      "Current iteration=94, the loss=1.047737068095107, the grad=0.6568136746517154\n",
      "Current iteration=95, the loss=1.0475849262848618, the grad=0.6564334609060765\n",
      "Current iteration=96, the loss=1.0474328468395133, the grad=0.6560536522998667\n",
      "Current iteration=97, the loss=1.0472808297518432, the grad=0.6556742486031913\n",
      "Current iteration=98, the loss=1.0471288750145065, the grad=0.6552952495848626\n",
      "Current iteration=99, the loss=1.04697698262003, the grad=0.6549166550124043\n",
      "Current iteration=100, the loss=1.046825152560814, the grad=0.6545384646520601\n",
      "Current iteration=101, the loss=1.046673384829131, the grad=0.6541606782688015\n",
      "Current iteration=102, the loss=1.0465216794171295, the grad=0.6537832956263325\n",
      "Current iteration=103, the loss=1.046370036316833, the grad=0.6534063164870983\n",
      "Current iteration=104, the loss=1.046218455520139, the grad=0.6530297406122912\n",
      "Current iteration=105, the loss=1.046066937018822, the grad=0.6526535677618595\n",
      "Current iteration=106, the loss=1.045915480804533, the grad=0.6522777976945111\n",
      "Current iteration=107, the loss=1.0457640868688, the grad=0.6519024301677239\n",
      "Current iteration=108, the loss=1.0456127552030288, the grad=0.6515274649377509\n",
      "Current iteration=109, the loss=1.045461485798504, the grad=0.651152901759627\n",
      "Current iteration=110, the loss=1.0453102786463881, the grad=0.6507787403871771\n",
      "Current iteration=111, the loss=1.0451591337377246, the grad=0.6504049805730213\n",
      "Current iteration=112, the loss=1.0450080510634356, the grad=0.6500316220685843\n",
      "Current iteration=113, the loss=1.0448570306143246, the grad=0.6496586646240994\n",
      "Current iteration=114, the loss=1.044706072381076, the grad=0.6492861079886177\n",
      "Current iteration=115, the loss=1.0445551763542558, the grad=0.6489139519100139\n",
      "Current iteration=116, the loss=1.0444043425243135, the grad=0.6485421961349932\n",
      "Current iteration=117, the loss=1.044253570881579, the grad=0.6481708404090986\n",
      "Current iteration=118, the loss=1.044102861416268, the grad=0.6477998844767173\n",
      "Current iteration=119, the loss=1.0439522141184794, the grad=0.6474293280810876\n",
      "Current iteration=120, the loss=1.0438016289781957, the grad=0.6470591709643065\n",
      "Current iteration=121, the loss=1.0436511059852858, the grad=0.6466894128673354\n",
      "Current iteration=122, the loss=1.043500645129503, the grad=0.6463200535300079\n",
      "Current iteration=123, the loss=1.0433502464004878, the grad=0.6459510926910353\n",
      "Current iteration=124, the loss=1.0431999097877664, the grad=0.6455825300880149\n",
      "Current iteration=125, the loss=1.0430496352807526, the grad=0.6452143654574366\n",
      "Current iteration=126, the loss=1.042899422868748, the grad=0.6448465985346883\n",
      "Current iteration=127, the loss=1.0427492725409426, the grad=0.6444792290540636\n",
      "Current iteration=128, the loss=1.0425991842864153, the grad=0.6441122567487696\n",
      "Current iteration=129, the loss=1.0424491580941337, the grad=0.6437456813509312\n",
      "Current iteration=130, the loss=1.0422991939529556, the grad=0.6433795025916009\n",
      "Current iteration=131, the loss=1.0421492918516295, the grad=0.643013720200762\n",
      "Current iteration=132, the loss=1.0419994517787943, the grad=0.6426483339073381\n",
      "Current iteration=133, the loss=1.0418496737229803, the grad=0.6422833434391992\n",
      "Current iteration=134, the loss=1.0416999576726103, the grad=0.6419187485231677\n",
      "Current iteration=135, the loss=1.0415503036159985, the grad=0.6415545488850255\n",
      "Current iteration=136, the loss=1.041400711541354, the grad=0.6411907442495202\n",
      "Current iteration=137, the loss=1.0412511814367769, the grad=0.6408273343403729\n",
      "Current iteration=138, the loss=1.0411017132902631, the grad=0.6404643188802833\n",
      "Current iteration=139, the loss=1.0409523070897022, the grad=0.6401016975909379\n",
      "Current iteration=140, the loss=1.0408029628228792, the grad=0.6397394701930158\n",
      "Current iteration=141, the loss=1.0406536804774746, the grad=0.6393776364061946\n",
      "Current iteration=142, the loss=1.0405044600410636, the grad=0.6390161959491585\n",
      "Current iteration=143, the loss=1.04035530150112, the grad=0.6386551485396036\n",
      "Current iteration=144, the loss=1.040206204845013, the grad=0.6382944938942456\n",
      "Current iteration=145, the loss=1.0400571700600103, the grad=0.6379342317288257\n",
      "Current iteration=146, the loss=1.039908197133276, the grad=0.6375743617581165\n",
      "Current iteration=147, the loss=1.0397592860518752, the grad=0.6372148836959298\n",
      "Current iteration=148, the loss=1.039610436802769, the grad=0.6368557972551224\n",
      "Current iteration=149, the loss=1.0394616493728206, the grad=0.6364971021476031\n",
      "Current iteration=150, the loss=1.0393129237487917, the grad=0.6361387980843384\n",
      "Current iteration=151, the loss=1.039164259917345, the grad=0.6357808847753592\n",
      "Current iteration=152, the loss=1.039015657865042, the grad=0.635423361929768\n",
      "Current iteration=153, the loss=1.0388671175783486, the grad=0.6350662292557446\n",
      "Current iteration=154, the loss=1.0387186390436312, the grad=0.6347094864605526\n",
      "Current iteration=155, the loss=1.0385702222471584, the grad=0.6343531332505463\n",
      "Current iteration=156, the loss=1.0384218671751015, the grad=0.6339971693311762\n",
      "Current iteration=157, the loss=1.0382735738135351, the grad=0.6336415944069965\n",
      "Current iteration=158, the loss=1.0381253421484378, the grad=0.6332864081816706\n",
      "Current iteration=159, the loss=1.0379771721656923, the grad=0.6329316103579778\n",
      "Current iteration=160, the loss=1.0378290638510852, the grad=0.6325772006378199\n",
      "Current iteration=161, the loss=1.0376810171903097, the grad=0.6322231787222266\n",
      "Current iteration=162, the loss=1.0375330321689629, the grad=0.6318695443113632\n",
      "Current iteration=163, the loss=1.0373851087725494, the grad=0.6315162971045357\n",
      "Current iteration=164, the loss=1.0372372469864792, the grad=0.6311634368001975\n",
      "Current iteration=165, the loss=1.037089446796069, the grad=0.6308109630959563\n",
      "Current iteration=166, the loss=1.0369417081865449, the grad=0.6304588756885788\n",
      "Current iteration=167, the loss=1.0367940311430377, the grad=0.6301071742739988\n",
      "Current iteration=168, the loss=1.0366464156505895, the grad=0.6297558585473217\n",
      "Current iteration=169, the loss=1.0364988616941493, the grad=0.6294049282028324\n",
      "Current iteration=170, the loss=1.0363513692585755, the grad=0.6290543829339996\n",
      "Current iteration=171, the loss=1.0362039383286368, the grad=0.6287042224334838\n",
      "Current iteration=172, the loss=1.036056568889011, the grad=0.6283544463931423\n",
      "Current iteration=173, the loss=1.0359092609242877, the grad=0.6280050545040354\n",
      "Current iteration=174, the loss=1.0357620144189656, the grad=0.6276560464564334\n",
      "Current iteration=175, the loss=1.0356148293574563, the grad=0.6273074219398217\n",
      "Current iteration=176, the loss=1.0354677057240824, the grad=0.6269591806429072\n",
      "Current iteration=177, the loss=1.0353206435030793, the grad=0.6266113222536248\n",
      "Current iteration=178, the loss=1.0351736426785947, the grad=0.6262638464591427\n",
      "Current iteration=179, the loss=1.0350267032346887, the grad=0.6259167529458695\n",
      "Current iteration=180, the loss=1.034879825155336, the grad=0.6255700413994584\n",
      "Current iteration=181, the loss=1.034733008424425, the grad=0.625223711504816\n",
      "Current iteration=182, the loss=1.0345862530257581, the grad=0.6248777629461053\n",
      "Current iteration=183, the loss=1.0344395589430528, the grad=0.6245321954067541\n",
      "Current iteration=184, the loss=1.0342929261599412, the grad=0.6241870085694592\n",
      "Current iteration=185, the loss=1.0341463546599714, the grad=0.6238422021161931\n",
      "Current iteration=186, the loss=1.0339998444266079, the grad=0.6234977757282099\n",
      "Current iteration=187, the loss=1.033853395443231, the grad=0.6231537290860518\n",
      "Current iteration=188, the loss=1.0337070076931383, the grad=0.6228100618695537\n",
      "Current iteration=189, the loss=1.0335606811595437, the grad=0.6224667737578495\n",
      "Current iteration=190, the loss=1.0334144158255805, the grad=0.6221238644293792\n",
      "Current iteration=191, the loss=1.0332682116742982, the grad=0.6217813335618926\n",
      "Current iteration=192, the loss=1.033122068688666, the grad=0.621439180832457\n",
      "Current iteration=193, the loss=1.0329759868515715, the grad=0.621097405917462\n",
      "Current iteration=194, the loss=1.0328299661458216, the grad=0.6207560084926254\n",
      "Current iteration=195, the loss=1.032684006554143, the grad=0.6204149882329988\n",
      "Current iteration=196, the loss=1.0325381080591822, the grad=0.6200743448129746\n",
      "Current iteration=197, the loss=1.0323922706435062, the grad=0.6197340779062898\n",
      "Current iteration=198, the loss=1.0322464942896032, the grad=0.619394187186033\n",
      "Current iteration=199, the loss=1.0321007789798828, the grad=0.6190546723246497\n",
      "Current iteration=200, the loss=1.0319551246966752, the grad=0.6187155329939481\n",
      "Current iteration=201, the loss=1.0318095314222337, the grad=0.6183767688651047\n",
      "Current iteration=202, the loss=1.0316639991387333, the grad=0.6180383796086698\n",
      "Current iteration=203, the loss=1.0315185278282721, the grad=0.6177003648945734\n",
      "Current iteration=204, the loss=1.0313731174728717, the grad=0.6173627243921304\n",
      "Current iteration=205, the loss=1.031227768054477, the grad=0.6170254577700464\n",
      "Current iteration=206, the loss=1.0310824795549562, the grad=0.6166885646964237\n",
      "Current iteration=207, the loss=1.030937251956103, the grad=0.616352044838766\n",
      "Current iteration=208, the loss=1.030792085239635, the grad=0.6160158978639844\n",
      "Current iteration=209, the loss=1.030646979387195, the grad=0.6156801234384031\n",
      "Current iteration=210, the loss=1.0305019343803512, the grad=0.6153447212277643\n",
      "Current iteration=211, the loss=1.0303569502005983, the grad=0.6150096908972345\n",
      "Current iteration=212, the loss=1.0302120268293562, the grad=0.614675032111409\n",
      "Current iteration=213, the loss=1.0300671642479722, the grad=0.6143407445343181\n",
      "Current iteration=214, the loss=1.0299223624377192, the grad=0.6140068278294317\n",
      "Current iteration=215, the loss=1.0297776213797998, the grad=0.6136732816596662\n",
      "Current iteration=216, the loss=1.0296329410553415, the grad=0.6133401056873877\n",
      "Current iteration=217, the loss=1.029488321445402, the grad=0.6130072995744198\n",
      "Current iteration=218, the loss=1.0293437625309667, the grad=0.6126748629820467\n",
      "Current iteration=219, the loss=1.0291992642929488, the grad=0.6123427955710197\n",
      "Current iteration=220, the loss=1.0290548267121917, the grad=0.6120110970015626\n",
      "Current iteration=221, the loss=1.028910449769468, the grad=0.6116797669333767\n",
      "Current iteration=222, the loss=1.0287661334454805, the grad=0.6113488050256456\n",
      "Current iteration=223, the loss=1.0286218777208613, the grad=0.6110182109370415\n",
      "Current iteration=224, the loss=1.0284776825761734, the grad=0.6106879843257292\n",
      "Current iteration=225, the loss=1.0283335479919105, the grad=0.6103581248493725\n",
      "Current iteration=226, the loss=1.028189473948499, the grad=0.6100286321651386\n",
      "Current iteration=227, the loss=1.028045460426294, the grad=0.609699505929703\n",
      "Current iteration=228, the loss=1.0279015074055855, the grad=0.609370745799256\n",
      "Current iteration=229, the loss=1.0277576148665937, the grad=0.6090423514295065\n",
      "Current iteration=230, the loss=1.0276137827894725, the grad=0.6087143224756874\n",
      "Current iteration=231, the loss=1.0274700111543082, the grad=0.6083866585925614\n",
      "Current iteration=232, the loss=1.0273262999411212, the grad=0.6080593594344248\n",
      "Current iteration=233, the loss=1.0271826491298641, the grad=0.6077324246551139\n",
      "Current iteration=234, the loss=1.0270390587004248, the grad=0.6074058539080093\n",
      "Current iteration=235, the loss=1.026895528632625, the grad=0.6070796468460408\n",
      "Current iteration=236, the loss=1.0267520589062207, the grad=0.6067538031216926\n",
      "Current iteration=237, the loss=1.026608649500904, the grad=0.6064283223870088\n",
      "Current iteration=238, the loss=1.0264653003963007, the grad=0.6061032042935973\n",
      "Current iteration=239, the loss=1.0263220115719738, the grad=0.6057784484926357\n",
      "Current iteration=240, the loss=1.026178783007421, the grad=0.6054540546348756\n",
      "Current iteration=241, the loss=1.026035614682077, the grad=0.6051300223706475\n",
      "Current iteration=242, the loss=1.0258925065753135, the grad=0.6048063513498665\n",
      "Current iteration=243, the loss=1.025749458666438, the grad=0.6044830412220358\n",
      "Current iteration=244, the loss=1.025606470934696, the grad=0.604160091636253\n",
      "Current iteration=245, the loss=1.0254635433592707, the grad=0.6038375022412135\n",
      "Current iteration=246, the loss=1.0253206759192832, the grad=0.603515272685217\n",
      "Current iteration=247, the loss=1.0251778685937925, the grad=0.6031934026161698\n",
      "Current iteration=248, the loss=1.025035121361796, the grad=0.6028718916815929\n",
      "Current iteration=249, the loss=1.024892434202231, the grad=0.6025507395286231\n",
      "Current iteration=250, the loss=1.0247498070939722, the grad=0.602229945804021\n",
      "Current iteration=251, the loss=1.024607240015836, the grad=0.6019095101541733\n",
      "Current iteration=252, the loss=1.024464732946577, the grad=0.6015894322250982\n",
      "Current iteration=253, the loss=1.0243222858648904, the grad=0.6012697116624514\n",
      "Current iteration=254, the loss=1.024179898749412, the grad=0.6009503481115286\n",
      "Current iteration=255, the loss=1.024037571578718, the grad=0.6006313412172716\n",
      "Current iteration=256, the loss=1.0238953043313261, the grad=0.6003126906242725\n",
      "Current iteration=257, the loss=1.0237530969856954, the grad=0.5999943959767782\n",
      "Current iteration=258, the loss=1.0236109495202261, the grad=0.5996764569186941\n",
      "Current iteration=259, the loss=1.0234688619132608, the grad=0.5993588730935913\n",
      "Current iteration=260, the loss=1.0233268341430843, the grad=0.599041644144708\n",
      "Current iteration=261, the loss=1.023184866187924, the grad=0.5987247697149557\n",
      "Current iteration=262, the loss=1.02304295802595, the grad=0.5984082494469238\n",
      "Current iteration=263, the loss=1.0229011096352758, the grad=0.5980920829828833\n",
      "Current iteration=264, the loss=1.0227593209939583, the grad=0.5977762699647917\n",
      "Current iteration=265, the loss=1.022617592079998, the grad=0.5974608100342973\n",
      "Current iteration=266, the loss=1.0224759228713398, the grad=0.5971457028327436\n",
      "Current iteration=267, the loss=1.022334313345873, the grad=0.5968309480011739\n",
      "Current iteration=268, the loss=1.022192763481431, the grad=0.5965165451803356\n",
      "Current iteration=269, the loss=1.022051273255793, the grad=0.5962024940106843\n",
      "Current iteration=270, the loss=1.0219098426466826, the grad=0.5958887941323883\n",
      "Current iteration=271, the loss=1.0217684716317694, the grad=0.5955754451853326\n",
      "Current iteration=272, the loss=1.0216271601886684, the grad=0.5952624468091245\n",
      "Current iteration=273, the loss=1.0214859082949415, the grad=0.594949798643096\n",
      "Current iteration=274, the loss=1.0213447159280964, the grad=0.5946375003263096\n",
      "Current iteration=275, the loss=1.021203583065587, the grad=0.5943255514975615\n",
      "Current iteration=276, the loss=1.021062509684816, the grad=0.5940139517953862\n",
      "Current iteration=277, the loss=1.020921495763131, the grad=0.5937027008580608\n",
      "Current iteration=278, the loss=1.0207805412778284, the grad=0.5933917983236096\n",
      "Current iteration=279, the loss=1.0206396462061522, the grad=0.5930812438298069\n",
      "Current iteration=280, the loss=1.0204988105252948, the grad=0.5927710370141828\n",
      "Current iteration=281, the loss=1.0203580342123966, the grad=0.5924611775140262\n",
      "Current iteration=282, the loss=1.0202173172445466, the grad=0.592151664966389\n",
      "Current iteration=283, the loss=1.020076659598783, the grad=0.5918424990080914\n",
      "Current iteration=284, the loss=1.0199360612520927, the grad=0.5915336792757236\n",
      "Current iteration=285, the loss=1.0197955221814123, the grad=0.5912252054056528\n",
      "Current iteration=286, the loss=1.0196550423636286, the grad=0.5909170770340244\n",
      "Current iteration=287, the loss=1.019514621775578, the grad=0.5906092937967681\n",
      "Current iteration=288, the loss=1.0193742603940465, the grad=0.5903018553296004\n",
      "Current iteration=289, the loss=1.0192339581957721, the grad=0.5899947612680303\n",
      "Current iteration=290, the loss=1.0190937151574428, the grad=0.5896880112473613\n",
      "Current iteration=291, the loss=1.0189535312556972, the grad=0.5893816049026964\n",
      "Current iteration=292, the loss=1.0188134064671257, the grad=0.5890755418689425\n",
      "Current iteration=293, the loss=1.018673340768271, the grad=0.5887698217808129\n",
      "Current iteration=294, the loss=1.0185333341356266, the grad=0.5884644442728327\n",
      "Current iteration=295, the loss=1.0183933865456385, the grad=0.5881594089793413\n",
      "Current iteration=296, the loss=1.018253497974706, the grad=0.5878547155344971\n",
      "Current iteration=297, the loss=1.0181136683991792, the grad=0.5875503635722813\n",
      "Current iteration=298, the loss=1.0179738977953623, the grad=0.5872463527265009\n",
      "Current iteration=299, the loss=1.0178341861395128, the grad=0.586942682630794\n",
      "Current iteration=300, the loss=1.0176945334078416, the grad=0.5866393529186319\n",
      "Current iteration=301, the loss=1.0175549395765122, the grad=0.5863363632233236\n",
      "Current iteration=302, the loss=1.0174154046216437, the grad=0.5860337131780203\n",
      "Current iteration=303, the loss=1.0172759285193083, the grad=0.5857314024157176\n",
      "Current iteration=304, the loss=1.0171365112455326, the grad=0.5854294305692601\n",
      "Current iteration=305, the loss=1.016997152776299, the grad=0.5851277972713451\n",
      "Current iteration=306, the loss=1.0168578530875432, the grad=0.5848265021545257\n",
      "Current iteration=307, the loss=1.0167186121551572, the grad=0.5845255448512154\n",
      "Current iteration=308, the loss=1.0165794299549888, the grad=0.5842249249936909\n",
      "Current iteration=309, the loss=1.0164403064628404, the grad=0.5839246422140957\n",
      "Current iteration=310, the loss=1.0163012416544714, the grad=0.583624696144444\n",
      "Current iteration=311, the loss=1.0161622355055957, the grad=0.5833250864166246\n",
      "Current iteration=312, the loss=1.0160232879918865, the grad=0.5830258126624035\n",
      "Current iteration=313, the loss=1.0158843990889703, the grad=0.5827268745134283\n",
      "Current iteration=314, the loss=1.0157455687724335, the grad=0.5824282716012313\n",
      "Current iteration=315, the loss=1.0156067970178175, the grad=0.5821300035572333\n",
      "Current iteration=316, the loss=1.0154680838006227, the grad=0.5818320700127464\n",
      "Current iteration=317, the loss=1.0153294290963062, the grad=0.5815344705989784\n",
      "Current iteration=318, the loss=1.015190832880283, the grad=0.5812372049470355\n",
      "Current iteration=319, the loss=1.0150522951279266, the grad=0.5809402726879256\n",
      "Current iteration=320, the loss=1.0149138158145687, the grad=0.580643673452563\n",
      "Current iteration=321, the loss=1.0147753949155, the grad=0.5803474068717703\n",
      "Current iteration=322, the loss=1.014637032405969, the grad=0.5800514725762825\n",
      "Current iteration=323, the loss=1.0144987282611846, the grad=0.5797558701967501\n",
      "Current iteration=324, the loss=1.0143604824563142, the grad=0.5794605993637424\n",
      "Current iteration=325, the loss=1.014222294966484, the grad=0.5791656597077517\n",
      "Current iteration=326, the loss=1.014084165766782, the grad=0.5788710508591945\n",
      "Current iteration=327, the loss=1.0139460948322545, the grad=0.5785767724484177\n",
      "Current iteration=328, the loss=1.0138080821379087, the grad=0.5782828241056991\n",
      "Current iteration=329, the loss=1.0136701276587121, the grad=0.577989205461252\n",
      "Current iteration=330, the loss=1.0135322313695927, the grad=0.5776959161452294\n",
      "Current iteration=331, the loss=1.01339439324544, the grad=0.5774029557877245\n",
      "Current iteration=332, the loss=1.013256613261104, the grad=0.5771103240187767\n",
      "Current iteration=333, the loss=1.0131188913913964, the grad=0.5768180204683728\n",
      "Current iteration=334, the loss=1.0129812276110906, the grad=0.5765260447664515\n",
      "Current iteration=335, the loss=1.0128436218949215, the grad=0.5762343965429054\n",
      "Current iteration=336, the loss=1.0127060742175855, the grad=0.5759430754275853\n",
      "Current iteration=337, the loss=1.012568584553743, the grad=0.5756520810503022\n",
      "Current iteration=338, the loss=1.012431152878016, the grad=0.5753614130408313\n",
      "Current iteration=339, the loss=1.0122937791649878, the grad=0.5750710710289143\n",
      "Current iteration=340, the loss=1.012156463389207, the grad=0.574781054644263\n",
      "Current iteration=341, the loss=1.0120192055251833, the grad=0.5744913635165624\n",
      "Current iteration=342, the loss=1.0118820055473912, the grad=0.5742019972754733\n",
      "Current iteration=343, the loss=1.0117448634302681, the grad=0.573912955550635\n",
      "Current iteration=344, the loss=1.011607779148215, the grad=0.57362423797167\n",
      "Current iteration=345, the loss=1.0114707526755977, the grad=0.5733358441681844\n",
      "Current iteration=346, the loss=1.0113337839867453, the grad=0.5730477737697734\n",
      "Current iteration=347, the loss=1.0111968730559515, the grad=0.5727600264060225\n",
      "Current iteration=348, the loss=1.0110600198574757, the grad=0.5724726017065113\n",
      "Current iteration=349, the loss=1.0109232243655406, the grad=0.5721854993008153\n",
      "Current iteration=350, the loss=1.010786486554336, the grad=0.5718987188185107\n",
      "Current iteration=351, the loss=1.0106498063980138, the grad=0.5716122598891757\n",
      "Current iteration=352, the loss=1.0105131838706942, the grad=0.5713261221423935\n",
      "Current iteration=353, the loss=1.0103766189464622, the grad=0.5710403052077562\n",
      "Current iteration=354, the loss=1.010240111599369, the grad=0.5707548087148661\n",
      "Current iteration=355, the loss=1.010103661803431, the grad=0.5704696322933401\n",
      "Current iteration=356, the loss=1.009967269532632, the grad=0.5701847755728112\n",
      "Current iteration=357, the loss=1.0098309347609218, the grad=0.5699002381829321\n",
      "Current iteration=358, the loss=1.009694657462216, the grad=0.5696160197533776\n",
      "Current iteration=359, the loss=1.0095584376103997, the grad=0.5693321199138467\n",
      "Current iteration=360, the loss=1.0094222751793218, the grad=0.5690485382940671\n",
      "Current iteration=361, the loss=1.0092861701428015, the grad=0.5687652745237965\n",
      "Current iteration=362, the loss=1.0091501224746238, the grad=0.5684823282328252\n",
      "Current iteration=363, the loss=1.0090141321485417, the grad=0.5681996990509799\n",
      "Current iteration=364, the loss=1.008878199138277, the grad=0.5679173866081254\n",
      "Current iteration=365, the loss=1.008742323417519, the grad=0.5676353905341671\n",
      "Current iteration=366, the loss=1.0086065049599244, the grad=0.5673537104590549\n",
      "Current iteration=367, the loss=1.00847074373912, the grad=0.5670723460127846\n",
      "Current iteration=368, the loss=1.0083350397287008, the grad=0.5667912968254006\n",
      "Current iteration=369, the loss=1.0081993929022304, the grad=0.5665105625269995\n",
      "Current iteration=370, the loss=1.0080638032332416, the grad=0.5662301427477314\n",
      "Current iteration=371, the loss=1.0079282706952368, the grad=0.5659500371178035\n",
      "Current iteration=372, the loss=1.007792795261688, the grad=0.5656702452674813\n",
      "Current iteration=373, the loss=1.0076573769060362, the grad=0.5653907668270937\n",
      "Current iteration=374, the loss=1.007522015601693, the grad=0.5651116014270318\n",
      "Current iteration=375, the loss=1.0073867113220396, the grad=0.5648327486977543\n",
      "Current iteration=376, the loss=1.0072514640404282, the grad=0.56455420826979\n",
      "Current iteration=377, the loss=1.0071162737301806, the grad=0.564275979773738\n",
      "Current iteration=378, the loss=1.0069811403645899, the grad=0.5639980628402723\n",
      "Current iteration=379, the loss=1.0068460639169194, the grad=0.5637204571001427\n",
      "Current iteration=380, the loss=1.0067110443604037, the grad=0.5634431621841793\n",
      "Current iteration=381, the loss=1.0065760816682494, the grad=0.5631661777232919\n",
      "Current iteration=382, the loss=1.0064411758136336, the grad=0.5628895033484757\n",
      "Current iteration=383, the loss=1.0063063267697048, the grad=0.562613138690811\n",
      "Current iteration=384, the loss=1.0061715345095836, the grad=0.5623370833814667\n",
      "Current iteration=385, the loss=1.006036799006364, the grad=0.5620613370517034\n",
      "Current iteration=386, the loss=1.0059021202331089, the grad=0.5617858993328738\n",
      "Current iteration=387, the loss=1.0057674981628568, the grad=0.5615107698564269\n",
      "Current iteration=388, the loss=1.0056329327686166, the grad=0.5612359482539094\n",
      "Current iteration=389, the loss=1.005498424023372, the grad=0.560961434156968\n",
      "Current iteration=390, the loss=1.0053639719000762, the grad=0.5606872271973513\n",
      "Current iteration=391, the loss=1.005229576371659, the grad=0.5604133270069135\n",
      "Current iteration=392, the loss=1.0050952374110207, the grad=0.5601397332176153\n",
      "Current iteration=393, the loss=1.0049609549910379, the grad=0.5598664454615265\n",
      "Current iteration=394, the loss=1.0048267290845572, the grad=0.559593463370828\n",
      "Current iteration=395, the loss=1.0046925596644025, the grad=0.5593207865778153\n",
      "Current iteration=396, the loss=1.0045584467033688, the grad=0.5590484147148979\n",
      "Current iteration=397, the loss=1.0044243901742271, the grad=0.558776347414605\n",
      "Current iteration=398, the loss=1.0042903900497213, the grad=0.5585045843095848\n",
      "Current iteration=399, the loss=1.0041564463025716, the grad=0.5582331250326082\n",
      "shape of teOpti  (568238, 23)\n",
      "end of the logistic_regression with w= [ 0.04195632 -0.06259875 -0.0025392   0.03139999  0.0045528  -0.00460336\n",
      "  0.02398858 -0.03531562  0.04671585  0.04121729 -0.00021553 -0.0008228\n",
      " -0.00640812  0.00017936  0.00075927  0.00234622  0.00130382  0.02073689\n",
      "  0.02019364  0.02458354  0.02343298  0.02343278  0.02033609]  and loss= 1.0041564463025716\n"
     ]
    }
   ],
   "source": [
    "# test logistic regression\n",
    "\n",
    "txOpti = dataClean(tx_tr)\n",
    "print(\"shape of txOpti \", txOpti.shape)\n",
    "initial_w = np.zeros(txOpti.shape[1])\n",
    "# initial_w = w\n",
    "max_iters = 400\n",
    "gamma = 0.0005\n",
    "\n",
    "w, loss = logistic_regression(y_tr, txOpti, initial_w, max_iters, gamma)\n",
    "\n",
    "teOpti = dataClean(tx_te)\n",
    "print(\"shape of teOpti \", teOpti.shape)\n",
    "\n",
    "y_pred = predict(w, teOpti)\n",
    "y_pred[y_pred==0] = -1\n",
    "\n",
    "OUTPUT_PATH = 'sample-submission LR'\n",
    "create_csv_submission(ids_te, y_pred, OUTPUT_PATH)\n",
    "\n",
    "print(\"end of the logistic_regression with w=\",w,\" and loss=\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0b54129f-2e9e-489f-989e-cffe093f018d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[105.012 46.524 73.752 38.468 -999.000 -999.000 -999.000 2.492 12.316\n",
      " 120.665 1.280 -0.356 -999.000 31.804 -0.023 -0.033 40.516 -0.045 0.086\n",
      " 34.802 -0.024 179.739 1.000 38.960 -1.872 -2.093 -999.000 -999.000\n",
      " -999.000 40.513]\n",
      "[-49.023 49.240 81.182 57.896 -708.421 -601.237 -709.357 2.373 18.917\n",
      " 158.432 1.438 -0.128 -708.985 38.707 -0.011 -0.008 46.660 -0.020 0.044\n",
      " 41.717 -0.010 209.797 0.979 -348.330 -399.254 -399.260 -692.381 -709.122\n",
      " -709.119 73.065]\n",
      "(array([ 4,  5,  6, 12, 26, 27, 28]),)\n",
      "index : 4\n",
      "177457\n",
      "index : 5\n",
      "177457\n",
      "index : 6\n",
      "177457\n",
      "index : 12\n",
      "177457\n",
      "index : 26\n",
      "177457\n",
      "index : 27\n",
      "177457\n",
      "index : 28\n",
      "177457\n",
      "counts of each column  [177457, 177457, 177457, 177457, 177457, 177457, 177457]\n"
     ]
    }
   ],
   "source": [
    "#clean column , remove column that has only -999 elements. \n",
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.3f}\".format(x)})\n",
    "\n",
    "median = np.median(tx_tr, axis=0) \n",
    "mean = np.mean(tx_tr, axis=0) \n",
    "\n",
    "print(median)\n",
    "print(mean)\n",
    "\n",
    "indx = np.where(median == -999)\n",
    "print(indx)\n",
    "\n",
    "counts = []\n",
    "count = 0\n",
    "\n",
    "for i in indx[0]: \n",
    "    count = np.count_nonzero( tx_tr[:,i] == -999)\n",
    "    print(\"index :\", i)\n",
    "    print(count)\n",
    "    counts.append(count)\n",
    "\n",
    "print(\"counts of each column \", counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cd948ae1-5376-46f3-b929-6d5d233f0bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7,)\n",
      "(250000, 30)\n",
      "(250000, 23)\n"
     ]
    }
   ],
   "source": [
    "median = np.median(tx_tr, axis=0) \n",
    "indx = np.where(median == -999)\n",
    "tx1 = np.delete(tx_tr, indx[0], 1)\n",
    "print(indx[0].shape)\n",
    "print(tx_tr.shape)\n",
    "print(tx1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f062a94a-ff3d-4d4d-88b2-66b0ca8bf65c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of txOpti  (250000, 23)\n",
      "end of the logistic_regression with w= [ 4.50300842e-03 -6.62757642e-03 -2.65297946e-04  3.59712971e-03\n",
      "  2.58058701e-04 -3.09882051e-04  2.85152955e-03 -3.69117823e-03\n",
      "  5.10551184e-03  4.42795209e-03 -1.83273093e-05 -8.34744891e-05\n",
      " -6.11568648e-04  2.75644738e-05  7.80817587e-05  4.03966168e-04\n",
      "  1.40670181e-04  2.51657143e-03  2.47748581e-03  2.93239003e-03\n",
      "  2.79926559e-03  2.79924277e-03  2.49137767e-03]  and loss= 1.0562025575989353\n"
     ]
    }
   ],
   "source": [
    "# test regularized logistic regression\n",
    "txOpti = dataClean(tx_tr)\n",
    "print(\"shape of txOpti \", txOpti.shape)\n",
    "initial_w = np.zeros(txOpti.shape[1])\n",
    "# initial_w = w\n",
    "max_iters = 200\n",
    "gamma = 0.0001\n",
    "lambda_ = 0.0000001\n",
    "\n",
    "w, loss = reg_logistic_regression(y_tr, txOpti, lambda_, initial_w, max_iters, gamma)\n",
    "\n",
    "print(\"end of the logistic_regression with w=\",w,\" and loss=\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9b9982-bb31-408a-86a5-c11433e7df5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
