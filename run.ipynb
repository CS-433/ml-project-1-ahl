{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "882a9018-5319-4a82-87e7-2b0d420475bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from implementations import *\n",
    "from helpers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d56f0e3-9ee7-448f-a9dd-dd349209cc8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading the training dataset\n",
      "training dataset loaded\n"
     ]
    }
   ],
   "source": [
    "# load the training dataset\n",
    "print(\"loading the training dataset\")\n",
    "DATA_TRAIN_PATH = 'train.csv'\n",
    "y_tr, tx_tr, ids_tr = load_csv_data(DATA_TRAIN_PATH)\n",
    "print(\"training dataset loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af90503c-e3fa-4e1b-a222-63ef11b39a80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading the test dataset\n",
      "test dataset loaded\n"
     ]
    }
   ],
   "source": [
    "# load the test dataset\n",
    "print(\"loading the test dataset\")\n",
    "#DATA_TEST_PATH = 'test.csv'\n",
    "#y_te, tx_te, ids_te = load_csv_data(DATA_TEST_PATH)\n",
    "print(\"test dataset loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa8da103-df5b-4f1e-b595-9ae0cd91750e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shape of the training y is  (250000,)  and the training tx is  (250000, 30)\n"
     ]
    }
   ],
   "source": [
    "# printing the shapes of the arrays\n",
    "print(\"the shape of the training y is \",y_tr.shape,\" and the training tx is \", tx_tr.shape)\n",
    "#print(\"the shape of the test y is \",y_te.shape,\" and the test tx is \",tx_te.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6884129-6892-4ec8-bce6-fe61104b9ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing the mean square gd\n",
    "initial_w = np.zeros(tx_tr.shape[1])\n",
    "max_iters = 100\n",
    "gamma = 0.00001\n",
    "\n",
    "#w, loss = mean_squared_error_gd(y_tr, tx_tr, initial_w, max_iters, gamma)\n",
    "\n",
    "#print(\"end of the mean_squared_error_gd with w=\",w,\" and loss=\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3abc58ab-10d3-4a01-b62a-6e3445c65b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end of the least_squares with w= [ 8.03494312e-05 -7.20202273e-03 -6.05417273e-03 -5.47559065e-04\n",
      " -1.93874700e-02  4.73451621e-04 -2.60379054e-02  3.25106300e-01\n",
      " -3.80780282e-05 -2.72724919e+00 -2.21220140e-01  9.50794091e-02\n",
      "  6.40351613e-02  2.73550887e+00 -3.31801241e-04 -9.54325120e-04\n",
      "  2.74026561e+00 -5.34164891e-04  9.73498581e-04  3.69225052e-03\n",
      "  3.54487449e-04 -5.43344598e-04 -3.30448035e-01 -1.40800498e-03\n",
      "  8.31432888e-04  1.02117272e-03 -1.68047416e-03 -5.83664818e-03\n",
      " -1.11087997e-02  2.72770912e+00]  and loss= 0.3396868094770935\n"
     ]
    }
   ],
   "source": [
    "# test least square\n",
    "w, loss = least_squares(y_tr, tx_tr)\n",
    "print(\"end of the least_squares with w=\",w,\" and loss=\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f1f09a81-21c9-4587-ba44-9bb03ab90776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, the loss=74.91345659003325, the grad=106.97083333333332\n",
      "Current iteration=1, the loss=12392.556368254693, the grad=-455.55433176537247\n",
      "Current iteration=2, the loss=112760647.50036944, the grad=46482.65905417211\n",
      "Current iteration=3, the loss=1112261752357.543, the grad=-4682027.228883041\n",
      "Current iteration=4, the loss=2695596429638917.0, the grad=197618401.02166304\n",
      "Current iteration=5, the loss=83975891266.18178, the grad=-22250425.66709368\n",
      "Current iteration=6, the loss=1.5092183113583854e+19, the grad=-16979721853.698744\n",
      "Current iteration=7, the loss=1.095749602184886e+23, the grad=1480740913400.7751\n",
      "Current iteration=8, the loss=2.6036529915573767e+26, the grad=-61156746240325.234\n",
      "Current iteration=9, the loss=2.3135833684968652e+30, the grad=6934219007026078.0\n",
      "Current iteration=10, the loss=1.0206913741504133e+26, the grad=-536369215572939.06\n",
      "Current iteration=11, the loss=5.441937450238136e+33, the grad=-3.119288039736992e+17\n",
      "Current iteration=12, the loss=5.201319289418342e+37, the grad=3.2368714237208445e+19\n",
      "Current iteration=13, the loss=2.7162121703223053e+35, the grad=-1.90764868185899e+19\n",
      "Current iteration=14, the loss=7.856520936245099e+40, the grad=-1.160467259564222e+21\n",
      "Current iteration=15, the loss=8.74622618573292e+44, the grad=1.3264728713769365e+23\n",
      "Current iteration=16, the loss=6.912371851571816e+48, the grad=-1.1788028530346522e+25\n",
      "Current iteration=17, the loss=1.287685557010188e+49, the grad=1.0165029058310811e+25\n",
      "Current iteration=18, the loss=7.691038472925817e+49, the grad=3.7096603149883046e+25\n",
      "Current iteration=19, the loss=3.7834695157783087e+53, the grad=-2.620172502531918e+27\n",
      "Current iteration=20, the loss=3.710326074631507e+57, the grad=2.658067413233483e+29\n",
      "Current iteration=21, the loss=9.056889878047813e+60, the grad=-1.1613698018440078e+31\n",
      "Current iteration=22, the loss=8.364860418509966e+64, the grad=1.3020374462435897e+33\n",
      "Current iteration=23, the loss=2.481466653804312e+68, the grad=-6.904888574793855e+34\n",
      "Current iteration=24, the loss=1.771660351573267e+72, the grad=6.010136717733602e+36\n",
      "Current iteration=25, the loss=4.2451831150208505e+75, the grad=-2.6230354074984205e+38\n",
      "Current iteration=26, the loss=1.174970269528358e+73, the grad=8.765722068199908e+37\n",
      "Current iteration=27, the loss=1.973545357457751e+79, the grad=1.840498899243073e+40\n",
      "Current iteration=28, the loss=9.479627289976257e+82, the grad=-1.2990099649030799e+42\n",
      "Current iteration=29, the loss=1.4600123523939577e+81, the grad=6.30758863979661e+41\n",
      "Current iteration=30, the loss=5.190222018709956e+80, the grad=-7.195403880276065e+41\n",
      "Current iteration=31, the loss=5.416207151948342e+86, the grad=9.588697083097543e+43\n",
      "Current iteration=32, the loss=5.212446080663041e+90, the grad=-1.0202310334682098e+46\n",
      "Current iteration=33, the loss=5.1633775993769985e+94, the grad=1.0115426230970436e+48\n",
      "Current iteration=34, the loss=5.109348680253294e+98, the grad=-1.0001193159859773e+50\n",
      "Current iteration=35, the loss=5.006625653530369e+102, the grad=1.0036865477027412e+52\n",
      "Current iteration=36, the loss=4.929989605387821e+106, the grad=-1.0005215161144224e+54\n",
      "Current iteration=37, the loss=4.880794649342412e+110, the grad=9.710865797019169e+55\n",
      "Current iteration=38, the loss=4.815320562943485e+114, the grad=-9.771403044101231e+57\n",
      "Current iteration=39, the loss=4.757184506235738e+118, the grad=9.63910070752754e+59\n",
      "Current iteration=40, the loss=4.707642459095186e+122, the grad=-9.555464175243683e+61\n",
      "Current iteration=41, the loss=1.0966360307721514e+126, the grad=4.445174252192903e+63\n",
      "Current iteration=42, the loss=1.2525694669143153e+130, the grad=-5.051079626391674e+65\n",
      "Current iteration=43, the loss=1.4915061355477492e+134, the grad=5.531165888346498e+67\n",
      "Current iteration=44, the loss=2.8568887654318324e+137, the grad=-2.1486836498861984e+69\n",
      "Current iteration=45, the loss=2.6723468565471427e+141, the grad=2.29288813005944e+71\n",
      "Current iteration=46, the loss=2.7041229735693205e+145, the grad=-2.2060633807425094e+73\n",
      "Current iteration=47, the loss=1.832323156848217e+144, the grad=2.7697981080696526e+73\n",
      "Current iteration=48, the loss=1.1272221150363228e+149, the grad=1.4604424662373836e+75\n",
      "Current iteration=49, the loss=2.639550912612049e+152, the grad=-6.880857031677927e+76\n",
      "Current iteration=50, the loss=2.6174909504243147e+156, the grad=6.828272325298114e+78\n",
      "Current iteration=51, the loss=6.4072397348350446e+159, the grad=-2.7536969437955623e+80\n",
      "Current iteration=52, the loss=1.9991196265975615e+157, the grad=-1.6942626003553039e+80\n",
      "Current iteration=53, the loss=8.316101003471103e+157, the grad=4.0485846381677045e+80\n",
      "Current iteration=54, the loss=1.652064822173452e+163, the grad=1.7337767992365458e+82\n",
      "Current iteration=55, the loss=1.6053003418346501e+167, the grad=-1.7862057402329718e+84\n",
      "Current iteration=56, the loss=3.8331269659928496e+170, the grad=7.219299076990842e+85\n",
      "Current iteration=57, the loss=2.2880683097775074e+174, the grad=-6.4222407250105925e+87\n",
      "Current iteration=58, the loss=2.6657495902207475e+178, the grad=7.422769316372341e+89\n",
      "Current iteration=59, the loss=8.942990786420986e+181, the grad=-3.952833373972206e+91\n",
      "Current iteration=60, the loss=3.678653082053025e+178, the grad=1.1174014045108469e+91\n",
      "Current iteration=61, the loss=3.4088347861995693e+185, the grad=2.278578582759797e+93\n",
      "Current iteration=62, the loss=3.6574391494409894e+189, the grad=-2.744159368279652e+95\n",
      "Current iteration=63, the loss=2.9181873974352863e+193, the grad=2.334544737401541e+97\n",
      "Current iteration=64, the loss=6.885477420356625e+196, the grad=-1.091506089697425e+99\n",
      "Current iteration=65, the loss=3.7254523642468205e+200, the grad=6.6700503219474745e+100\n",
      "Current iteration=66, the loss=1.6574553429484912e+204, the grad=-5.629090561528918e+102\n",
      "Current iteration=67, the loss=8.127502419058055e+207, the grad=3.7422978553080745e+104\n",
      "Current iteration=68, the loss=7.847958632894691e+211, the grad=-3.914396788327281e+106\n",
      "Current iteration=69, the loss=9.175167045586439e+215, the grad=4.3082390949499406e+108\n",
      "Current iteration=70, the loss=3.0087224827388523e+214, the grad=-4.0771089649721175e+108\n",
      "Current iteration=71, the loss=5.517428101867969e+215, the grad=1.673920159822649e+109\n",
      "Current iteration=72, the loss=1.9432724058153455e+220, the grad=-6.398008609691891e+110\n",
      "Current iteration=73, the loss=3.681399039300785e+223, the grad=2.642498294018763e+112\n",
      "Current iteration=74, the loss=1.927635688037624e+227, the grad=-1.584993452158192e+114\n",
      "Current iteration=75, the loss=1.997796625108551e+231, the grad=2.0167086058394333e+116\n",
      "Current iteration=76, the loss=3.736923311653787e+234, the grad=-8.428031442573835e+117\n",
      "Current iteration=77, the loss=4.3073680203072625e+238, the grad=9.20199450869605e+119\n",
      "Current iteration=78, the loss=1.418599821170252e+242, the grad=-5.114355945239242e+121\n",
      "Current iteration=79, the loss=2.0285003945950706e+242, the grad=-2.5400316266849127e+121\n",
      "Current iteration=80, the loss=1.3025649409099425e+238, the grad=1.6744954649056089e+121\n",
      "Current iteration=81, the loss=2.0419971583950335e+246, the grad=6.468306400443539e+123\n",
      "Current iteration=82, the loss=2.0133410898486883e+250, the grad=-6.388272351258467e+125\n",
      "Current iteration=83, the loss=1.990741566029341e+254, the grad=6.287759013075272e+127\n",
      "Current iteration=84, the loss=1.95502440032891e+258, the grad=-6.247038956559202e+129\n",
      "Current iteration=85, the loss=4.691344562058565e+261, the grad=2.701135074801573e+131\n",
      "Current iteration=86, the loss=7.709597963054624e+254, the grad=1.2321966949134082e+130\n",
      "Current iteration=87, the loss=4.423869033370455e+265, the grad=-2.9602848691679165e+133\n",
      "Current iteration=88, the loss=1.0345950456201349e+269, the grad=1.3508412131363923e+135\n",
      "Current iteration=89, the loss=4.957647071239809e+272, the grad=-9.58214300140144e+136\n",
      "Current iteration=90, the loss=2.3764686169417996e+276, the grad=6.704055041591686e+138\n",
      "Current iteration=91, the loss=2.307893450910828e+280, the grad=-6.838595853890742e+140\n",
      "Current iteration=92, the loss=2.71630406811923e+284, the grad=7.397218084887971e+142\n",
      "Current iteration=93, the loss=2.1640689256307392e+288, the grad=-6.706016514830887e+144\n",
      "Current iteration=94, the loss=2.127319011700393e+292, the grad=6.607329285421071e+146\n",
      "Current iteration=95, the loss=3.122485434275423e+291, the grad=-1.066601140852702e+146\n",
      "Current iteration=96, the loss=1.6800636478882583e+296, the grad=-5.807554293147396e+148\n",
      "Current iteration=97, the loss=1.6549814348836154e+300, the grad=5.752857141091536e+150\n",
      "Current iteration=98, the loss=2.478170359722223e+299, the grad=-8.533169840278689e+150\n",
      "Current iteration=99, the loss=4.641698589063738e+303, the grad=-3.0963422095365077e+152\n",
      "end of the mean_squared_error_sgd with w= [ 8.95937309e+147 -4.81555740e+146 -7.51101605e+146 -7.19091279e+145\n",
      "  8.64621546e+147  9.33000644e+147  8.63681709e+147 -2.62295017e+145\n",
      " -1.57901319e+146 -1.52859792e+146 -1.12737388e+145  1.25929339e+145\n",
      "  8.64167360e+147 -1.46654835e+146  9.90842924e+142 -2.55778766e+145\n",
      " -2.47145772e+146  2.00928598e+145 -3.72727601e+144 -3.18133961e+146\n",
      "  1.39548134e+145 -4.17314192e+146  3.55035159e+144  8.79812318e+147\n",
      "  8.64287709e+147  8.64292549e+147  8.68699607e+147  8.63795315e+147\n",
      "  8.63882057e+147  2.40931984e+146]  and loss= 4.641698589063738e+303\n"
     ]
    }
   ],
   "source": [
    "# test mean squared error sgd\n",
    "\n",
    "initial_w = np.zeros(tx_tr.shape[1])\n",
    "max_iters = 100\n",
    "gamma = 0.00001\n",
    "\n",
    "w, loss = mean_squared_error_sgd(y_tr, tx_tr, initial_w, max_iters, gamma)\n",
    "\n",
    "print(\"end of the mean_squared_error_sgd with w=\",w,\" and loss=\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "34c8ae3d-d431-4939-8e10-d0915a0fc194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end of the ridge_regression with w= [ 8.21350624e-05 -7.21942061e-03 -6.00961895e-03 -5.73434628e-04\n",
      " -1.98051845e-02  4.74476262e-04 -2.60798499e-02  3.21559164e-01\n",
      " -3.86264527e-05  5.20366193e-03 -2.19459421e-01  9.50755083e-02\n",
      "  6.42870333e-02  3.06043952e-03 -3.33175236e-04 -9.60829247e-04\n",
      "  7.74415852e-03 -5.31341546e-04  9.69570234e-04  3.70847241e-03\n",
      "  3.55825331e-04 -5.46401063e-04 -3.26118784e-01 -1.36700534e-03\n",
      "  8.07614825e-04  9.99406315e-04 -1.65624282e-03 -5.76690313e-03\n",
      " -1.09991101e-02 -4.78210585e-03]  and loss= 0.3396893863480035\n"
     ]
    }
   ],
   "source": [
    "# test ridge regression \n",
    "\n",
    "initial_w = np.zeros(tx_tr.shape[1])\n",
    "max_iters = 100\n",
    "lambda_ = 0.0005\n",
    "\n",
    "w, loss = ridge_regression(y_tr, tx_tr, lambda_)\n",
    "\n",
    "print(\"end of the ridge_regression with w=\",w,\" and loss=\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f5c7de-0c97-4975-93cb-71f4916eb96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
