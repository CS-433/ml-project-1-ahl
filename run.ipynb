{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "882a9018-5319-4a82-87e7-2b0d420475bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from implementations import *\n",
    "from helpers import *\n",
    "from helper_functions import *\n",
    "from optimization import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6001fe01-6df9-475b-9bd1-0dc91e667ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d56f0e3-9ee7-448f-a9dd-dd349209cc8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading the training dataset\n",
      "training dataset loaded\n"
     ]
    }
   ],
   "source": [
    "# load the training dataset\n",
    "print(\"loading the training dataset\")\n",
    "DATA_TRAIN_PATH = 'train.csv'\n",
    "y_tr, tx_tr, ids_tr = load_csv_data(DATA_TRAIN_PATH)\n",
    "print(\"training dataset loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af90503c-e3fa-4e1b-a222-63ef11b39a80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading the test dataset\n",
      "test dataset loaded\n"
     ]
    }
   ],
   "source": [
    "# load the test dataset\n",
    "print(\"loading the test dataset\")\n",
    "DATA_TEST_PATH = 'test.csv'\n",
    "y_te, tx_te, ids_te = load_csv_data(DATA_TEST_PATH)\n",
    "print(\"test dataset loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa8da103-df5b-4f1e-b595-9ae0cd91750e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shape of the training y is (250000,) and the training tx is (250000, 30)\n",
      "the shape of the test y is (568238,) and the test tx is (568238, 30)\n"
     ]
    }
   ],
   "source": [
    "# printing the shapes of the arrays\n",
    "print(f'the shape of the training y is {y_tr.shape} and the training tx is {tx_tr.shape}')\n",
    "print(f'the shape of the test y is {y_te.shape} and the test tx is {tx_te.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5e092ef-cd07-432f-83fc-1472fc29ddb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(formatter={'float': lambda x: \"{0:10.3f}\".format(x)})\n",
    "\n",
    "def print_col_info(t):\n",
    "    mean_ = np.nanmean(t, axis=0)\n",
    "    max_ = np.nanmax(t, axis=0)\n",
    "    min_ = np.nanmin(t, axis=0)\n",
    "    median_ = np.nanmedian(t, axis=0)\n",
    "    std_ = np.nanstd(t, axis=0)\n",
    "    print(t.shape)\n",
    "\n",
    "    table = np.vstack([range(t.shape[1]), mean_, max_, min_, median_, std_])\n",
    "\n",
    "    for line in table.T:\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6884129-6892-4ec8-bce6-fe61104b9ff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test mean square GD\n",
      "Current iteration=0\t the loss=0.1176670\t the grad=0.5271830\n",
      "Current iteration=10\t the loss=0.0819012\t the grad=0.0148576\n",
      "Current iteration=20\t the loss=0.0817723\t the grad=0.0031852\n",
      "Current iteration=30\t the loss=0.0817611\t the grad=0.0012227\n",
      "Current iteration=40\t the loss=0.0817590\t the grad=0.0005862\n",
      "Current iteration=50\t the loss=0.0817585\t the grad=0.0003265\n",
      "Current iteration=60\t the loss=0.0817583\t the grad=0.0001973\n",
      "Current iteration=62\t the loss=0.0817583\t the grad=0.0001794\n",
      "end of the mean_squared_error_gd with w= [     0.343      0.090     -0.120      0.030     -0.026     -0.011\n",
      "     -0.004     -0.016     -0.015     -0.013     -0.014     -0.040\n",
      "      0.053      0.063      0.042     -0.001     -0.000     -0.005\n",
      "      0.002      0.002     -0.022     -0.000      0.005      0.026\n",
      "      0.020     -0.002     -0.001     -0.001     -0.000     -0.001\n",
      "      0.014]  and loss= 0.0817582975816111\n",
      "the accuracy of the method is acc= 0.761\n",
      "end of mean square GD\n"
     ]
    }
   ],
   "source": [
    "# testing the mean square gd\n",
    "print(\"test mean square GD\") \n",
    "#y_tr[np.where(y_tr == 0)] = -1\n",
    "y_tr[np.where(y_tr == -1)] = 0\n",
    "txOpti = dataClean_without_splitting(tx_tr)\n",
    "\n",
    "initial_w = np.zeros(txOpti.shape[1])\n",
    "max_iters = 100\n",
    "gamma = 0.3 #best gamma here\n",
    "w, loss = mean_squared_error_gd(y_tr, txOpti, initial_w, max_iters, gamma)\n",
    "print(\"end of the mean_squared_error_gd with w=\",w,\" and loss=\", loss)\n",
    "\n",
    "label = predict(w, txOpti)\n",
    "y_tr[np.where(y_tr == 0)] = -1\n",
    "acc = calculate_accuracy(y_tr, label)\n",
    "print(\"the accuracy of the method is acc=\",acc)\n",
    "print(\"end of mean square GD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "56180f30-5013-4162-b64b-e435e36f9806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test mean square GD with spliting\n",
      "shape of txOpti[i]  (99913, 19)\n",
      "Current iteration=0\t the loss=0.0882093\t the grad=0.4316712\n",
      "Current iteration=25\t the loss=0.0694140\t the grad=0.0023148\n",
      "Current iteration=50\t the loss=0.0694055\t the grad=0.0003233\n",
      "Current iteration=58\t the loss=0.0694053\t the grad=0.0001792\n",
      "end of the least_square with w= [     0.255      0.030     -0.062     -0.000     -0.005      0.032\n",
      "     -0.005      0.078     -0.076     -0.001      0.018     -0.001\n",
      "     -0.002     -0.037      0.008     -0.001     -0.087     -0.002\n",
      "      0.016]  and loss= 0.06940534792972657\n",
      "shape of txOpti[i]  (77544, 23)\n",
      "Current iteration=0\t the loss=0.1272059\t the grad=0.4789405\n",
      "Current iteration=25\t the loss=0.0908357\t the grad=0.0031104\n",
      "Current iteration=50\t the loss=0.0908197\t the grad=0.0004366\n",
      "Current iteration=62\t the loss=0.0908195\t the grad=0.0001722\n",
      "end of the least_square with w= [     0.357      0.119     -0.108      0.006     -0.004     -0.027\n",
      "      0.011     -0.013     -0.046      0.050      0.041     -0.001\n",
      "      0.000      0.011     -0.003      0.005     -0.012      0.002\n",
      "      0.002      0.019     -0.002      0.001      0.019]  and loss= 0.09081945696069274\n",
      "shape of txOpti[i]  (50379, 30)\n",
      "Current iteration=0\t the loss=0.1587760\t the grad=0.6632477\n",
      "Current iteration=25\t the loss=0.0872088\t the grad=0.0043406\n",
      "Current iteration=50\t the loss=0.0871751\t the grad=0.0007346\n",
      "Current iteration=71\t the loss=0.0871741\t the grad=0.0001848\n",
      "end of the least_square with w= [     0.511      0.155     -0.049      0.018      0.045     -0.002\n",
      "      0.014     -0.028     -0.030     -0.036     -0.036     -0.026\n",
      "      0.052      0.117      0.026     -0.001      0.001     -0.004\n",
      "     -0.004      0.000     -0.025      0.002     -0.025      0.023\n",
      "     -0.000     -0.001      0.026     -0.001     -0.003      0.021]  and loss= 0.08717406728303581\n",
      "shape of txOpti[i]  (22164, 30)\n",
      "Current iteration=0\t the loss=0.1174953\t the grad=0.3745001\n",
      "Current iteration=25\t the loss=0.0888071\t the grad=0.0034085\n",
      "Current iteration=50\t the loss=0.0887878\t the grad=0.0005575\n",
      "Current iteration=70\t the loss=0.0887871\t the grad=0.0001826\n",
      "end of the least_square with w= [     0.304      0.148     -0.051      0.003      0.056     -0.014\n",
      "     -0.009     -0.026     -0.038     -0.013     -0.032     -0.019\n",
      "      0.036      0.005      0.024     -0.007      0.001     -0.009\n",
      "      0.003      0.001     -0.012      0.007     -0.024      0.001\n",
      "     -0.006      0.001      0.012      0.006      0.002      0.006]  and loss= 0.08878713517956718\n",
      "the accuracy on the train set is  0.8131374295637205\n",
      "the accuracy on the train set is  0.7321649644073043\n",
      "the accuracy on the train set is  0.7477321899998015\n",
      "the accuracy on the train set is  0.7285688503880166\n",
      "the total accuracy on the train set is  0.7554008585897106\n"
     ]
    }
   ],
   "source": [
    "#testing mean square GD with spliting the data \n",
    "print(\"test mean square GD with spliting\") \n",
    "#y_tr[np.where(y_tr == 0)] = -1\n",
    "y_tr[np.where(y_tr == -1)] = 0\n",
    "\n",
    "txOpti, yOpti, idsOpti = dataClean(tx_tr, y_tr)\n",
    "ws = []\n",
    "losses = []\n",
    "max_iters = 200\n",
    "gamma = 0.3\n",
    "\n",
    "for i in range(4): \n",
    "    print(\"shape of txOpti[i] \", txOpti[i].shape)\n",
    "    initial_w = np.zeros(txOpti[i].shape[1])\n",
    "    # initial_w = ws\n",
    "    w, loss = mean_squared_error_gd(yOpti[i], txOpti[i], initial_w, max_iters, gamma)\n",
    "    losses.append(loss) \n",
    "    ws.append(w)\n",
    "    print(\"end of the least_square with w=\",w,\" and loss=\", loss)\n",
    "    \n",
    "accs = []\n",
    "labels = []\n",
    "for i in range(4): \n",
    "    label = predict(ws[i], txOpti[i])\n",
    "    yOpti[i][np.where(yOpti[i] == 0)] = -1\n",
    "    acc = calculate_accuracy(yOpti[i], label)\n",
    "    print(\"the accuracy on the train set is \", acc)\n",
    "    accs.append(acc)\n",
    "    labels.append(label)\n",
    "    \n",
    "accTot = (accs[0] + accs[1] + accs[2] + accs[3])/4\n",
    "print(\"the total accuracy on the train set is \", accTot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b0c1077a-ee73-4b34-b546-b7f1910ff3fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test mean square SGD\n",
      "Current iteration=0\t the loss=0.0848699\t the grad=4.4272075\n",
      "Current iteration=25\t the loss=0.0113748\t the grad=1.62929178\n",
      "Current iteration=50\t the loss=0.0022188\t the grad=1.5002008\n",
      "Current iteration=75\t the loss=0.0637552\t the grad=8.31897734\n",
      "Current iteration=100\t the loss=0.0557767\t the grad=4.2659434\n",
      "Current iteration=125\t the loss=0.0631818\t the grad=6.8008771\n",
      "Current iteration=150\t the loss=0.1368293\t the grad=6.86031757\n",
      "Current iteration=175\t the loss=0.0101451\t the grad=1.78398114\n",
      "end of the mean_squared_error_sgd with w= [    -0.145      0.137     -0.256      0.020     -0.048     -0.137\n",
      "      0.324     -0.614      0.063     -0.062     -0.074      0.493\n",
      "      0.147      0.262     -0.049      0.064     -0.064      0.033\n",
      "     -0.211      0.159     -0.086      0.182      0.041      0.129\n",
      "      0.119     -0.009     -0.210      0.031      0.364      0.114\n",
      "      0.056]  and loss= 0.02758508732088929\n",
      "the accuracy of the method is acc= 0.649984\n",
      "end of mean square SGD\n"
     ]
    }
   ],
   "source": [
    "#test mean square SGD\n",
    "print(\"test mean square SGD\")\n",
    "y_tr[np.where(y_tr == 0)] = -1\n",
    "#y_tr[np.where(y_tr == -1)] = 0\n",
    "txOpti = dataClean_without_splitting(tx_tr)\n",
    "\n",
    "initial_w = np.zeros(txOpti.shape[1])\n",
    "max_iters = 200\n",
    "gamma = 0.03\n",
    "w, loss = mean_squared_error_sgd(y_tr, txOpti, initial_w, max_iters, gamma)\n",
    "print(\"end of the mean_squared_error_sgd with w=\",w,\" and loss=\", loss)\n",
    "\n",
    "label = predict(w, txOpti)\n",
    "#y_tr[np.where(y_tr == 0)] = -1\n",
    "acc = calculate_accuracy(y_tr, label)\n",
    "print(\"the accuracy of the method is acc=\",acc)\n",
    "print(\"end of mean square SGD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ad472f25-7c3d-4e36-8b40-b5e0289764a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test mean square SGD with spliting\n",
      "shape of txOpti[i]  (99913, 19)\n",
      "Current iteration=0\t the loss=0.0412636\t the grad=4.8741653\n",
      "Current iteration=25\t the loss=0.1833951\t the grad=5.90437427\n",
      "Current iteration=50\t the loss=0.1698078\t the grad=4.3910432\n",
      "Current iteration=75\t the loss=0.0244233\t the grad=0.6722776\n",
      "end of the least_square with w= [    -0.461     -0.001     -0.121     -0.007     -0.089     -0.150\n",
      "     -0.089      0.123     -0.012     -0.066     -0.021     -0.125\n",
      "     -0.117     -0.099     -0.014      0.135     -0.152     -0.043\n",
      "      0.034]  and loss= 0.044599909673553054\n",
      "shape of txOpti[i]  (77544, 23)\n",
      "Current iteration=0\t the loss=0.0715228\t the grad=4.5526050\n",
      "Current iteration=25\t the loss=0.0686201\t the grad=2.1498725\n",
      "Current iteration=50\t the loss=0.0909811\t the grad=5.3024104\n",
      "Current iteration=75\t the loss=0.1468967\t the grad=6.92107945\n",
      "end of the least_square with w= [    -0.280      0.099     -0.378      0.060     -0.078     -0.109\n",
      "     -0.011     -0.233     -0.089      0.129      0.036      0.287\n",
      "     -0.140     -0.067     -0.332      0.183      0.121     -0.029\n",
      "     -0.097      0.102      0.017      0.239      0.102]  and loss= 0.0026375021337914457\n",
      "shape of txOpti[i]  (50379, 30)\n",
      "Current iteration=0\t the loss=0.0106304\t the grad=5.3360069\n",
      "Current iteration=25\t the loss=0.0207442\t the grad=8.0637280\n",
      "Current iteration=50\t the loss=0.0224751\t the grad=8.07706871\n",
      "Current iteration=75\t the loss=0.0926495\t the grad=8.96159003\n",
      "end of the least_square with w= [    -0.082      0.253     -0.053      0.122      0.218      0.277\n",
      "      0.032     -0.251      0.045     -0.188     -0.251     -0.117\n",
      "     -0.004      0.309      0.042     -0.019      0.214     -0.105\n",
      "     -0.136      0.092      0.113     -0.031     -0.271      0.009\n",
      "     -0.104     -0.312     -0.048      0.047      0.032     -0.116]  and loss= 0.06724354939332124\n",
      "shape of txOpti[i]  (22164, 30)\n",
      "Current iteration=0\t the loss=0.0454314\t the grad=4.8255070\n",
      "Current iteration=25\t the loss=0.0801947\t the grad=6.2158335\n",
      "Current iteration=50\t the loss=0.1296501\t the grad=5.83049916\n",
      "Current iteration=75\t the loss=0.0010805\t the grad=4.41144744\n",
      "end of the least_square with w= [    -0.332      0.506      0.317      0.010      0.240     -0.007\n",
      "      0.074     -0.044     -0.116     -0.184      0.106      0.051\n",
      "      0.192      0.247     -0.077      0.100      0.045     -0.116\n",
      "     -0.025     -0.039      0.021      0.214     -0.116     -0.182\n",
      "     -0.181      0.189      0.021     -0.129     -0.155      0.079]  and loss= 0.17238481446046433\n",
      "the accuracy on the train set is  0.7457087666269655\n",
      "the accuracy on the train set is  0.6489090064995358\n",
      "the accuracy on the train set is  0.6306794497707378\n",
      "the accuracy on the train set is  0.6695993502977802\n",
      "the total accuracy on the train set is  0.6737241432987549\n"
     ]
    }
   ],
   "source": [
    "#testing mean square SGD with spliting the data \n",
    "print(\"test mean square SGD with spliting\") \n",
    "y_tr[np.where(y_tr == 0)] = -1\n",
    "#y_tr[np.where(y_tr == -1)] = 0\n",
    "\n",
    "txOpti, yOpti, idsOpti = dataClean(tx_tr, y_tr)\n",
    "ws = []\n",
    "losses = []\n",
    "max_iters = 100\n",
    "gamma = 0.03\n",
    "\n",
    "for i in range(4): \n",
    "    print(\"shape of txOpti[i] \", txOpti[i].shape)\n",
    "    initial_w = np.zeros(txOpti[i].shape[1])\n",
    "    w, loss = mean_squared_error_sgd(yOpti[i], txOpti[i], initial_w, max_iters, gamma)\n",
    "    losses.append(loss) \n",
    "    ws.append(w)\n",
    "    print(\"end of the least_square with w=\",w,\" and loss=\", loss)\n",
    "    \n",
    "accs = []\n",
    "labels = []\n",
    "for i in range(4): \n",
    "    label = predict(ws[i], txOpti[i])\n",
    "    #yOpti[i][np.where(yOpti[i] == 0)] = -1\n",
    "    acc = calculate_accuracy(yOpti[i], label)\n",
    "    print(\"the accuracy on the train set is \", acc)\n",
    "    accs.append(acc)\n",
    "    labels.append(label)\n",
    "    \n",
    "accTot = (accs[0] + accs[1] + accs[2] + accs[3])/4\n",
    "print(\"the total accuracy on the train set is \", accTot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3abc58ab-10d3-4a01-b62a-6e3445c65b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test least square\n",
      "end of the least_squares with w= [     0.343      0.091     -0.120      0.030     -0.026     -0.011\n",
      "     -0.004     -0.016     -0.015     -0.013     -0.015     -0.040\n",
      "      0.053      0.063      0.042     -0.001     -0.000     -0.005\n",
      "      0.002      0.002     -0.022     -0.000      0.005      0.026\n",
      "      0.020     -0.002     -0.001     -0.001     -0.000     -0.001\n",
      "      0.014]  and loss= 0.08175819768998804\n",
      "the accuracy of the method is acc= 0.760976\n"
     ]
    }
   ],
   "source": [
    "# test least square\n",
    "print(\"test least square\")\n",
    "#y_tr[np.where(y_tr == 0)] = -1\n",
    "y_tr[np.where(y_tr == -1)] = 0\n",
    "txOpti = dataClean_without_splitting(tx_tr)\n",
    "w, loss = least_squares(y_tr, txOpti)\n",
    "\n",
    "label = predict(w, txOpti)\n",
    "y_tr[np.where(y_tr == 0)] = -1\n",
    "acc = calculate_accuracy(y_tr, label)\n",
    "print(\"end of the least_squares with w=\",w,\" and loss=\", loss)\n",
    "print(\"the accuracy of the method is acc=\",acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d29301f8-cf86-4615-8965-ccd4864ecef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test the least square with spliting the dataset\n",
      "shape of txOpti[i]  (99913, 19)\n",
      "end of the least_square with w= [     0.255      0.030     -0.062     -0.000    313.853      0.032\n",
      "   -313.864      0.078     -0.076     -0.001      0.018     -0.001\n",
      "     -0.002     -0.037      0.008     -0.001     -0.087     -0.002\n",
      "      0.016]  and loss= 0.06940501382214669\n",
      "shape of txOpti[i]  (77544, 23)\n",
      "end of the least_square with w= [     0.357      0.120     -0.108      0.006     -0.004     -0.027\n",
      "      0.011     -0.014     -0.046      0.050      0.041     -0.001\n",
      "      0.000      0.011     -0.003      0.005     -0.012      0.002\n",
      "      0.002   -201.364     -0.002      0.001    201.402]  and loss= 0.09081924257946217\n",
      "shape of txOpti[i]  (50379, 30)\n",
      "end of the least_square with w= [     0.511      0.155     -0.048      0.018      0.045     -0.002\n",
      "      0.014     -0.028     -0.030     -0.036     -0.037     -0.026\n",
      "      0.052      0.117      0.026     -0.001      0.001     -0.004\n",
      "     -0.004      0.000     -0.025      0.002     -0.025      0.023\n",
      "     -0.000     -0.001      0.026     -0.001     -0.003      0.021]  and loss= 0.08717399061994073\n",
      "shape of txOpti[i]  (22164, 30)\n",
      "end of the least_square with w= [     0.304      0.148     -0.051      0.003      0.056     -0.014\n",
      "     -0.009     -0.026     -0.037     -0.013     -0.033     -0.019\n",
      "      0.036      0.005      0.024     -0.007      0.001     -0.009\n",
      "      0.003      0.001     -0.012      0.007     -0.024      0.001\n",
      "     -0.006      0.001      0.012      0.006      0.002      0.006]  and loss= 0.08878703991889393\n",
      "the accuracy on the train set is  0.813147438271296\n",
      "the accuracy on the train set is  0.7321262766945218\n",
      "the accuracy on the train set is  0.747513845054487\n",
      "the accuracy on the train set is  0.7285237321783071\n",
      "the total accuracy on the train set is  0.755327823049653\n"
     ]
    }
   ],
   "source": [
    "# test least square with spliting data\n",
    "print(\"test the least square with spliting the dataset\")\n",
    "y_tr[np.where(y_tr == -1)] = 0\n",
    "#y_tr[np.where(y_tr == 0)] = -1\n",
    "txOpti, yOpti, idsOpti = dataClean(tx_tr, y_tr)\n",
    "ws = []\n",
    "losses = []\n",
    "\n",
    "for i in range(4): \n",
    "    print(\"shape of txOpti[i] \", txOpti[i].shape)\n",
    "    w, loss = least_squares(yOpti[i], txOpti[i])\n",
    "    losses.append(loss) \n",
    "    ws.append(w)\n",
    "    print(\"end of the least_square with w=\",w,\" and loss=\", loss)\n",
    "    \n",
    "accs = []\n",
    "labels = []\n",
    "for i in range(4): \n",
    "    label = predict(ws[i], txOpti[i])\n",
    "    yOpti[i][np.where(yOpti[i] == 0)] = -1\n",
    "    acc = calculate_accuracy(yOpti[i], label)\n",
    "    print(\"the accuracy on the train set is \", acc)\n",
    "    accs.append(acc)\n",
    "    labels.append(label)\n",
    "    \n",
    "accTot = (accs[0] + accs[1] + accs[2] + accs[3])/4\n",
    "print(\"the total accuracy on the train set is \", accTot)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 53,
=======
   "execution_count": 21,
>>>>>>> 4407e07f603a468effa38a3c578d17288c7b3307
   "id": "34c8ae3d-d431-4939-8e10-d0915a0fc194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test ridge regression\n",
      "shape of txOpti  (250000, 31)\n",
      "best lambda is  1e-06  and the best degree is  2\n",
<<<<<<< HEAD
      "end of the logistic_regression with w= [ 1.10121349e-01  1.10121349e-01  1.03045174e-01 -1.09452934e-01\n",
      "  1.99710929e-02 -2.58064237e-02 -8.97498703e-03 -3.19842556e-02\n",
      " -1.15345582e-02 -1.55579720e-02 -1.21345422e-02 -5.36353338e-02\n",
      " -5.42066674e-02  3.96769832e-02  6.25077605e-02  4.81043711e-02\n",
      " -1.04799312e-03 -1.85716601e-04  1.51471914e-02  1.85097125e-03\n",
      "  1.66630072e-03 -3.14574658e-02 -2.80450698e-05  1.98993976e-02\n",
      " -7.22557584e-03  2.06583885e-02 -2.16777752e-03 -7.86384212e-04\n",
      "  1.96325476e-02 -2.60206063e-06 -3.34045728e-04  8.05581956e-02\n",
      "  1.10121349e-01 -2.44031412e-02 -3.57296170e-04 -2.13485874e-02\n",
      "  7.54893548e-03  4.10113982e-03  7.45115734e-03 -3.62159988e-04\n",
      " -4.52591872e-03  2.87455680e-03  1.84287112e-02  2.01846333e-02\n",
      "  4.61614442e-02 -3.74265457e-03 -9.79169660e-03 -4.25224880e-03\n",
      " -1.08193259e-03 -1.25981468e-02 -8.41859423e-03 -1.56783863e-03\n",
      "  8.16568487e-03  7.48296837e-04 -1.33380785e-02  3.45492092e-02\n",
      " -4.33753817e-03  8.81577135e-03  4.49051155e-04 -4.65434657e-03\n",
      "  1.67468833e-03 -4.07848123e-03 -2.99908858e-02]  and loss= 0.07827237263520095\n",
=======
      "end of the logistic_regression with w= [     0.110      0.110      0.103     -0.109      0.020     -0.026\n",
      "     -0.009     -0.032     -0.012     -0.016     -0.012     -0.054\n",
      "     -0.054      0.040      0.063      0.048     -0.001     -0.000\n",
      "      0.015      0.002      0.002     -0.031     -0.000      0.020\n",
      "     -0.007      0.021     -0.002     -0.001      0.020     -0.000\n",
      "     -0.000      0.081      0.110     -0.024     -0.000     -0.021\n",
      "      0.008      0.004      0.007     -0.000     -0.005      0.003\n",
      "      0.018      0.020      0.046     -0.004     -0.010     -0.004\n",
      "     -0.001     -0.013     -0.008     -0.002      0.008      0.001\n",
      "     -0.013      0.035     -0.004      0.009      0.000     -0.005\n",
      "      0.002     -0.004     -0.030]  and loss= 0.07827237263520095\n",
>>>>>>> 4407e07f603a468effa38a3c578d17288c7b3307
      "the accuracy on the train set is  0.78142\n"
     ]
    }
   ],
   "source": [
    "# test ridge regression \n",
    "print(\"test ridge regression\")\n",
    "y_tr[np.where(y_tr == -1)] = 0\n",
    "txOpti = dataClean_without_splitting(tx_tr)\n",
    "print(\"shape of txOpti \", txOpti.shape)\n",
    "\n",
    "lambda_, degree = best_lambda_degree(y_tr, txOpti, 4, np.logspace(-6, 0, 30), np.arange(2,5), 1)\n",
    "print(\"best lambda is \",lambda_,\" and the best degree is \", degree)\n",
    "\n",
    "txOpti_poly = build_poly(txOpti, degree)\n",
    "\n",
    "w, loss = ridge_regression(y_tr, txOpti_poly, lambda_)\n",
    "\n",
    "label = predict(w, txOpti_poly)\n",
    "y_tr[np.where(y_tr == 0)] = -1\n",
    "acc = calculate_accuracy(y_tr, label)\n",
    "\n",
    "print(\"end of the logistic_regression with w=\",w,\" and loss=\", loss)\n",
    "print(\"the accuracy on the train set is \", acc)\n",
    "\n",
    "#y_pred = predict(w, tx_te)\n",
    "#y_pred[y_pred==0] = -1\n",
    "\n",
    "#OUTPUT_PATH = 'sample-submission'\n",
    "#create_csv_submission(ids_te, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 52,
=======
   "execution_count": 25,
>>>>>>> 4407e07f603a468effa38a3c578d17288c7b3307
   "id": "57b136b3-1f85-412e-a3f6-6133c06a594d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test ridge regression by spliting data\n",
<<<<<<< HEAD
      "(99913, 19)\n",
      "(99913,)\n",
      "(77544, 23)\n",
      "(77544,)\n",
      "(50379, 30)\n",
      "(50379,)\n",
      "(22164, 30)\n",
      "(22164,)\n",
      "shape of txOpti[i]  (99913, 19)\n",
      "best lambda is  1e-06  and best degree is  2\n",
      "end of the ridge_regression with w= [ 0.08676151  0.08676151  0.04347355 -0.05263314  0.01097542  0.0002635\n",
      "  0.0255538  -0.00025692  0.03962095 -0.09632831  0.00902205  0.00683254\n",
      " -0.00065502 -0.00218045 -0.00991471  0.00687986 -0.00101939 -0.09678281\n",
      " -0.00250305  0.01531424  0.08676151 -0.01844627 -0.00597179 -0.02157731\n",
      " -0.0083841  -0.0113921  -0.00273198  0.00946822  0.04224987 -0.00207319\n",
      "  0.0059638  -0.00108927  0.0002007  -0.01246601 -0.01028292 -0.00132573\n",
      "  0.03251301  0.0025017  -0.00229936]  and loss= 0.06631046682865395\n",
      "shape of txOpti[i]  (77544, 23)\n",
      "best lambda is  1e-06  and best degree is  2\n",
      "end of the ridge_regression with w= [ 1.35037388e-01  1.35037388e-01  1.24236943e-01 -1.03580774e-01\n",
      "  1.63671293e-03 -3.98972139e-03 -2.63559799e-02  1.26617589e-02\n",
      " -3.28786474e-02 -6.50449384e-02  8.79492803e-02  4.98012486e-02\n",
      " -6.17406571e-04  4.21103292e-04  3.81409309e-02 -2.07624716e-03\n",
      "  4.17076427e-03 -6.24160421e-03  1.71356476e-03  3.86988993e-03\n",
      "  3.24246866e-02 -2.49390291e-03  1.00331905e-03  3.27033478e-02\n",
      "  1.35037388e-01 -2.24415389e-02  5.02279457e-03 -1.88377070e-02\n",
      "  3.30368259e-03 -1.58055582e-02 -2.80990839e-03  7.98356561e-03\n",
      "  2.67989450e-02  3.65523448e-02 -1.37269992e-02 -4.83462436e-03\n",
      " -2.56719012e-03 -1.67772567e-02 -1.01928968e-02 -7.27877144e-04\n",
      " -1.43208583e-02 -4.67205734e-05 -5.74198635e-03 -7.50245526e-03\n",
      "  1.69306694e-02  9.41719632e-04 -8.96707165e-03]  and loss= 0.08737292658592541\n",
      "shape of txOpti[i]  (50379, 30)\n",
      "best lambda is  1e-06  and best degree is  2\n",
      "end of the ridge_regression with w= [ 0.16259582  0.16259582  0.17210133 -0.04642161  0.00817311  0.04594544\n",
      "  0.01616241 -0.03430428 -0.01390512 -0.03282589 -0.0407353  -0.08056023\n",
      " -0.03338766  0.06116386  0.11654672  0.04075065 -0.00070814  0.00164794\n",
      "  0.00974392 -0.00293899  0.00032649 -0.0379739   0.00198196 -0.02098346\n",
      "  0.06745721 -0.00049372  0.00063491  0.04270467 -0.00093522 -0.00136226\n",
      "  0.0194383   0.16259582 -0.04492747  0.00822219 -0.02030738 -0.01381614\n",
      "  0.02098724  0.02432284 -0.00504018 -0.00029346  0.01060564  0.02703976\n",
      "  0.01504566  0.0102829   0.02624881 -0.01418222 -0.00416698 -0.00077693\n",
      " -0.00516831 -0.00523267 -0.00344724  0.00373816  0.0010146   0.00243213\n",
      " -0.02404363  0.01180059 -0.00062775 -0.01013374  0.01594845 -0.00228975\n",
      " -0.00021485]  and loss= 0.08144864801811093\n",
      "shape of txOpti[i]  (22164, 30)\n",
      "best lambda is  1e-06  and best degree is  2\n",
      "end of the ridge_regression with w= [ 0.09882097  0.09882097  0.2061859  -0.03989592 -0.00416482  0.05025446\n",
      " -0.00648909 -0.03713226 -0.02713786 -0.0619505  -0.0112586  -0.07800838\n",
      " -0.02230019  0.03617733 -0.03255911  0.02104974 -0.0054659   0.00106962\n",
      " -0.00354478  0.00116061  0.00028114 -0.0249965   0.00731767 -0.02146581\n",
      "  0.0144433  -0.00521054 -0.0003121   0.01850726  0.00601761  0.00305558\n",
      "  0.03208381  0.09882097 -0.05334915  0.00201854 -0.01961483 -0.01083318\n",
      "  0.00634324  0.01791133  0.00611491  0.00687143 -0.00111936  0.02685142\n",
      "  0.01165521  0.00852639  0.02804713 -0.0037756  -0.0123819   0.00091806\n",
      " -0.00334252 -0.01119784  0.00041122  0.00310524 -0.00200218  0.00494643\n",
      " -0.00598026  0.00892561  0.00156033 -0.00192581  0.00937335  0.00230107\n",
      " -0.01313074]  and loss= 0.08200918247690629\n",
      "the accuracy on the train set is  0.8202035771120875\n",
      "the accuracy on the train set is  0.7517280511709481\n",
      "the accuracy on the train set is  0.7763353778360031\n",
      "the accuracy on the train set is  0.787222523010287\n",
      "the total accuracy on the train set is  0.7838723822823314\n"
=======
      "shape of txOpti[0] (99913, 19)\n",
      "best lambda is  1e-06  and best degree is  1\n",
      "end of the ridge_regression with loss=0.06940528855719576\n",
      "shape of txOpti[1] (77544, 23)\n",
      "best lambda is  1e-06  and best degree is  1\n",
      "end of the ridge_regression with loss=0.0908194056759205\n",
      "shape of txOpti[2] (50379, 30)\n",
      "best lambda is  1e-06  and best degree is  1\n",
      "end of the ridge_regression with loss=0.08717399062025641\n",
      "shape of txOpti[3] (22164, 30)\n",
      "best lambda is  1e-06  and best degree is  1\n",
      "end of the ridge_regression with loss=0.08878703991906979\n",
      "the accuracy on the train set is  0.8131574469788716\n",
      "the accuracy on the train set is  0.7320875889817394\n",
      "the accuracy on the train set is  0.747513845054487\n",
      "the accuracy on the train set is  0.7285237321783071\n",
      "the total accuracy on the train set is  0.7553206532983512\n"
>>>>>>> 4407e07f603a468effa38a3c578d17288c7b3307
     ]
    }
   ],
   "source": [
    "# test ridge regression with spliting the data\n",
    "print(\"test ridge regression by spliting data\")\n",
    "\n",
    "y_tr[np.where(y_tr == -1)] = 0\n",
    "txOpti, yOpti, idsOpti = dataClean(tx_tr, y_tr)\n",
    "\n",
    "losses = []\n",
    "ws = []\n",
    "txPoly = [] \n",
    "for i in range(4): \n",
<<<<<<< HEAD
    "    print(\"shape of txOpti[i] \", txOpti[i].shape)\n",
    "    lambda_, degree = best_lambda_degree(yOpti[i], txOpti[i], 4, np.logspace(-6, 0, 30), np.arange(2, 5), 1)\n",
=======
    "    print(f'shape of txOpti[{i}] {txOpti[i].shape}')\n",
    "    lambda_, degree = best_lambda_degree(yOpti[i], txOpti[i], 4, np.logspace(-6, 0, 30), np.arange(1, 5), 1)\n",
>>>>>>> 4407e07f603a468effa38a3c578d17288c7b3307
    "    print(\"best lambda is \",lambda_,\" and best degree is \", degree)\n",
    "    t = build_poly(txOpti[i], degree)\n",
    "    w, loss = ridge_regression(yOpti[i], t, lambda_)\n",
    "    losses.append(loss) \n",
    "    ws.append(w) \n",
    "    txPoly.append(t)\n",
    "    print(f'end of the ridge_regression with loss={loss}')\n",
    "\n",
    "\n",
    "accs = []\n",
    "labels = []\n",
    "for i in range(4): \n",
    "    label = predict(ws[i], txPoly[i])\n",
    "    yOpti[i][np.where(yOpti[i] == 0)] = -1\n",
    "    acc = calculate_accuracy(yOpti[i], label)\n",
    "    print(\"the accuracy on the train set is \", acc)\n",
    "    accs.append(acc)\n",
    "    labels.append(label)\n",
    "    \n",
    "accTot = (accs[0] + accs[1] + accs[2] + accs[3])/4\n",
    "print(\"the total accuracy on the train set is \", accTot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d5f5c7de-0c97-4975-93cb-71f4916eb96e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of txOpti[i]  (99913, 19)\n",
      "Preiteration, the loss=0.9023999996647266, the grad=0.42567363610337283\n",
      "Current iteration=0\t the loss=0.8669964\t the grad=0.4256736\n",
      "Current iteration=25\t the loss=0.8223110\t the grad=0.0370658\n",
      "Current iteration=50\t the loss=0.8286025\t the grad=0.0130025\n",
      "Current iteration=75\t the loss=0.8307620\t the grad=0.0060880\n",
      "end of the logistic_regression with w= [    -1.523      0.096     -0.366      0.101     -0.025      0.292\n",
      "     -0.025      0.444     -0.530      0.001      0.115     -0.008\n",
      "     -0.014     -0.204      0.056     -0.002     -0.614     -0.019\n",
      "      0.129]  and loss= 0.8315736795015117\n",
      "shape of txOpti[i]  (77544, 23)\n",
      "Preiteration, the loss=0.8736879810552399, the grad=0.3493400704524976\n",
      "Current iteration=0\t the loss=0.8487077\t the grad=0.3493401\n",
      "Current iteration=25\t the loss=0.8288509\t the grad=0.0252360\n",
      "Current iteration=50\t the loss=0.8324185\t the grad=0.0087206\n",
      "Current iteration=75\t the loss=0.8328594\t the grad=0.0042534\n",
      "end of the logistic_regression with w= [    -0.792      0.646     -0.618      0.101     -0.014     -0.136\n",
      "      0.068     -0.058     -0.241      0.361      0.193     -0.004\n",
      "      0.003      0.037     -0.016      0.024     -0.058      0.008\n",
      "      0.009      0.096     -0.013      0.006      0.096]  and loss= 0.8328343600982739\n",
      "shape of txOpti[i]  (50379, 30)\n",
      "Preiteration, the loss=0.830575772040897, the grad=0.42318955307514045\n",
      "Current iteration=0\t the loss=0.7802044\t the grad=0.4231896\n",
      "Current iteration=25\t the loss=0.6956681\t the grad=0.0244353\n",
      "Current iteration=50\t the loss=0.6915462\t the grad=0.0106369\n",
      "Current iteration=75\t the loss=0.6902069\t the grad=0.0058434\n",
      "end of the logistic_regression with w= [     0.040      0.843     -0.298      0.144      0.248      0.013\n",
      "      0.071     -0.144     -0.144     -0.194     -0.160     -0.148\n",
      "      0.319      0.601      0.141     -0.005      0.006     -0.037\n",
      "     -0.019      0.000     -0.117      0.015     -0.146      0.116\n",
      "      0.000     -0.004      0.143     -0.006     -0.014      0.105]  and loss= 0.6895906281549239\n",
      "shape of txOpti[i]  (22164, 30)\n",
      "Preiteration, the loss=0.8887612240209609, the grad=0.2942102007130968\n",
      "Current iteration=0\t the loss=0.8780598\t the grad=0.2942102\n",
      "Current iteration=25\t the loss=0.8720084\t the grad=0.0281010\n",
      "Current iteration=50\t the loss=0.8749455\t the grad=0.0094240\n",
      "Current iteration=75\t the loss=0.8750555\t the grad=0.0043801\n",
      "end of the logistic_regression with w= [    -1.011      0.762     -0.339      0.070      0.326     -0.073\n",
      "     -0.040     -0.142     -0.176     -0.070     -0.158     -0.126\n",
      "      0.259      0.025      0.127     -0.039      0.002     -0.040\n",
      "      0.019     -0.002     -0.037      0.042     -0.141      0.009\n",
      "     -0.031     -0.000      0.063      0.031      0.009      0.015]  and loss= 0.8748975615846665\n",
      "the accuracy on the train set is  0.8145086225015764\n",
      "the accuracy on the train set is  0.7325260497266068\n",
      "the accuracy on the train set is  0.7490621092121718\n",
      "the accuracy on the train set is  0.7361487096192023\n",
      "the total accuracy on the train set is  0.7580613727648893\n"
     ]
    }
   ],
   "source": [
    "# test logistic regression\n",
    "\n",
    "y_tr[np.where(y_tr == -1)] = 0\n",
    "txOpti, yOpti, idsOpti = dataClean(tx_tr, y_tr)\n",
    "\n",
    "max_iters = 100\n",
    "gamma = 0.5\n",
    "ws = []\n",
    "losses = []\n",
    "\n",
    "for i in range(4): \n",
    "    print(\"shape of txOpti[i] \", txOpti[i].shape)\n",
    "    initial_w = np.zeros(txOpti[i].shape[1])\n",
    "    w, loss = logistic_regression(yOpti[i], txOpti[i], initial_w, max_iters, gamma)\n",
    "    ws.append(w)\n",
    "    losses.append(loss)\n",
    "    print(\"end of the logistic_regression with w=\",w,\" and loss=\", loss)\n",
    "\n",
    "    \n",
    "labels = []\n",
    "accs = []\n",
    "for i in range(4): \n",
    "    label = predict_logistic(ws[i], txOpti[i])\n",
    "    yOpti[i][np.where(yOpti[i] == 0)] = -1\n",
    "    acc = calculate_accuracy(yOpti[i], label)\n",
    "    print(\"the accuracy on the train set is \", acc)\n",
    "    accs.append(acc)\n",
    "    labels.append(label)\n",
    "\n",
    "accTot = (accs[0] + accs[1] + accs[2] + accs[3])/4\n",
    "print(\"the total accuracy on the train set is \", accTot)\n",
    "\n",
    "#y_pred = predict(w, teOpti)\n",
    "\n",
    "#OUTPUT_PATH = 'sample-submission LR 2'\n",
    "#create_csv_submission(ids_te, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "273f5528-0c1a-40cc-b818-c2c1cb6dfe49",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = 'logreg_nosplit_072'\n",
    "create_csv_submission(ids_te, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 7,
=======
   "execution_count": 34,
>>>>>>> 4407e07f603a468effa38a3c578d17288c7b3307
   "id": "f062a94a-ff3d-4d4d-88b2-66b0ca8bf65c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test regularized logistic regression\n",
      "shape of txOpti  (250000, 31)\n",
      "shape of y  (250000,)\n",
<<<<<<< HEAD
      "Current iteration=0, the loss=0.678099652939391, the grad=1.1426633287405872\n",
      "Current iteration=1, the loss=0.3817599972570856, the grad=0.8887657442574256\n",
      "Current iteration=2, the loss=0.12463479073305783, the grad=0.7894436249321913\n",
      "Current iteration=3, the loss=-0.1123290789738448, the grad=0.735467426225005\n",
      "Current iteration=4, the loss=-0.33739730949194185, the grad=0.7018267664206048\n",
      "Current iteration=5, the loss=-0.5548228919018777, the grad=0.6793812012128749\n",
      "Current iteration=6, the loss=-0.7670320696961654, the grad=0.663706007204545\n",
      "Current iteration=7, the loss=-0.9755093980670165, the grad=0.6523605872616359\n",
      "Current iteration=8, the loss=-1.1812132543728349, the grad=0.6438991133192041\n",
      "Current iteration=9, the loss=-1.3847903098592105, the grad=0.6374238226281024\n",
      "Current iteration=10, the loss=-1.5866937229831815, the grad=0.632356776704125\n",
      "Current iteration=11, the loss=-1.7872515950695556, the grad=0.6283143599971668\n",
      "Current iteration=12, the loss=-1.9867082834514866, the grad=0.6250348847951795\n",
      "Current iteration=13, the loss=-2.1852502509103915, the grad=0.6223353342213667\n",
      "Current iteration=14, the loss=-2.383022778109442, the grad=0.620084758843043\n",
      "Current iteration=15, the loss=-2.5801410986317186, the grad=0.6181874919712598\n",
      "Current iteration=16, the loss=-2.7766980258240026, the grad=0.6165723069899913\n",
      "Current iteration=17, the loss=-2.9727693103131916, the grad=0.6151852560744742\n",
      "Current iteration=18, the loss=-3.168417490603665, the grad=0.6139848402867479\n",
      "Current iteration=19, the loss=-3.363694718288187, the grad=0.6129386871698966\n",
      "Current iteration=20, the loss=-3.5586448695809025, the grad=0.6120212226593329\n",
      "Current iteration=21, the loss=-3.753305149697525, the grad=0.6112120113390769\n",
      "Current iteration=22, the loss=-3.9477073299428938, the grad=0.6104945540547122\n",
      "Current iteration=23, the loss=-4.141878714174508, the grad=0.6098554038322137\n",
      "Current iteration=24, the loss=-4.335842902735541, the grad=0.609283506867928\n",
      "Current iteration=25, the loss=-4.529620402668312, the grad=0.6087697050413371\n",
      "Current iteration=26, the loss=-4.723229119762896, the grad=0.6083063559584339\n",
      "Current iteration=27, the loss=-4.916684758722593, the grad=0.6078870396223272\n",
      "Current iteration=28, the loss=-5.110001151135501, the grad=0.6075063297213249\n",
      "Current iteration=29, the loss=-5.303190526184273, the grad=0.6071596136552295\n",
      "Current iteration=30, the loss=-5.4962637355453055, the grad=0.6068429497041927\n",
      "Current iteration=31, the loss=-5.689230441349261, the grad=0.6065529527763082\n",
      "Current iteration=32, the loss=-5.8820992741408595, the grad=0.6062867023420232\n",
      "Current iteration=33, the loss=-6.074877966310177, the grad=0.6060416677371704\n",
      "Current iteration=34, the loss=-6.267573465345764, the grad=0.6058156471689274\n",
      "Current iteration=35, the loss=-6.460192030393228, the grad=0.6056067176116205\n",
      "Current iteration=36, the loss=-6.6527393149276985, the grad=0.6054131934159926\n",
      "Current iteration=37, the loss=-6.845220437818581, the grad=0.6052335919352674\n",
      "Current iteration=38, the loss=-7.037640044645657, the grad=0.6050666048358323\n",
      "Current iteration=39, the loss=-7.230002360791837, the grad=0.6049110740394815\n",
      "Current iteration=40, the loss=-7.422311237570398, the grad=0.6047659714594731\n",
      "Current iteration=41, the loss=-7.6145701924288876, the grad=0.6046303818599793\n",
      "Current iteration=42, the loss=-7.806782444097309, the grad=0.6045034882993049\n",
      "Current iteration=43, the loss=-7.998950943405709, the grad=0.6043845597202512\n",
      "Current iteration=44, the loss=-8.191078400379906, the grad=0.6042729403324925\n",
      "Current iteration=45, the loss=-8.383167308128188, the grad=0.6041680404967418\n",
      "Current iteration=46, the loss=-8.575219963952692, the grad=0.6040693288724174\n",
      "Current iteration=47, the loss=-8.767238488053533, the grad=0.6039763256323176\n",
      "Current iteration=48, the loss=-8.959224840138964, the grad=0.6038885965815936\n",
      "Current iteration=49, the loss=-9.151180834209388, the grad=0.6038057480457301\n",
      "Current iteration=50, the loss=-9.34310815174436, the grad=0.6037274224146453\n",
      "Current iteration=51, the loss=-9.53500835348979, the grad=0.603653294248337\n",
      "Current iteration=52, the loss=-9.726882890015093, the grad=0.6035830668645894\n",
      "Current iteration=53, the loss=-9.918733111186905, the grad=0.6035164693416977\n",
      "Current iteration=54, the loss=-10.110560274686666, the grad=0.603453253879492\n",
      "Current iteration=55, the loss=-10.302365553682224, the grad=0.6033931934705155\n",
      "Current iteration=56, the loss=-10.49415004374964, the grad=0.6033360798403841\n",
      "Current iteration=57, the loss=-10.685914769129079, the grad=0.6032817216223275\n",
      "Current iteration=58, the loss=-10.877660688387982, the grad=0.603229942735979\n",
      "Current iteration=59, the loss=-11.069388699555923, the grad=0.6031805809446948\n",
      "Current iteration=60, the loss=-11.261099644787427, the grad=0.6031334865692854\n",
      "Current iteration=61, the loss=-11.452794314602563, the grad=0.6030885213390694\n",
      "Current iteration=62, the loss=-11.644473451749048, the grad=0.6030455573637539\n",
      "Current iteration=63, the loss=-11.836137754724533, the grad=0.6030044762118275\n",
      "Current iteration=64, the loss=-12.027787880993376, the grad=0.6029651680830405\n",
      "Current iteration=65, the loss=-12.21942444992822, the grad=0.6029275310641533\n",
      "Current iteration=66, the loss=-12.411048045503428, the grad=0.6028914704585144\n",
      "Current iteration=67, the loss=-12.602659218764298, the grad=0.6028568981812112\n",
      "Current iteration=68, the loss=-12.79425849009348, the grad=0.6028237322125669\n",
      "Current iteration=69, the loss=-12.985846351293757, the grad=0.6027918961036363\n",
      "Current iteration=70, the loss=-13.177423267504164, the grad=0.6027613185281262\n",
      "Current iteration=71, the loss=-13.368989678964786, the grad=0.6027319328758224\n",
      "Current iteration=72, the loss=-13.56054600264397, the grad=0.6027036768831786\n",
      "Current iteration=73, the loss=-13.752092633740247, the grad=0.6026764922972515\n",
      "Current iteration=74, the loss=-13.94362994706995, the grad=0.6026503245695696\n",
      "Current iteration=75, the loss=-14.135158298350692, the grad=0.6026251225769346\n",
      "Current iteration=76, the loss=-14.326678025389523, the grad=0.6026008383664772\n",
      "Current iteration=77, the loss=-14.518189449183915, the grad=0.6025774269225922\n",
      "Current iteration=78, the loss=-14.709692874943027, the grad=0.6025548459536271\n",
      "Current iteration=79, the loss=-14.901188593035721, the grad=0.6025330556964377\n",
      "Current iteration=80, the loss=-15.092676879871535, the grad=0.6025120187371168\n",
      "Current iteration=81, the loss=-15.284157998719907, the grad=0.6024916998463882\n",
      "Current iteration=82, the loss=-15.475632200472694, the grad=0.6024720658283036\n",
      "Current iteration=83, the loss=-15.667099724354546, the grad=0.6024530853810344\n",
      "Current iteration=84, the loss=-15.85856079858512, the grad=0.6024347289686606\n",
      "Current iteration=85, the loss=-16.050015640996996, the grad=0.6024169687029777\n",
      "Current iteration=86, the loss=-16.241464459612594, the grad=0.6023997782344329\n",
      "Current iteration=87, the loss=-16.432907453183354, the grad=0.6023831326513974\n",
      "Current iteration=88, the loss=-16.624344811693877, the grad=0.6023670083870521\n",
      "Current iteration=89, the loss=-16.81577671683378, the grad=0.602351383133232\n",
      "Current iteration=90, the loss=-17.00720334243951, the grad=0.6023362357606509\n",
      "Current iteration=91, the loss=-17.198624854908417, the grad=0.6023215462449583\n",
      "Current iteration=92, the loss=-17.39004141358708, the grad=0.6023072955981614\n",
      "Current iteration=93, the loss=-17.581453171135642, the grad=0.602293465804954\n",
      "Current iteration=94, the loss=-17.77286027386999, the grad=0.6022800397635727\n",
      "Current iteration=95, the loss=-17.96426286208324, the grad=0.6022670012308013\n",
      "Current iteration=96, the loss=-18.155661070347982, the grad=0.6022543347708063\n",
      "Current iteration=97, the loss=-18.347055027800614, the grad=0.6022420257074882\n",
      "Current iteration=98, the loss=-18.538444858409065, the grad=0.6022300600800877\n",
      "Current iteration=99, the loss=-18.729830681224797, the grad=0.6022184246017882\n",
      "Current iteration=100, the loss=-18.921212610620547, the grad=0.6022071066210841\n",
      "Current iteration=101, the loss=-19.112590756514237, the grad=0.602196094085712\n",
      "Current iteration=102, the loss=-19.30396522458041, the grad=0.6021853755089448\n",
      "Current iteration=103, the loss=-19.495336116449835, the grad=0.6021749399380809\n",
      "Current iteration=104, the loss=-19.686703529897823, the grad=0.6021647769249597\n",
      "Current iteration=105, the loss=-19.87806755902251, the grad=0.602154876498358\n",
      "Current iteration=106, the loss=-20.069428294413136, the grad=0.6021452291381362\n",
      "Current iteration=107, the loss=-20.26078582330937, the grad=0.6021358257510008\n",
      "Current iteration=108, the loss=-20.452140229752104, the grad=0.60212665764777\n",
      "Current iteration=109, the loss=-20.643491594726186, the grad=0.60211771652205\n",
      "Current iteration=110, the loss=-20.83483999629567, the grad=0.6021089944301952\n",
      "Current iteration=111, the loss=-21.02618550973203, the grad=0.6021004837724977\n",
      "Current iteration=112, the loss=-21.217528207635677, the grad=0.602092177275493\n",
      "Current iteration=113, the loss=-21.4088681600513, the grad=0.6020840679753233\n",
      "Current iteration=114, the loss=-21.600205434577287, the grad=0.6020761492020809\n",
      "Current iteration=115, the loss=-21.79154009646969, the grad=0.6020684145650664\n",
      "Current iteration=116, the loss=-21.98287220874088, the grad=0.6020608579389018\n",
      "Current iteration=117, the loss=-22.174201832253416, the grad=0.6020534734504408\n",
      "Current iteration=118, the loss=-22.3655290258092, the grad=0.6020462554664294\n",
      "Current iteration=119, the loss=-22.556853846234375, the grad=0.6020391985818611\n",
      "Current iteration=120, the loss=-22.74817634845993, the grad=0.6020322976089854\n",
      "Current iteration=121, the loss=-22.93949658559864, the grad=0.6020255475669326\n",
      "Current iteration=122, the loss=-23.130814609018216, the grad=0.6020189436719058\n",
      "Current iteration=123, the loss=-23.322130468410958, the grad=0.6020124813279144\n",
      "Current iteration=124, the loss=-23.51344421186021, the grad=0.6020061561180096\n",
      "Current iteration=125, the loss=-23.704755885903705, the grad=0.6019999637959951\n",
      "Current iteration=126, the loss=-23.896065535593923, the grad=0.6019939002785778\n",
      "Current iteration=127, the loss=-24.087373204555732, the grad=0.6019879616379412\n",
      "Current iteration=128, the loss=-24.278678935041345, the grad=0.6019821440947031\n",
      "Current iteration=129, the loss=-24.46998276798288, the grad=0.6019764440112484\n",
      "Current iteration=130, the loss=-24.661284743042415, the grad=0.6019708578854066\n",
      "Current iteration=131, the loss=-24.852584898659966, the grad=0.6019653823444532\n",
      "Current iteration=132, the loss=-25.043883272099187, the grad=0.6019600141394207\n",
      "Current iteration=133, the loss=-25.235179899491172, the grad=0.6019547501396986\n",
      "Current iteration=134, the loss=-25.42647481587634, the grad=0.6019495873279029\n",
      "Current iteration=135, the loss=-25.617768055244373, the grad=0.6019445227950081\n",
      "Current iteration=136, the loss=-25.809059650572625, the grad=0.6019395537357162\n",
      "Current iteration=137, the loss=-26.000349633862694, the grad=0.6019346774440585\n",
      "Current iteration=138, the loss=-26.191638036175622, the grad=0.6019298913092099\n",
      "Current iteration=139, the loss=-26.382924887665443, the grad=0.6019251928115092\n",
      "Current iteration=140, the loss=-26.574210217611462, the grad=0.6019205795186714\n",
      "Current iteration=141, the loss=-26.765494054449054, the grad=0.6019160490821833\n",
      "Current iteration=142, the loss=-26.956776425799372, the grad=0.601911599233867\n",
      "Current iteration=143, the loss=-27.14805735849761, the grad=0.6019072277826125\n",
      "Current iteration=144, the loss=-27.33933687862025, the grad=0.6019029326112587\n",
      "Current iteration=145, the loss=-27.530615011511262, the grad=0.6018987116736246\n",
      "Current iteration=146, the loss=-27.72189178180704, the grad=0.6018945629916753\n",
      "Current iteration=147, the loss=-27.913167213460554, the grad=0.6018904846528195\n",
      "Current iteration=148, the loss=-28.10444132976438, the grad=0.6018864748073323\n",
      "Current iteration=149, the loss=-28.29571415337296, the grad=0.6018825316658958\n",
      "Current iteration=150, the loss=-28.486985706323814, the grad=0.6018786534972478\n",
      "Current iteration=151, the loss=-28.678256010058124, the grad=0.6018748386259394\n",
      "Current iteration=152, the loss=-28.869525085440312, the grad=0.6018710854301943\n",
      "Current iteration=153, the loss=-29.06079295277707, the grad=0.6018673923398574\n",
      "Current iteration=154, the loss=-29.25205963183546, the grad=0.6018637578344385\n",
      "Current iteration=155, the loss=-29.443325141860488, the grad=0.6018601804412423\n",
      "Current iteration=156, the loss=-29.634589501591897, the grad=0.6018566587335781\n",
      "Current iteration=157, the loss=-29.825852729280353, the grad=0.6018531913290451\n",
      "Current iteration=158, the loss=-30.017114842703126, the grad=0.601849776887899\n",
      "Current iteration=159, the loss=-30.20837585917899, the grad=0.6018464141114777\n",
      "Current iteration=160, the loss=-30.399635795582785, the grad=0.6018431017407049\n",
      "Current iteration=161, the loss=-30.590894668359255, the grad=0.601839838554649\n",
      "Current iteration=162, the loss=-30.782152493536508, the grad=0.6018366233691459\n",
      "Current iteration=163, the loss=-30.97340928673892, the grad=0.6018334550354817\n",
      "Current iteration=164, the loss=-31.16466506319963, the grad=0.6018303324391224\n",
      "Current iteration=165, the loss=-31.355919837772507, the grad=0.6018272544985087\n",
      "Current iteration=166, the loss=-31.547173624943703, the grad=0.6018242201638859\n",
      "Current iteration=167, the loss=-31.738426438842893, the grad=0.6018212284161925\n",
      "Current iteration=168, the loss=-31.929678293253982, the grad=0.6018182782659874\n",
      "Current iteration=169, the loss=-32.120929201625536, the grad=0.6018153687524277\n",
      "Current iteration=170, the loss=-32.31217917708076, the grad=0.6018124989422766\n",
      "Current iteration=171, the loss=-32.50342823242714, the grad=0.6018096679289631\n",
      "Current iteration=172, the loss=-32.69467638016596, the grad=0.6018068748316697\n",
      "Current iteration=173, the loss=-32.885923632501076, the grad=0.6018041187944595\n",
      "Current iteration=174, the loss=-33.07717000134778, the grad=0.6018013989854412\n",
      "Current iteration=175, the loss=-33.2684154983412, the grad=0.6017987145959579\n",
      "Current iteration=176, the loss=-33.459660134844306, the grad=0.6017960648398183\n",
      "Current iteration=177, the loss=-33.650903921955894, the grad=0.6017934489525468\n",
      "Current iteration=178, the loss=-33.84214687051809, the grad=0.6017908661906718\n",
      "Current iteration=179, the loss=-34.033388991123715, the grad=0.601788315831034\n",
      "Current iteration=180, the loss=-34.224630294123344, the grad=0.6017857971701265\n",
      "Current iteration=181, the loss=-34.41587078963216, the grad=0.6017833095234547\n",
      "Current iteration=182, the loss=-34.607110487536595, the grad=0.6017808522249263\n",
      "Current iteration=183, the loss=-34.798349397500694, the grad=0.6017784246262566\n",
      "Current iteration=184, the loss=-34.989587528972386, the grad=0.6017760260964039\n",
      "Current iteration=185, the loss=-35.18082489118938, the grad=0.6017736560210192\n",
      "Current iteration=186, the loss=-35.37206149318507, the grad=0.601771313801921\n",
      "Current iteration=187, the loss=-35.56329734379404, the grad=0.6017689988565849\n",
      "Current iteration=188, the loss=-35.75453245165761, the grad=0.6017667106176573\n",
      "Current iteration=189, the loss=-35.94576682522897, the grad=0.6017644485324808\n",
      "Current iteration=190, the loss=-36.13700047277843, the grad=0.6017622120626391\n",
      "Current iteration=191, the loss=-36.328233402398176, the grad=0.6017600006835243\n",
      "Current iteration=192, the loss=-36.51946562200717, the grad=0.6017578138839055\n",
      "Current iteration=193, the loss=-36.71069713935572, the grad=0.6017556511655276\n",
      "Current iteration=194, the loss=-36.90192796203004, the grad=0.6017535120427155\n",
      "Current iteration=195, the loss=-37.09315809745643, the grad=0.6017513960419955\n",
      "Current iteration=196, the loss=-37.28438755290568, the grad=0.6017493027017272\n",
      "Current iteration=197, the loss=-37.47561633549701, the grad=0.6017472315717528\n",
      "Current iteration=198, the loss=-37.666844452202064, the grad=0.6017451822130541\n",
      "Current iteration=199, the loss=-37.85807190984877, the grad=0.6017431541974233\n",
      "end of the logistic_regression with loss= -37.85807190984877\n",
      "the accuracy on the train set is  0.741044\n"
=======
      "Current iteration=0\t the loss=0.8374429\t the grad=0.4304113\n",
      "Current iteration=25\t the loss=0.8338336\t the grad=0.0000000\n",
      "Current iteration=50\t the loss=0.8338336\t the grad=0.0000000\n",
      "Current iteration=75\t the loss=0.8338336\t the grad=0.0000000\n",
      "end of the logistic_regression with loss= 0.8338335535064907\n",
      "the accuracy on the train set is  0.22644\n",
      "shape of teOpti  (568238, 31)\n"
>>>>>>> 4407e07f603a468effa38a3c578d17288c7b3307
     ]
    }
   ],
   "source": [
    "# test regularized logistic regression\n",
<<<<<<< HEAD
    "print(\"test regularized logistic regression\")\n",
    "y_tr[np.where(y_tr == -1)] = 0\n",
    "#y_tr[np.where(y_tr == 0)] = -1\n",
=======
>>>>>>> 4407e07f603a468effa38a3c578d17288c7b3307
    "txOpti = dataClean_without_splitting(tx_tr)\n",
    "print(\"shape of txOpti \", txOpti.shape)\n",
    "print(\"shape of y \", y_tr.shape)\n",
    "initial_w = np.zeros(txOpti.shape[1])\n",
    "max_iters = 200\n",
    "gamma = 0.5\n",
    "lambda_ = 1e-06\n",
    "\n",
    "w, loss = reg_logistic_regression(y_tr, txOpti, lambda_, initial_w, max_iters, gamma)\n",
    "\n",
    "label = predict_logistic(w, txOpti)\n",
    "y_tr[np.where(y_tr == 0)] = -1\n",
    "acc = calculate_accuracy(y_tr, label)\n",
    "\n",
    "# print(\"end of the logistic_regression with w=\",w,\" and loss=\", loss)\n",
    "print(\"end of the logistic_regression with loss=\", loss)\n",
    "print(\"the accuracy on the train set is \", acc)\n",
    "\n",
    "#teOpti = dataClean_without_splitting(tx_te)\n",
    "#print(\"shape of teOpti \", teOpti.shape)\n",
    "\n",
    "#y_pred = predict_logistic(w, teOpti)\n",
    "\n",
    "#OUTPUT_PATH = 'sample-submission LR regression r=1.5, gamma,lambda=0.5'\n",
    "#create_csv_submission(ids_te, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 8,
=======
   "execution_count": 31,
>>>>>>> 4407e07f603a468effa38a3c578d17288c7b3307
   "id": "2d44796c-228e-46b0-8a37-699a3f746b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of txOpti[i]  (99913, 19)\n",
<<<<<<< HEAD
      "Current iteration=0, the loss=0.8669964109343472, the grad=0.42567363610337283\n",
      "Current iteration=1, the loss=0.8465805026679354, the grad=0.32346490021795526\n",
      "Current iteration=2, the loss=0.8343725664874708, the grad=0.2582804860858883\n",
      "Current iteration=3, the loss=0.8268064815115201, the grad=0.2151640421888607\n",
      "Current iteration=4, the loss=0.8220227668022972, the grad=0.18477447738028685\n",
      "Current iteration=5, the loss=0.8189945335635934, the grad=0.16205621868541814\n",
      "Current iteration=6, the loss=0.8171176505499427, the grad=0.14425037246291625\n",
      "Current iteration=7, the loss=0.8160167918336085, the grad=0.12978252608845464\n",
      "Current iteration=8, the loss=0.8154483884859117, the grad=0.11770596548999028\n",
      "Current iteration=9, the loss=0.8152489526118375, the grad=0.10742104129588159\n",
      "Current iteration=10, the loss=0.8153059112146894, the grad=0.09852879141495631\n",
      "Current iteration=11, the loss=0.8155402718339954, the grad=0.09075149813676399\n",
      "Current iteration=12, the loss=0.8158958494566865, the grad=0.08388785161727522\n",
      "Current iteration=13, the loss=0.8163323123799288, the grad=0.07778668385118098\n",
      "Current iteration=14, the loss=0.8168205488284076, the grad=0.07233102395123836\n",
      "Current iteration=15, the loss=0.817339498395678, the grad=0.06742807969513095\n",
      "Current iteration=16, the loss=0.8178739389484174, the grad=0.06300272704552526\n",
      "Current iteration=17, the loss=0.8184129144649268, the grad=0.05899313750452698\n",
      "Current iteration=18, the loss=0.8189486030564236, the grad=0.05534774593574119\n",
      "Current iteration=19, the loss=0.8194754932476234, the grad=0.052023083043096434\n",
      "Current iteration=20, the loss=0.8199897796028852, the grad=0.04898218165469304\n",
      "Current iteration=21, the loss=0.8204889164646583, the grad=0.04619337473695666\n",
      "Current iteration=22, the loss=0.8209712868614397, the grad=0.04362936838398389\n",
      "Current iteration=23, the loss=0.8214359560114445, the grad=0.04126651304917607\n",
      "Current iteration=24, the loss=0.8218824873818255, the grad=0.039084221306502576\n",
      "Current iteration=25, the loss=0.8223108052520355, the grad=0.03706449639560234\n",
      "Current iteration=26, the loss=0.8227210919934667, the grad=0.035191546214534596\n",
      "Current iteration=27, the loss=0.8231137113494125, the grad=0.033451464363475646\n",
      "Current iteration=28, the loss=0.8234891512348558, the grad=0.03183196457659468\n",
      "Current iteration=29, the loss=0.8238479812158208, the grad=0.03032215818507604\n",
      "Current iteration=30, the loss=0.8241908210397788, the grad=0.028912366616683782\n",
      "Current iteration=31, the loss=0.8245183174888863, the grad=0.027593962663526806\n",
      "Current iteration=32, the loss=0.8248311274998809, the grad=0.026359235537518556\n",
      "Current iteration=33, the loss=0.8251299059981111, the grad=0.02520127571199919\n",
      "Current iteration=34, the loss=0.8254152972718607, the grad=0.024113876304606225\n",
      "Current iteration=35, the loss=0.8256879289986475, the grad=0.02309144834966648\n",
      "Current iteration=36, the loss=0.8259484082509271, the grad=0.022128947779110137\n",
      "Current iteration=37, the loss=0.8261973189719813, the grad=0.021221812308278125\n",
      "Current iteration=38, the loss=0.8264352205366293, the grad=0.02036590672809316\n",
      "Current iteration=39, the loss=0.8266626471054058, the grad=0.019557475353483955\n",
      "Current iteration=40, the loss=0.8268801075522986, the grad=0.018793100581419156\n",
      "Current iteration=41, the loss=0.8270880858004169, the grad=0.018069666679393095\n",
      "Current iteration=42, the loss=0.8272870414412518, the grad=0.01738432806366535\n",
      "Current iteration=43, the loss=0.8274774105445607, the grad=0.01673448144144896\n",
      "Current iteration=44, the loss=0.8276596065897478, the grad=0.01611774128689888\n",
      "Current iteration=45, the loss=0.8278340214676928, the grad=0.015531918200632254\n",
      "Current iteration=46, the loss=0.8280010265156993, the grad=0.014974999769404862\n",
      "Current iteration=47, the loss=0.8281609735585846, the grad=0.014445133598730757\n",
      "Current iteration=48, the loss=0.828314195936753, the grad=0.013940612238503996\n",
      "Current iteration=49, the loss=0.8284610095079711, the grad=0.01345985976156419\n",
      "Current iteration=50, the loss=0.8286017136139362, the grad=0.013001419788875646\n",
      "Current iteration=51, the loss=0.82873659200601, the grad=0.012563944783576975\n",
      "Current iteration=52, the loss=0.828865913726879, the grad=0.012146186460442038\n",
      "Current iteration=53, the loss=0.8289899339466649, the grad=0.011746987177967249\n",
      "Current iteration=54, the loss=0.8291088947532718, the grad=0.011365272197938864\n",
      "Current iteration=55, the loss=0.8292230258976429, the grad=0.011000042712415236\n",
      "Current iteration=56, the loss=0.8293325454952208, the grad=0.010650369550981335\n",
      "Current iteration=57, the loss=0.8294376606853054, the grad=0.010315387492227296\n",
      "Current iteration=58, the loss=0.8295385682502573, the grad=0.009994290112949287\n",
      "Current iteration=59, the loss=0.8296354551966358, the grad=0.009686325116800804\n",
      "Current iteration=60, the loss=0.8297284993004134, the grad=0.009390790091232261\n",
      "Current iteration=61, the loss=0.8298178696184034, the grad=0.00910702864771125\n",
      "Current iteration=62, the loss=0.8299037269680105, the grad=0.008834426905553236\n",
      "Current iteration=63, the loss=0.8299862243773155, the grad=0.008572410284330936\n",
      "Current iteration=64, the loss=0.8300655075074442, the grad=0.008320440573868871\n",
      "Current iteration=65, the loss=0.8301417150490501, the grad=0.008078013254352161\n",
      "Current iteration=66, the loss=0.8302149790946495, the grad=0.007844655042156945\n",
      "Current iteration=67, the loss=0.8302854254884278, the grad=0.007619921639705104\n",
      "Current iteration=68, the loss=0.8303531741550438, the grad=0.007403395670009424\n",
      "Current iteration=69, the loss=0.8304183394088424, the grad=0.007194684778652702\n",
      "Current iteration=70, the loss=0.830481030244799, the grad=0.006993419887772079\n",
      "Current iteration=71, the loss=0.8305413506124175, the grad=0.006799253588231812\n",
      "Current iteration=72, the loss=0.8305993996737194, the grad=0.00661185865759121\n",
      "Current iteration=73, the loss=0.8306552720463789, the grad=0.0064309266927339796\n",
      "Current iteration=74, the loss=0.8307090580329762, the grad=0.006256166847140926\n",
      "Current iteration=75, the loss=0.8307608438372828, the grad=0.006087304663779029\n",
      "Current iteration=76, the loss=0.8308107117684085, the grad=0.005924080995460048\n",
      "Current iteration=77, the loss=0.8308587404335946, the grad=0.0057662510053063535\n",
      "Current iteration=78, the loss=0.8309050049203697, the grad=0.005613583240660875\n",
      "Current iteration=79, the loss=0.8309495769687386, the grad=0.0054658587744027855\n",
      "Current iteration=80, the loss=0.830992525134027, the grad=0.005322870408189403\n",
      "Current iteration=81, the loss=0.8310339149409522, the grad=0.005184421932645412\n",
      "Current iteration=82, the loss=0.8310738090294637, the grad=0.0050503274399697215\n",
      "Current iteration=83, the loss=0.8311122672928433, the grad=0.00492041068483408\n",
      "Current iteration=84, the loss=0.8311493470085334, the grad=0.004794504489810588\n",
      "Current iteration=85, the loss=0.831185102962122, the grad=0.004672450191892663\n",
      "Current iteration=86, the loss=0.8312195875648889, the grad=0.004554097126969457\n",
      "Current iteration=87, the loss=0.8312528509652853, the grad=0.00443930214938026\n",
      "Current iteration=88, the loss=0.8312849411546972, the grad=0.0043279291839172975\n",
      "Current iteration=89, the loss=0.8313159040678209, the grad=0.004219848807863644\n",
      "Current iteration=90, the loss=0.8313457836779561, the grad=0.004114937860851653\n",
      "Current iteration=91, the loss=0.831374622087497, the grad=0.004013079080507416\n",
      "Current iteration=92, the loss=0.8314024596138992, the grad=0.00391416076201053\n",
      "Current iteration=93, the loss=0.831429334871361, the grad=0.003818076439847663\n",
      "Current iteration=94, the loss=0.831455284848461, the grad=0.0037247245901743864\n",
      "Current iteration=95, the loss=0.831480344981975, the grad=0.0036340083523237017\n",
      "Current iteration=96, the loss=0.8315045492270681, the grad=0.003545835268112961\n",
      "Current iteration=97, the loss=0.8315279301240682, the grad=0.0034601170377045304\n",
      "Current iteration=98, the loss=0.8315505188619947, the grad=0.003376769290870312\n",
      "Current iteration=99, the loss=0.8315723453390188, the grad=0.0032957113725970554\n",
      "end of the reg_logistic_regression with w= [-1.52334524e+00  9.57843354e-02 -3.65850345e-01  1.01020064e-01\n",
      " -2.53491206e-02  2.92169235e-01 -2.53491979e-02  4.43563559e-01\n",
      " -5.30198456e-01  8.18725275e-04  1.15053553e-01 -7.82611496e-03\n",
      " -1.40717421e-02 -2.04232653e-01  5.56827098e-02 -2.33932641e-03\n",
      " -6.13555227e-01 -1.89575457e-02  1.28600613e-01]  and loss= 0.8315723453390188\n",
      "shape of txOpti[i]  (77544, 23)\n",
      "Current iteration=0, the loss=0.8487076854369229, the grad=0.3493400704524976\n",
      "Current iteration=1, the loss=0.8356473836949326, the grad=0.2548449782869961\n",
      "Current iteration=2, the loss=0.8284431217162158, the grad=0.19911932319399395\n",
      "Current iteration=3, the loss=0.8243587171758742, the grad=0.16354576331474285\n",
      "Current iteration=4, the loss=0.822069198168224, the grad=0.13883755911521936\n",
      "Current iteration=5, the loss=0.8208695446682177, the grad=0.12053083735228227\n",
      "Current iteration=6, the loss=0.820354799017953, the grad=0.10631321019073611\n",
      "Current iteration=7, the loss=0.8202787097504404, the grad=0.0948804651423835\n",
      "Current iteration=8, the loss=0.8204851003834629, the grad=0.08544184332395177\n",
      "Current iteration=9, the loss=0.820871802437674, the grad=0.07748938660092823\n",
      "Current iteration=10, the loss=0.8213704591763614, the grad=0.07068126287258844\n",
      "Current iteration=11, the loss=0.8219346236530377, the grad=0.06477836519771857\n",
      "Current iteration=12, the loss=0.8225324447858647, the grad=0.05960780119143919\n",
      "Current iteration=13, the loss=0.8231420129739502, the grad=0.05504087586872335\n",
      "Current iteration=14, the loss=0.8237483094573329, the grad=0.050979310766554944\n",
      "Current iteration=15, the loss=0.8243411561885599, the grad=0.047346344568131485\n",
      "Current iteration=16, the loss=0.8249138086721745, the grad=0.0440808267490212\n",
      "Current iteration=17, the loss=0.8254619729018665, the grad=0.04113319904313673\n",
      "Current iteration=18, the loss=0.8259831084969177, the grad=0.03846269729456762\n",
      "Current iteration=19, the loss=0.8264759288710422, the grad=0.0360353599669423\n",
      "Current iteration=20, the loss=0.8269400394106553, the grad=0.033822581007727705\n",
      "Current iteration=21, the loss=0.8273756737610615, the grad=0.03180003738341563\n",
      "Current iteration=22, the loss=0.8277835007330049, the grad=0.02994687945318318\n",
      "Current iteration=23, the loss=0.8281644825752729, the grad=0.02824510916314932\n",
      "Current iteration=24, the loss=0.8285197709273292, the grad=0.02667909487283699\n",
      "Current iteration=25, the loss=0.8288506305995011, the grad=0.025235187298253327\n",
      "Current iteration=26, the loss=0.8291583840100888, the grad=0.02390141152312694\n",
      "Current iteration=27, the loss=0.829444371012017, the grad=0.022667217125353405\n",
      "Current iteration=28, the loss=0.8297099202095329, the grad=0.0215232733467973\n",
      "Current iteration=29, the loss=0.8299563288595582, the grad=0.020461299641596147\n",
      "Current iteration=30, the loss=0.8301848491817732, the grad=0.019473924350823534\n",
      "Current iteration=31, the loss=0.8303966794412686, the grad=0.018554565984645516\n",
      "Current iteration=32, the loss=0.8305929585698174, the grad=0.01769733285604105\n",
      "Current iteration=33, the loss=0.830774763393379, the grad=0.01689693774325232\n",
      "Current iteration=34, the loss=0.8309431077606323, the grad=0.01614862495689674\n",
      "Current iteration=35, the loss=0.8310989430392429, the grad=0.015448107717793117\n",
      "Current iteration=36, the loss=0.8312431595770251, the grad=0.014791514158731686\n",
      "Current iteration=37, the loss=0.8313765888244482, the grad=0.01417534057982043\n",
      "Current iteration=38, the loss=0.8315000058906129, the grad=0.013596410835582893\n",
      "Current iteration=39, the loss=0.8316141323625682, the grad=0.013051840929177853\n",
      "Current iteration=40, the loss=0.831719639261916, the grad=0.012539008047001006\n",
      "Current iteration=41, the loss=0.8318171500462831, the grad=0.012055523394398276\n",
      "Current iteration=42, the loss=0.8319072435888606, the grad=0.011599208296899761\n",
      "Current iteration=43, the loss=0.8319904570886923, the grad=0.011168073116278283\n",
      "Current iteration=44, the loss=0.8320672888791606, the grad=0.01076029860066629\n",
      "Current iteration=45, the loss=0.8321382011132537, the grad=0.01037421934587427\n",
      "Current iteration=46, the loss=0.8322036223125469, the grad=0.010008309093237463\n",
      "Current iteration=47, the loss=0.8322639497730454, the grad=0.009661167629577945\n",
      "Current iteration=48, the loss=0.8323195518255991, the grad=0.009331509088637259\n",
      "Current iteration=49, the loss=0.8323707699519112, the grad=0.009018151481753781\n",
      "Current iteration=50, the loss=0.8324179207595155, the grad=0.00872000730955062\n",
      "Current iteration=51, the loss=0.8324612978207112, the grad=0.008436075126710846\n",
      "Current iteration=52, the loss=0.8325011733815193, the grad=0.008165431949157389\n",
      "Current iteration=53, the loss=0.8325377999473684, the grad=0.007907226407625456\n",
      "Current iteration=54, the loss=0.8325714117525633, the grad=0.007660672564127485\n",
      "Current iteration=55, the loss=0.832602226120709, the grad=0.007425044318507629\n",
      "Current iteration=56, the loss=0.8326304447231965, the grad=0.007199670341446992\n",
      "Current iteration=57, the loss=0.8326562547427069, the grad=0.0069839294781504704\n",
      "Current iteration=58, the loss=0.832679829948422, the grad=0.00677724657371806\n",
      "Current iteration=59, the loss=0.8327013316893351, the grad=0.00657908867704527\n",
      "Current iteration=60, the loss=0.832720909811719, the grad=0.006388961585146989\n",
      "Current iteration=61, the loss=0.832738703506448, the grad=0.006206406694175536\n",
      "Current iteration=62, the loss=0.8327548420915223, the grad=0.006030998127204861\n",
      "Current iteration=63, the loss=0.8327694457347747, the grad=0.005862340112163179\n",
      "Current iteration=64, the loss=0.8327826261214021, the grad=0.005700064586185934\n",
      "Current iteration=65, the loss=0.8327944870706231, the grad=0.005543829005190275\n",
      "Current iteration=66, the loss=0.8328051251054507, the grad=0.0053933143396914475\n",
      "Current iteration=67, the loss=0.8328146299792573, the grad=0.005248223239833958\n",
      "Current iteration=68, the loss=0.8328230851625358, the grad=0.005108278354332497\n",
      "Current iteration=69, the loss=0.8328305682929878, the grad=0.004973220789540632\n",
      "Current iteration=70, the loss=0.832837151591827, the grad=0.00484280869621531\n",
      "Current iteration=71, the loss=0.8328429022489511, the grad=0.0047168159727454684\n",
      "Current iteration=72, the loss=0.8328478827794272, the grad=0.004595031074681907\n",
      "Current iteration=73, the loss=0.8328521513535387, the grad=0.004477255921360585\n",
      "Current iteration=74, the loss=0.8328557621024543, the grad=0.0043633048912658295\n",
      "Current iteration=75, the loss=0.8328587654014196, the grad=0.004253003898546314\n",
      "Current iteration=76, the loss=0.8328612081322171, the grad=0.004146189543785637\n",
      "Current iteration=77, the loss=0.8328631339264921, the grad=0.004042708332749125\n",
      "Current iteration=78, the loss=0.8328645833914221, the grad=0.0039424159573882974\n",
      "Current iteration=79, the loss=0.83286559431908, the grad=0.0038451766338892905\n",
      "Current iteration=80, the loss=0.8328662018807381, the grad=0.003750862493009287\n",
      "Current iteration=81, the loss=0.8328664388072556, the grad=0.0036593530183588806\n",
      "Current iteration=82, the loss=0.8328663355566016, the grad=0.0035705345286644445\n",
      "Current iteration=83, the loss=0.8328659204694833, the grad=0.0034842997003858463\n",
      "Current iteration=84, the loss=0.8328652199139684, the grad=0.003400547127375177\n",
      "Current iteration=85, the loss=0.8328642584199267, the grad=0.0033191809145448133\n",
      "Current iteration=86, the loss=0.8328630588040362, the grad=0.003240110302770134\n",
      "Current iteration=87, the loss=0.8328616422860633, the grad=0.0031632493224870504\n",
      "Current iteration=88, the loss=0.8328600285970473, the grad=0.0030885164736581716\n",
      "Current iteration=89, the loss=0.8328582360799849, the grad=0.00301583442997677\n",
      "Current iteration=90, the loss=0.8328562817835624, the grad=0.0029451297653560553\n",
      "Current iteration=91, the loss=0.832854181549435, the grad=0.0028763327009140817\n",
      "Current iteration=92, the loss=0.8328519500935195, the grad=0.0028093768708135273\n",
      "Current iteration=93, the loss=0.832849601081731, the grad=0.0027441991054518865\n",
      "Current iteration=94, the loss=0.832847147200559, the grad=0.0026807392306218824\n",
      "Current iteration=95, the loss=0.8328446002228481, the grad=0.0026189398813761347\n",
      "Current iteration=96, the loss=0.832841971069126, the grad=0.0025587463294343043\n",
      "Current iteration=97, the loss=0.832839269864789, the grad=0.0025001063230664306\n",
      "Current iteration=98, the loss=0.8328365059934363, the grad=0.002442969938473621\n",
      "Current iteration=99, the loss=0.8328336881466237, the grad=0.0023872894417673895\n",
      "end of the reg_logistic_regression with w= [-0.79225489  0.64636916 -0.61786967  0.10071399 -0.01356168 -0.13551257\n",
      "  0.06756957 -0.05819323 -0.24056227  0.3610886   0.19305987 -0.00392031\n",
      "  0.00258219  0.03722612 -0.01610692  0.02357572 -0.05829248  0.00841678\n",
      "  0.00947451  0.09638358 -0.01337368  0.00560801  0.09638367]  and loss= 0.8328336881466237\n",
      "shape of txOpti[i]  (50379, 30)\n",
      "Current iteration=0, the loss=0.7802043854332917, the grad=0.42318955307514045\n",
      "Current iteration=1, the loss=0.7560222558175479, the grad=0.27730424635407386\n",
      "Current iteration=2, the loss=0.7419768596807205, the grad=0.2020219033066087\n",
      "Current iteration=3, the loss=0.732719642962962, the grad=0.15797408284156672\n",
      "Current iteration=4, the loss=0.7261032732541933, the grad=0.12930472118966238\n",
      "Current iteration=5, the loss=0.7211084210041127, the grad=0.10923758059100504\n",
      "Current iteration=6, the loss=0.7171890761999294, the grad=0.09444205038803129\n",
      "Current iteration=7, the loss=0.714025323233427, the grad=0.08309547583657532\n",
      "Current iteration=8, the loss=0.7114162442974763, the grad=0.07412012451935186\n",
      "Current iteration=9, the loss=0.709228482613769, the grad=0.06684072159303123\n",
      "Current iteration=10, the loss=0.7073694685338843, the grad=0.06081442550458409\n",
      "Current iteration=11, the loss=0.7057725443765558, the grad=0.05573975591445868\n",
      "Current iteration=12, the loss=0.7043882376290005, the grad=0.05140487495860031\n",
      "Current iteration=13, the loss=0.7031788978813636, the grad=0.04765681399475009\n",
      "Current iteration=14, the loss=0.7021152659023114, the grad=0.04438244310974862\n",
      "Current iteration=15, the loss=0.7011742004925139, the grad=0.04149630996993734\n",
      "Current iteration=16, the loss=0.7003371253894669, the grad=0.03893264378699549\n",
      "Current iteration=17, the loss=0.6995889390100596, the grad=0.03663996411188343\n",
      "Current iteration=18, the loss=0.6989172305871336, the grad=0.034577363550505794\n",
      "Current iteration=19, the loss=0.6983117045591248, the grad=0.03271189247523753\n",
      "Current iteration=20, the loss=0.6977637499049426, the grad=0.031016685083578733\n",
      "Current iteration=21, the loss=0.6972661125401267, the grad=0.029469594011079264\n",
      "Current iteration=22, the loss=0.696812642419161, the grad=0.028052180037915014\n",
      "Current iteration=23, the loss=0.6963980957404448, the grad=0.02674895378006673\n",
      "Current iteration=24, the loss=0.696017978438173, the grad=0.025546798878401406\n",
      "Current iteration=25, the loss=0.6956684210516364, the grad=0.024434527734450266\n",
      "Current iteration=26, the loss=0.6953460777487888, the grad=0.023402535302952387\n",
      "Current iteration=27, the loss=0.6950480441606322, the grad=0.02244252631437343\n",
      "Current iteration=28, the loss=0.6947717900196092, the grad=0.02154729812392587\n",
      "Current iteration=29, the loss=0.6945151035599105, the grad=0.020710566165879712\n",
      "Current iteration=30, the loss=0.6942760453436121, the grad=0.019926822384198076\n",
      "Current iteration=31, the loss=0.6940529096999152, the grad=0.019191219443781726\n",
      "Current iteration=32, the loss=0.6938441923573682, the grad=0.018499475290326438\n",
      "Current iteration=33, the loss=0.6936485631467395, the grad=0.017847793917946594\n",
      "Current iteration=34, the loss=0.6934648428804273, the grad=0.017232799157926113\n",
      "Current iteration=35, the loss=0.6932919836908176, the grad=0.016651479013647833\n",
      "Current iteration=36, the loss=0.6931290522477941, the grad=0.01610113860232155\n",
      "Current iteration=37, the loss=0.6929752153839832, the grad=0.015579360170706548\n",
      "Current iteration=38, the loss=0.6928297277422542, the grad=0.015083968963339127\n",
      "Current iteration=39, the loss=0.6926919211285631, the grad=0.014613003962175264\n",
      "Current iteration=40, the loss=0.6925611953083336, the grad=0.01416469270374476\n",
      "Current iteration=41, the loss=0.6924370100290804, the grad=0.013737429526854845\n",
      "Current iteration=42, the loss=0.6923188780881618, the grad=0.013329756720146607\n",
      "Current iteration=43, the loss=0.6922063592940886, the grad=0.012940348131506905\n",
      "Current iteration=44, the loss=0.6920990551940696, the grad=0.012567994875792465\n",
      "Current iteration=45, the loss=0.6919966044604591, the grad=0.012211592837542399\n",
      "Current iteration=46, the loss=0.6918986788453179, the grad=0.011870131714383664\n",
      "Current iteration=47, the loss=0.6918049796260504, the grad=0.011542685386999665\n",
      "Current iteration=48, the loss=0.6917152344765514, the grad=0.011228403434626515\n",
      "Current iteration=49, the loss=0.6916291947078965, the grad=0.01092650364245622\n",
      "Current iteration=50, the loss=0.691546632830678, the grad=0.010636265370147297\n",
      "Current iteration=51, the loss=0.6914673403978788, the grad=0.010357023669726715\n",
      "Current iteration=52, the loss=0.691391126092923, the grad=0.010088164057190924\n",
      "Current iteration=53, the loss=0.69131781403241, the grad=0.009829117855619008\n",
      "Current iteration=54, the loss=0.6912472422571699, the grad=0.009579358039033202\n",
      "Current iteration=55, the loss=0.6911792613888138, the grad=0.009338395515932517\n",
      "Current iteration=56, the loss=0.6911137334319543, the grad=0.009105775799670385\n",
      "Current iteration=57, the loss=0.6910505307048573, the grad=0.008881076019881633\n",
      "Current iteration=58, the loss=0.690989534883504, the grad=0.008663902235179535\n",
      "Current iteration=59, the loss=0.6909306361459366, the grad=0.008453887012500482\n",
      "Current iteration=60, the loss=0.6908737324054204, the grad=0.008250687242903024\n",
      "Current iteration=61, the loss=0.690818728622361, the grad=0.008053982167441086\n",
      "Current iteration=62, the loss=0.6907655361861563, the grad=0.007863471590018921\n",
      "Current iteration=63, the loss=0.6907140723592211, the grad=0.007678874256976645\n",
      "Current iteration=64, the loss=0.6906642597763617, the grad=0.0074999263856140785\n",
      "Current iteration=65, the loss=0.6906160259934718, the grad=0.007326380325992865\n",
      "Current iteration=66, the loss=0.6905693030802377, the grad=0.007158003342208603\n",
      "Current iteration=67, the loss=0.6905240272521534, the grad=0.006994576500936039\n",
      "Current iteration=68, the loss=0.6904801385376815, the grad=0.006835893656454455\n",
      "Current iteration=69, the loss=0.6904375804768754, the grad=0.006681760522585875\n",
      "Current iteration=70, the loss=0.69039629984819, the grad=0.006531993823050641\n",
      "Current iteration=71, the loss=0.6903562464205742, the grad=0.006386420512683352\n",
      "Current iteration=72, the loss=0.6903173727282579, the grad=0.006244877062775516\n",
      "Current iteration=73, the loss=0.6902796338659333, the grad=0.006107208804535042\n",
      "Current iteration=74, the loss=0.6902429873022834, the grad=0.005973269325289242\n",
      "Current iteration=75, the loss=0.6902073927100173, the grad=0.005842919912619826\n",
      "Current iteration=76, the loss=0.6901728118107903, the grad=0.005716029042114036\n",
      "Current iteration=77, the loss=0.6901392082335407, the grad=0.005592471904854866\n",
      "Current iteration=78, the loss=0.6901065473849406, the grad=0.005472129971161909\n",
      "Current iteration=79, the loss=0.6900747963307892, the grad=0.005354890587439467\n",
      "Current iteration=80, the loss=0.6900439236873023, the grad=0.0052406466032952435\n",
      "Current iteration=81, the loss=0.6900138995213584, the grad=0.005129296026365988\n",
      "Current iteration=82, the loss=0.6899846952588552, the grad=0.00502074170253022\n",
      "Current iteration=83, the loss=0.6899562836004242, the grad=0.00491489101940565\n",
      "Current iteration=84, the loss=0.689928638443817, the grad=0.004811655631223785\n",
      "Current iteration=85, the loss=0.6899017348123538, the grad=0.004710951203348594\n",
      "Current iteration=86, the loss=0.6898755487888834, the grad=0.004612697174862932\n",
      "Current iteration=87, the loss=0.6898500574547548, the grad=0.0045168165377870145\n",
      "Current iteration=88, the loss=0.6898252388333579, the grad=0.004423235631620131\n",
      "Current iteration=89, the loss=0.689801071837822, the grad=0.004331883952011118\n",
      "Current iteration=90, the loss=0.6897775362225158, the grad=0.004242693972466013\n",
      "Current iteration=91, the loss=0.6897546125380117, the grad=0.0041556009780948635\n",
      "Current iteration=92, the loss=0.6897322820892238, the grad=0.004070542910483857\n",
      "Current iteration=93, the loss=0.6897105268964421, the grad=0.003987460222855554\n",
      "Current iteration=94, the loss=0.6896893296590282, the grad=0.003906295744749367\n",
      "Current iteration=95, the loss=0.6896686737215431, the grad=0.00382699455551746\n",
      "Current iteration=96, the loss=0.6896485430421128, the grad=0.003749503865988705\n",
      "Current iteration=97, the loss=0.6896289221628433, the grad=0.003673772907705424\n",
      "Current iteration=98, the loss=0.6896097961821318, the grad=0.003599752829185307\n",
      "Current iteration=99, the loss=0.6895911507287075, the grad=0.003527396598704369\n",
      "end of the reg_logistic_regression with w= [ 3.98726055e-02  8.43108953e-01 -2.98001535e-01  1.43563469e-01\n",
      "  2.47975214e-01  1.31292144e-02  7.14153516e-02 -1.43896488e-01\n",
      " -1.43948830e-01 -1.94186673e-01 -1.60171118e-01 -1.47984442e-01\n",
      "  3.19242267e-01  6.00659614e-01  1.40665264e-01 -4.52845988e-03\n",
      "  6.24029174e-03 -3.74103739e-02 -1.89525894e-02  1.27291442e-04\n",
      " -1.16575787e-01  1.50399434e-02 -1.46390394e-01  1.16304351e-01\n",
      "  4.38156422e-04 -3.71960559e-03  1.43403432e-01 -5.80495071e-03\n",
      " -1.42985848e-02  1.05044859e-01]  and loss= 0.6895911507287075\n",
      "shape of txOpti[i]  (22164, 30)\n",
      "Current iteration=0, the loss=0.8780597554820373, the grad=0.2942102007130968\n",
      "Current iteration=1, the loss=0.871201475432478, the grad=0.2450909406104256\n",
      "Current iteration=2, the loss=0.8669518261301089, the grad=0.2074318271471073\n",
      "Current iteration=3, the loss=0.8644337747530471, the grad=0.1782951155371696\n",
      "Current iteration=4, the loss=0.8630614136265294, the grad=0.15531443099960918\n",
      "Current iteration=5, the loss=0.8624480125433686, the grad=0.1368061464914517\n",
      "Current iteration=6, the loss=0.8623366568467894, the grad=0.12161059883663174\n",
      "Current iteration=7, the loss=0.8625546592761684, the grad=0.10892643422545979\n",
      "Current iteration=8, the loss=0.862984604467351, the grad=0.09819010310935884\n",
      "Current iteration=9, the loss=0.8635459372914859, the grad=0.08899619207651345\n",
      "Current iteration=10, the loss=0.8641830892472094, the grad=0.0810460601313024\n",
      "Current iteration=11, the loss=0.8648576799093068, the grad=0.07411466730946509\n",
      "Current iteration=12, the loss=0.8655432983294188, the grad=0.06802886575348334\n",
      "Current iteration=13, the loss=0.8662219495820574, the grad=0.06265292794052359\n",
      "Current iteration=14, the loss=0.8668815974706419, the grad=0.05787869897375573\n",
      "Current iteration=15, the loss=0.8675144425324666, the grad=0.05361875239609688\n",
      "Current iteration=16, the loss=0.8681157018343648, the grad=0.04980153339472265\n",
      "Current iteration=17, the loss=0.8686827364968346, the grad=0.04636784265284904\n",
      "Current iteration=18, the loss=0.8692144234421757, the grad=0.043268242241930134\n",
      "Current iteration=19, the loss=0.8697107006692317, the grad=0.04046110781064103\n",
      "Current iteration=20, the loss=0.8701722370428637, the grad=0.03791114218205183\n",
      "Current iteration=21, the loss=0.8706001921686758, the grad=0.03558822418873221\n",
      "Current iteration=22, the loss=0.8709960418848287, the grad=0.033466505147865144\n",
      "Current iteration=23, the loss=0.8713614518050571, the grad=0.03152369113107085\n",
      "Current iteration=24, the loss=0.8716981861912934, the grad=0.029740466654343992\n",
      "Current iteration=25, the loss=0.8720080428732662, the grad=0.02810002745283132\n",
      "Current iteration=26, the loss=0.8722928073986578, the grad=0.026587698429346836\n",
      "Current iteration=27, the loss=0.8725542213819003, the grad=0.025190618848135535\n",
      "Current iteration=28, the loss=0.8727939613211131, the grad=0.0238974811553806\n",
      "Current iteration=29, the loss=0.8730136251083154, the grad=0.02269831295598475\n",
      "Current iteration=30, the loss=0.8732147241639501, the grad=0.021584294005958272\n",
      "Current iteration=31, the loss=0.8733986796508567, the grad=0.020547601825685546\n",
      "Current iteration=32, the loss=0.8735668216136268, the grad=0.019581280863279624\n",
      "Current iteration=33, the loss=0.8737203901817151, the grad=0.01867913115236254\n",
      "Current iteration=34, the loss=0.8738605381941512, the grad=0.017835613195051222\n",
      "Current iteration=35, the loss=0.8739883347687437, the grad=0.017045766416081323\n",
      "Current iteration=36, the loss=0.8741047694630044, the grad=0.016305139019460867\n",
      "Current iteration=37, the loss=0.874210756767708, the grad=0.015609727465292999\n",
      "Current iteration=38, the loss=0.8743071407446485, the grad=0.014955924094029833\n",
      "Current iteration=39, the loss=0.8743946996733255, the grad=0.014340471675322639\n",
      "Current iteration=40, the loss=0.8744741506112952, the grad=0.013760423861603775\n",
      "Current iteration=41, the loss=0.8745461538028709, the grad=0.01321311069233269\n",
      "Current iteration=42, the loss=0.8746113168932375, the grad=0.012696108430978613\n",
      "Current iteration=43, the loss=0.8746701989216299, the grad=0.012207213129138954\n",
      "Current iteration=44, the loss=0.8747233140794423, the grad=0.011744417405276437\n",
      "Current iteration=45, the loss=0.8747711352280196, the grad=0.011305890003008585\n",
      "Current iteration=46, the loss=0.8748140971772581, the grad=0.010889957758566772\n",
      "Current iteration=47, the loss=0.8748525997306151, the grad=0.010495089661249556\n",
      "Current iteration=48, the loss=0.8748870105051727, the grad=0.010119882736269665\n",
      "Current iteration=49, the loss=0.874917667537373, the grad=0.009763049517825974\n",
      "Current iteration=50, the loss=0.8749448816862286, the grad=0.009423406912732434\n",
      "Current iteration=51, the loss=0.874968938846402, the grad=0.009099866282493613\n",
      "Current iteration=52, the loss=0.8749901019837238, the grad=0.00879142459514314\n",
      "Current iteration=53, the loss=0.8750086130055825, the grad=0.008497156518123244\n",
      "Current iteration=54, the loss=0.8750246944782698, the grad=0.008216207340533976\n",
      "Current iteration=55, the loss=0.8750385512028825, the grad=0.007947786627675313\n",
      "Current iteration=56, the loss=0.8750503716607946, the grad=0.007691162523326136\n",
      "Current iteration=57, the loss=0.875060329339079, the grad=0.007445656625966915\n",
      "Current iteration=58, the loss=0.8750685839456069, the grad=0.007210639374424519\n",
      "Current iteration=59, the loss=0.875075282522878, the grad=0.006985525886419541\n",
      "Current iteration=60, the loss=0.8750805604689943, the grad=0.006769772200416514\n",
      "Current iteration=61, the loss=0.8750845424735506, the grad=0.006562871877172892\n",
      "Current iteration=62, the loss=0.8750873433756164, the grad=0.006364352922586973\n",
      "Current iteration=63, the loss=0.8750890689504085, the grad=0.006173774997970363\n",
      "Current iteration=64, the loss=0.8750898166307203, the grad=0.005990726887813288\n",
      "Current iteration=65, the loss=0.8750896761686662, the grad=0.005814824198551565\n",
      "Current iteration=66, the loss=0.8750887302428384, the grad=0.0056457072648519145\n",
      "Current iteration=67, the loss=0.8750870550155375, the grad=0.005483039242566327\n",
      "Current iteration=68, the loss=0.8750847206443388, the grad=0.005326504369816541\n",
      "Current iteration=69, the loss=0.8750817917518909, the grad=0.005175806379699947\n",
      "Current iteration=70, the loss=0.8750783278575055, the grad=0.005030667049894409\n",
      "Current iteration=71, the loss=0.8750743837737821, the grad=0.004890824876014643\n",
      "Current iteration=72, the loss=0.8750700099712407, the grad=0.004756033856962726\n",
      "Current iteration=73, the loss=0.8750652529136586, the grad=0.0046260623817447185\n",
      "Current iteration=74, the loss=0.8750601553665882, the grad=0.004500692208313925\n",
      "Current iteration=75, the loss=0.8750547566813072, the grad=0.0043797175259664934\n",
      "Current iteration=76, the loss=0.8750490930562532, the grad=0.004262944093672615\n",
      "Current iteration=77, the loss=0.8750431977778258, the grad=0.004150188447488878\n",
      "Current iteration=78, the loss=0.8750371014422651, the grad=0.004041277170876532\n",
      "Current iteration=79, the loss=0.8750308321601721, the grad=0.003936046222355898\n",
      "Current iteration=80, the loss=0.8750244157450997, the grad=0.003834340315467908\n",
      "Current iteration=81, the loss=0.8750178758875178, the grad=0.003736012346497317\n",
      "Current iteration=82, the loss=0.8750112343153481, the grad=0.003640922865844917\n",
      "Current iteration=83, the loss=0.8750045109421541, the grad=0.0035489395893239734\n",
      "Current iteration=84, the loss=0.8749977240039866, the grad=0.003459936946004142\n",
      "Current iteration=85, the loss=0.8749908901857939, the grad=0.0033737956595390863\n",
      "Current iteration=86, the loss=0.8749840247382341, the grad=0.003290402360194931\n",
      "Current iteration=87, the loss=0.8749771415856481, the grad=0.0032096492250501484\n",
      "Current iteration=88, the loss=0.8749702534258994, the grad=0.0031314336440655005\n",
      "Current iteration=89, the loss=0.8749633718227153, the grad=0.0030556579099286735\n",
      "Current iteration=90, the loss=0.8749565072911215, the grad=0.0029822289297640123\n",
      "Current iteration=91, the loss=0.8749496693765065, the grad=0.0029110579569659218\n",
      "Current iteration=92, the loss=0.8749428667278109, the grad=0.0028420603415664353\n",
      "Current iteration=93, the loss=0.8749361071652952, the grad=0.0027751552976851912\n",
      "Current iteration=94, the loss=0.8749293977433034, the grad=0.002710265686734801\n",
      "Current iteration=95, the loss=0.8749227448084024, the grad=0.0026473178151676735\n",
      "Current iteration=96, the loss=0.8749161540532542, the grad=0.002586241245653332\n",
      "Current iteration=97, the loss=0.8749096305665388, the grad=0.0025269686206682786\n",
      "Current iteration=98, the loss=0.8749031788792292, the grad=0.0024694354975656266\n",
      "Current iteration=99, the loss=0.8748968030074915, the grad=0.002413580194268732\n",
      "end of the reg_logistic_regression with w= [-1.01116211e+00  7.62018253e-01 -3.38686525e-01  7.04882054e-02\n",
      "  3.25656931e-01 -7.27903137e-02 -3.99618573e-02 -1.42142435e-01\n",
      " -1.75677973e-01 -6.95000666e-02 -1.58341508e-01 -1.25881034e-01\n",
      "  2.58544673e-01  2.49180142e-02  1.27461474e-01 -3.90082798e-02\n",
      "  1.97685983e-03 -4.00967644e-02  1.89001626e-02 -1.53298570e-03\n",
      " -3.66085116e-02  4.23468641e-02 -1.40615848e-01  8.63629803e-03\n",
      " -3.11485022e-02 -3.91635221e-04  6.31043191e-02  3.10208070e-02\n",
      "  9.17269513e-03  1.49114330e-02]  and loss= 0.8748968030074915\n",
=======
      "Current iteration=0\t the loss=0.8669964\t the grad=0.4256736\n",
      "Current iteration=25\t the loss=0.8223108\t the grad=0.0370645\n",
      "Current iteration=50\t the loss=0.8286017\t the grad=0.0130014\n",
      "Current iteration=75\t the loss=0.8307608\t the grad=0.0060873\n",
      "end of the reg_logistic_regression with w= [    -1.523      0.096     -0.366      0.101     -0.025      0.292\n",
      "     -0.025      0.444     -0.530      0.001      0.115     -0.008\n",
      "     -0.014     -0.204      0.056     -0.002     -0.614     -0.019\n",
      "      0.129]  and loss= 0.8315723453390188\n",
      "shape of txOpti[i]  (77544, 23)\n",
      "Current iteration=0\t the loss=0.8487077\t the grad=0.3493401\n",
      "Current iteration=25\t the loss=0.8288506\t the grad=0.0252352\n",
      "Current iteration=50\t the loss=0.8324179\t the grad=0.0087200\n",
      "Current iteration=75\t the loss=0.8328588\t the grad=0.0042530\n",
      "end of the reg_logistic_regression with w= [    -0.792      0.646     -0.618      0.101     -0.014     -0.136\n",
      "      0.068     -0.058     -0.241      0.361      0.193     -0.004\n",
      "      0.003      0.037     -0.016      0.024     -0.058      0.008\n",
      "      0.009      0.096     -0.013      0.006      0.096]  and loss= 0.8328336881466237\n",
      "shape of txOpti[i]  (50379, 30)\n",
      "Current iteration=0\t the loss=0.7802044\t the grad=0.4231896\n",
      "Current iteration=25\t the loss=0.6956684\t the grad=0.0244345\n",
      "Current iteration=50\t the loss=0.6915466\t the grad=0.0106363\n",
      "Current iteration=75\t the loss=0.6902074\t the grad=0.0058429\n",
      "end of the reg_logistic_regression with w= [     0.040      0.843     -0.298      0.144      0.248      0.013\n",
      "      0.071     -0.144     -0.144     -0.194     -0.160     -0.148\n",
      "      0.319      0.601      0.141     -0.005      0.006     -0.037\n",
      "     -0.019      0.000     -0.117      0.015     -0.146      0.116\n",
      "      0.000     -0.004      0.143     -0.006     -0.014      0.105]  and loss= 0.6895911507287075\n",
      "shape of txOpti[i]  (22164, 30)\n",
      "Current iteration=0\t the loss=0.8780598\t the grad=0.2942102\n",
      "Current iteration=25\t the loss=0.8720080\t the grad=0.0281000\n",
      "Current iteration=50\t the loss=0.8749449\t the grad=0.0094234\n",
      "Current iteration=75\t the loss=0.8750548\t the grad=0.0043797\n",
      "end of the reg_logistic_regression with w= [    -1.011      0.762     -0.339      0.070      0.326     -0.073\n",
      "     -0.040     -0.142     -0.176     -0.070     -0.158     -0.126\n",
      "      0.259      0.025      0.127     -0.039      0.002     -0.040\n",
      "      0.019     -0.002     -0.037      0.042     -0.141      0.009\n",
      "     -0.031     -0.000      0.063      0.031      0.009      0.015]  and loss= 0.8748968030074915\n",
>>>>>>> 4407e07f603a468effa38a3c578d17288c7b3307
      "the accuracy on the train set is  0.8145086225015764\n",
      "the accuracy on the train set is  0.7325260497266068\n",
      "the accuracy on the train set is  0.7490621092121718\n",
      "the accuracy on the train set is  0.7361487096192023\n",
      "the total accuracy on the train set is  0.7580613727648893\n"
     ]
    }
   ],
   "source": [
    "#test logistic regularized regression with spliting data\n",
    "y_tr[np.where(y_tr == -1)] = 0\n",
    "txOpti, yOpti, idsOpti = dataClean(tx_tr, y_tr)\n",
    "\n",
    "max_iters = 100\n",
    "gamma = 0.5\n",
    "lambda_ = 1e-06\n",
    "ws = []\n",
    "losses = []\n",
    "\n",
    "for i in range(4): \n",
    "    print(f\"shape of txOpti[{i}] \", txOpti[i].shape)\n",
    "    initial_w = np.zeros(txOpti[i].shape[1])\n",
    "    w, loss = reg_logistic_regression(yOpti[i], txOpti[i], lambda_, initial_w, max_iters, gamma)\n",
    "    ws.append(w)\n",
    "    losses.append(loss)\n",
    "    print(\"end of the reg_logistic_regression with w=\",w,\" and loss=\", loss)\n",
    "\n",
    "    \n",
    "labels = []\n",
    "accs = []\n",
    "for i in range(4): \n",
    "    label = predict_logistic(ws[i], txOpti[i])\n",
    "    yOpti[i][np.where(yOpti[i] == 0)] = -1\n",
    "    acc = calculate_accuracy(yOpti[i], label)\n",
    "    print(\"the accuracy on the train set is \", acc)\n",
    "    accs.append(acc)\n",
    "    labels.append(label)\n",
    "\n",
    "accTot = (accs[0] + accs[1] + accs[2] + accs[3])/4\n",
    "print(\"the total accuracy on the train set is \", accTot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "79798248-189a-48f3-ab3f-39bcaf3a9620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_73/3621360310.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  print(np.array(ws).shape)\n"
     ]
    }
   ],
   "source": [
    "print(np.array(ws).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae72840e-5cfd-4982-8ec5-a684aec49857",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
