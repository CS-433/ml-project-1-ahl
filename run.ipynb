{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "882a9018-5319-4a82-87e7-2b0d420475bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from implementations import *\n",
    "from helpers import *\n",
    "from helper_functions import *\n",
    "from optimization import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d56f0e3-9ee7-448f-a9dd-dd349209cc8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading the training dataset\n",
      "training dataset loaded\n"
     ]
    }
   ],
   "source": [
    "# load the training dataset\n",
    "print(\"loading the training dataset\")\n",
    "DATA_TRAIN_PATH = 'train.csv'\n",
    "y_tr, tx_tr, ids_tr = load_csv_data(DATA_TRAIN_PATH)\n",
    "print(\"training dataset loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af90503c-e3fa-4e1b-a222-63ef11b39a80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading the test dataset\n",
      "test dataset loaded\n"
     ]
    }
   ],
   "source": [
    "# load the test dataset\n",
    "print(\"loading the test dataset\")\n",
    "DATA_TEST_PATH = 'test.csv'\n",
    "y_te, tx_te, ids_te = load_csv_data(DATA_TEST_PATH)\n",
    "print(\"test dataset loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa8da103-df5b-4f1e-b595-9ae0cd91750e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shape of the training y is  (250000,)  and the training tx is  (250000, 30)\n",
      "the shape of the test y is  (568238,)  and the test tx is  (568238, 30)\n"
     ]
    }
   ],
   "source": [
    "# printing the shapes of the arrays\n",
    "print(\"the shape of the training y is \",y_tr.shape,\" and the training tx is \", tx_tr.shape)\n",
    "print(\"the shape of the test y is \",y_te.shape,\" and the test tx is \",tx_te.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6884129-6892-4ec8-bce6-fe61104b9ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing the mean square gd\n",
    "initial_w = np.zeros(tx_tr.shape[1])\n",
    "max_iters = 500\n",
    "gamma = 0.00008\n",
    "\n",
    "#w, loss = mean_squared_error_gd(y_tr, tx_tr, initial_w, max_iters, gamma)\n",
    "\n",
    "#print(\"end of the mean_squared_error_gd with w=\",w,\" and loss=\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3abc58ab-10d3-4a01-b62a-6e3445c65b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end of the least_squares with w= [ 2.90775824e-02 -2.58174342e-01 -2.58075231e-01 -3.15695998e-02\n",
      "  2.70148463e-01 -3.56244790e-02 -3.77596359e+02 -1.88764858e-01\n",
      "  1.26980572e-01  7.33360865e+01 -7.90497167e-04 -9.03464001e-04\n",
      "  7.22984805e+01 -8.65751516e-04  2.23848044e-03  1.25628556e-01\n",
      "  8.13634257e-04 -7.51316144e-02  6.61315259e-02  2.08838944e-01\n",
      " -1.01704341e-01 -6.91450673e-02  3.19799465e+02]  and loss= 0.4016547492014886\n"
     ]
    }
   ],
   "source": [
    "# test least square\n",
    "txOpti = dataClean(tx_tr)\n",
    "w, loss = least_squares(y_tr, txOpti)\n",
    "print(\"end of the least_squares with w=\",w,\" and loss=\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1f09a81-21c9-4587-ba44-9bb03ab90776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test mean squared error sgd\n",
    "\n",
    "initial_w = np.zeros(tx_tr.shape[1])\n",
    "max_iters = 100\n",
    "gamma = 0.00001\n",
    "\n",
    "#w, loss = mean_squared_error_sgd(y_tr, tx_tr, initial_w, max_iters, gamma)\n",
    "\n",
    "#print(\"end of the mean_squared_error_sgd with w=\",w,\" and loss=\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34c8ae3d-d431-4939-8e10-d0915a0fc194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of txOpti  (250000, 23)\n",
      "Current lambda=1e-07, the loss=0.4016563436270216\n",
      "Current lambda=1.0302040816326533e-05, the loss=0.40165637140640603\n",
      "Current lambda=2.0504081632653063e-05, the loss=0.4016564423195787\n",
      "Current lambda=3.07061224489796e-05, the loss=0.4016565470725474\n",
      "Current lambda=4.090816326530613e-05, the loss=0.40165667800708466\n",
      "Current lambda=5.111020408163266e-05, the loss=0.40165682906733735\n",
      "Current lambda=6.13122448979592e-05, the loss=0.40165699547523187\n",
      "Current lambda=7.151428571428572e-05, the loss=0.4016571734530141\n",
      "Current lambda=8.171632653061226e-05, the loss=0.40165736000674596\n",
      "Current lambda=9.19183673469388e-05, the loss=0.4016575527592416\n",
      "Current lambda=0.00010212040816326532, the loss=0.40165774982091546\n",
      "Current lambda=0.00011232244897959185, the loss=0.40165794968944485\n",
      "Current lambda=0.0001225244897959184, the loss=0.40165815117138287\n",
      "Current lambda=0.00013272653061224493, the loss=0.4016583533205778\n",
      "Current lambda=0.00014292857142857144, the loss=0.4016585553895404\n",
      "Current lambda=0.00015313061224489797, the loss=0.40165875679084\n",
      "Current lambda=0.0001633326530612245, the loss=0.4016589570663222\n",
      "Current lambda=0.00017353469387755105, the loss=0.40165915586245426\n",
      "Current lambda=0.00018373673469387758, the loss=0.40165935291049903\n",
      "Current lambda=0.0001939387755102041, the loss=0.4016595480105147\n",
      "Current lambda=0.00020414081632653063, the loss=0.4016597410183984\n",
      "Current lambda=0.00021434285714285717, the loss=0.401659931835361\n",
      "Current lambda=0.0002245448979591837, the loss=0.40166012039935334\n",
      "Current lambda=0.00023474693877551024, the loss=0.40166030667806785\n",
      "Current lambda=0.0002449489795918368, the loss=0.40166049066320914\n",
      "Current lambda=0.0002551510204081633, the loss=0.4016606723657988\n",
      "Current lambda=0.00026535306122448985, the loss=0.40166085181231925\n",
      "Current lambda=0.00027555510204081636, the loss=0.4016610290415424\n",
      "Current lambda=0.00028575714285714287, the loss=0.4016612041019227\n",
      "Current lambda=0.00029595918367346944, the loss=0.4016613770494485\n",
      "Current lambda=0.00030616122448979595, the loss=0.4016615479458734\n",
      "Current lambda=0.0003163632653061225, the loss=0.4016617168572607\n",
      "Current lambda=0.000326565306122449, the loss=0.40166188385278484\n",
      "Current lambda=0.00033676734693877553, the loss=0.40166204900374813\n",
      "Current lambda=0.0003469693877551021, the loss=0.4016622123827731\n",
      "Current lambda=0.0003571714285714286, the loss=0.4016623740631429\n",
      "Current lambda=0.00036737346938775517, the loss=0.40166253411826286\n",
      "Current lambda=0.0003775755102040817, the loss=0.40166269262122406\n",
      "Current lambda=0.0003877775510204082, the loss=0.40166284964445104\n",
      "Current lambda=0.00039797959183673475, the loss=0.4016630052594204\n",
      "Current lambda=0.00040818163265306126, the loss=0.4016631595364373\n",
      "Current lambda=0.0004183836734693878, the loss=0.40166331254445975\n",
      "Current lambda=0.00042858571428571433, the loss=0.4016634643509662\n",
      "Current lambda=0.00043878775510204084, the loss=0.4016636150218529\n",
      "Current lambda=0.0004489897959183674, the loss=0.40166376462136266\n",
      "Current lambda=0.0004591918367346939, the loss=0.4016639132120344\n",
      "Current lambda=0.0004693938775510205, the loss=0.40166406085467316\n",
      "Current lambda=0.000479595918367347, the loss=0.401664207608334\n",
      "Current lambda=0.0004897979591836735, the loss=0.4016643535303215\n",
      "Current lambda=0.0005, the loss=0.40166449867619763\n",
      "best lambda is  1e-07\n",
      "end of the ridge_regression with w= [ 0.02907017 -0.25818776 -0.25807243 -0.03156359  0.27015066 -0.03562999\n",
      " -0.0023772  -0.188761    0.12697698  0.19676287 -0.00079167 -0.00090381\n",
      "  0.29206947 -0.00086357  0.00223468  0.12563208  0.00081575 -0.07511911\n",
      "  0.0661325   0.20866507 -0.10087154 -0.06981304 -0.06369628]  and loss= 0.4016563436270216\n"
     ]
    }
   ],
   "source": [
    "# test ridge regression \n",
    "\n",
    "txOpti = dataClean(tx_tr)\n",
    "print(\"shape of txOpti \", txOpti.shape)\n",
    "\n",
    "lambda_ = ridge_regression_best_lambda(y_tr, txOpti)\n",
    "print(\"best lambda is \",lambda_)\n",
    "w, loss = ridge_regression(y_tr, txOpti, lambda_)\n",
    "\n",
    "#y_pred = predict(w, tx_te)\n",
    "#y_pred[y_pred==0] = -1\n",
    "\n",
    "#OUTPUT_PATH = 'sample-submission'\n",
    "#create_csv_submission(ids_te, y_pred, OUTPUT_PATH)\n",
    "\n",
    "print(\"end of the ridge_regression with w=\",w,\" and loss=\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5f5c7de-0c97-4975-93cb-71f4916eb96e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of txOpti  (250000, 23)\n",
      "Preiteration, the loss=1.0624754799064415, the grad=-0.0572895281497229\n",
      "Current iteration=0, the loss=1.062317401733531, the grad=0.6943905988992103\n",
      "Current iteration=1, the loss=1.062159385930291, the grad=0.69397148489038\n",
      "Current iteration=2, the loss=1.0620014325047544, the grad=0.693552790847094\n",
      "Current iteration=3, the loss=1.0618435414647618, the grad=0.6931345166924245\n",
      "Current iteration=4, the loss=1.0616857128179562, the grad=0.6927166623474981\n",
      "Current iteration=5, the loss=1.0615279465717868, the grad=0.6922992277315012\n",
      "Current iteration=6, the loss=1.061370242733509, the grad=0.6918822127616887\n",
      "Current iteration=7, the loss=1.061212601310186, the grad=0.6914656173533885\n",
      "Current iteration=8, the loss=1.061055022308688, the grad=0.6910494414200097\n",
      "Current iteration=9, the loss=1.060897505735696, the grad=0.6906336848730477\n",
      "Current iteration=10, the loss=1.0607400515976986, the grad=0.6902183476220917\n",
      "Current iteration=11, the loss=1.060582659900995, the grad=0.6898034295748311\n",
      "Current iteration=12, the loss=1.060425330651698, the grad=0.6893889306370623\n",
      "Current iteration=13, the loss=1.0602680638557305, the grad=0.6889748507126949\n",
      "Current iteration=14, the loss=1.060110859518828, the grad=0.6885611897037588\n",
      "Current iteration=15, the loss=1.0599537176465421, the grad=0.6881479475104102\n",
      "Current iteration=16, the loss=1.0597966382442356, the grad=0.6877351240309395\n",
      "Current iteration=17, the loss=1.05963962131709, the grad=0.6873227191617768\n",
      "Current iteration=18, the loss=1.059482666870101, the grad=0.686910732797499\n",
      "Current iteration=19, the loss=1.059325774908081, the grad=0.6864991648308367\n",
      "Current iteration=20, the loss=1.0591689454356623, the grad=0.6860880151526807\n",
      "Current iteration=21, the loss=1.0590121784572935, the grad=0.685677283652089\n",
      "Current iteration=22, the loss=1.058855473977243, the grad=0.6852669702162935\n",
      "Current iteration=23, the loss=1.0586988319996, the grad=0.6848570747307063\n",
      "Current iteration=24, the loss=1.058542252528274, the grad=0.6844475970789273\n",
      "Current iteration=25, the loss=1.0583857355669966, the grad=0.6840385371427504\n",
      "Current iteration=26, the loss=1.0582292811193221, the grad=0.6836298948021705\n",
      "Current iteration=27, the loss=1.0580728891886269, the grad=0.6832216699353899\n",
      "Current iteration=28, the loss=1.0579165597781126, the grad=0.6828138624188265\n",
      "Current iteration=29, the loss=1.057760292890805, the grad=0.682406472127119\n",
      "Current iteration=30, the loss=1.0576040885295555, the grad=0.6819994989331342\n",
      "Current iteration=31, the loss=1.057447946697042, the grad=0.6815929427079749\n",
      "Current iteration=32, the loss=1.057291867395769, the grad=0.6811868033209852\n",
      "Current iteration=33, the loss=1.0571358506280697, the grad=0.6807810806397584\n",
      "Current iteration=34, the loss=1.056979896396105, the grad=0.6803757745301441\n",
      "Current iteration=35, the loss=1.0568240047018658, the grad=0.6799708848562539\n",
      "Current iteration=36, the loss=1.0566681755471732, the grad=0.6795664114804699\n",
      "Current iteration=37, the loss=1.0565124089336786, the grad=0.6791623542634498\n",
      "Current iteration=38, the loss=1.0563567048628648, the grad=0.6787587130641356\n",
      "Current iteration=39, the loss=1.0562010633360484, the grad=0.6783554877397594\n",
      "Current iteration=40, the loss=1.056045484354377, the grad=0.6779526781458513\n",
      "Current iteration=41, the loss=1.0558899679188338, the grad=0.6775502841362443\n",
      "Current iteration=42, the loss=1.0557345140302363, the grad=0.6771483055630847\n",
      "Current iteration=43, the loss=1.0555791226892364, the grad=0.6767467422768352\n",
      "Current iteration=44, the loss=1.055423793896323, the grad=0.6763455941262853\n",
      "Current iteration=45, the loss=1.055268527651821, the grad=0.6759448609585557\n",
      "Current iteration=46, the loss=1.0551133239558939, the grad=0.6755445426191067\n",
      "Current iteration=47, the loss=1.0549581828085417, the grad=0.6751446389517447\n",
      "Current iteration=48, the loss=1.0548031042096053, the grad=0.67474514979863\n",
      "Current iteration=49, the loss=1.0546480881587643, the grad=0.674346075000282\n",
      "Current iteration=50, the loss=1.0544931346555386, the grad=0.673947414395588\n",
      "Current iteration=51, the loss=1.0543382436992899, the grad=0.6735491678218102\n",
      "Current iteration=52, the loss=1.0541834152892204, the grad=0.6731513351145907\n",
      "Current iteration=53, the loss=1.0540286494243762, the grad=0.6727539161079609\n",
      "Current iteration=54, the loss=1.0538739461036462, the grad=0.6723569106343469\n",
      "Current iteration=55, the loss=1.053719305325763, the grad=0.6719603185245778\n",
      "Current iteration=56, the loss=1.0535647270893043, the grad=0.6715641396078923\n",
      "Current iteration=57, the loss=1.0534102113926929, the grad=0.6711683737119448\n",
      "Current iteration=58, the loss=1.053255758234197, the grad=0.6707730206628137\n",
      "Current iteration=59, the loss=1.053101367611933, the grad=0.6703780802850078\n",
      "Current iteration=60, the loss=1.0529470395238631, the grad=0.6699835524014738\n",
      "Current iteration=61, the loss=1.0527927739678, the grad=0.6695894368336026\n",
      "Current iteration=62, the loss=1.0526385709414017, the grad=0.6691957334012375\n",
      "Current iteration=63, the loss=1.052484430442179, the grad=0.6688024419226799\n",
      "Current iteration=64, the loss=1.0523303524674912, the grad=0.6684095622146982\n",
      "Current iteration=65, the loss=1.0521763370145494, the grad=0.6680170940925321\n",
      "Current iteration=66, the loss=1.0520223840804144, the grad=0.667625037369903\n",
      "Current iteration=67, the loss=1.0518684936620017, the grad=0.6672333918590178\n",
      "Current iteration=68, the loss=1.0517146657560785, the grad=0.666842157370579\n",
      "Current iteration=69, the loss=1.051560900359265, the grad=0.6664513337137895\n",
      "Current iteration=70, the loss=1.051407197468036, the grad=0.6660609206963609\n",
      "Current iteration=71, the loss=1.0512535570787216, the grad=0.6656709181245196\n",
      "Current iteration=72, the loss=1.0510999791875077, the grad=0.6652813258030149\n",
      "Current iteration=73, the loss=1.050946463790436, the grad=0.6648921435351259\n",
      "Current iteration=74, the loss=1.050793010883404, the grad=0.6645033711226678\n",
      "Current iteration=75, the loss=1.0506396204621684, the grad=0.6641150083659996\n",
      "Current iteration=76, the loss=1.050486292522344, the grad=0.6637270550640311\n",
      "Current iteration=77, the loss=1.0503330270594031, the grad=0.6633395110142297\n",
      "Current iteration=78, the loss=1.0501798240686786, the grad=0.6629523760126281\n",
      "Current iteration=79, the loss=1.0500266835453629, the grad=0.6625656498538307\n",
      "Current iteration=80, the loss=1.04987360548451, the grad=0.662179332331021\n",
      "Current iteration=81, the loss=1.0497205898810336, the grad=0.6617934232359687\n",
      "Current iteration=82, the loss=1.0495676367297118, the grad=0.661407922359036\n",
      "Current iteration=83, the loss=1.049414746025184, the grad=0.6610228294891863\n",
      "Current iteration=84, the loss=1.0492619177619524, the grad=0.6606381444139898\n",
      "Current iteration=85, the loss=1.0491091519343845, the grad=0.660253866919631\n",
      "Current iteration=86, the loss=1.0489564485367113, the grad=0.6598699967909157\n",
      "Current iteration=87, the loss=1.0488038075630293, the grad=0.6594865338112783\n",
      "Current iteration=88, the loss=1.0486512290073011, the grad=0.6591034777627888\n",
      "Current iteration=89, the loss=1.0484987128633558, the grad=0.6587208284261596\n",
      "Current iteration=90, the loss=1.048346259124889, the grad=0.6583385855807524\n",
      "Current iteration=91, the loss=1.048193867785464, the grad=0.6579567490045864\n",
      "Current iteration=92, the loss=1.0480415388385136, the grad=0.6575753184743431\n",
      "Current iteration=93, the loss=1.047889272277338, the grad=0.6571942937653759\n",
      "Current iteration=94, the loss=1.047737068095107, the grad=0.6568136746517154\n",
      "Current iteration=95, the loss=1.0475849262848618, the grad=0.6564334609060765\n",
      "Current iteration=96, the loss=1.0474328468395133, the grad=0.6560536522998667\n",
      "Current iteration=97, the loss=1.0472808297518432, the grad=0.6556742486031913\n",
      "Current iteration=98, the loss=1.0471288750145065, the grad=0.6552952495848626\n",
      "Current iteration=99, the loss=1.04697698262003, the grad=0.6549166550124043\n",
      "Current iteration=100, the loss=1.046825152560814, the grad=0.6545384646520601\n",
      "Current iteration=101, the loss=1.046673384829131, the grad=0.6541606782688015\n",
      "Current iteration=102, the loss=1.0465216794171295, the grad=0.6537832956263325\n",
      "Current iteration=103, the loss=1.046370036316833, the grad=0.6534063164870983\n",
      "Current iteration=104, the loss=1.046218455520139, the grad=0.6530297406122912\n",
      "Current iteration=105, the loss=1.046066937018822, the grad=0.6526535677618595\n",
      "Current iteration=106, the loss=1.045915480804533, the grad=0.6522777976945111\n",
      "Current iteration=107, the loss=1.0457640868688, the grad=0.6519024301677239\n",
      "Current iteration=108, the loss=1.0456127552030288, the grad=0.6515274649377509\n",
      "Current iteration=109, the loss=1.045461485798504, the grad=0.651152901759627\n",
      "Current iteration=110, the loss=1.0453102786463881, the grad=0.6507787403871771\n",
      "Current iteration=111, the loss=1.0451591337377246, the grad=0.6504049805730213\n",
      "Current iteration=112, the loss=1.0450080510634356, the grad=0.6500316220685843\n",
      "Current iteration=113, the loss=1.0448570306143246, the grad=0.6496586646240994\n",
      "Current iteration=114, the loss=1.044706072381076, the grad=0.6492861079886177\n",
      "Current iteration=115, the loss=1.0445551763542558, the grad=0.6489139519100139\n",
      "Current iteration=116, the loss=1.0444043425243135, the grad=0.6485421961349932\n",
      "Current iteration=117, the loss=1.044253570881579, the grad=0.6481708404090986\n",
      "Current iteration=118, the loss=1.044102861416268, the grad=0.6477998844767173\n",
      "Current iteration=119, the loss=1.0439522141184794, the grad=0.6474293280810876\n",
      "Current iteration=120, the loss=1.0438016289781957, the grad=0.6470591709643065\n",
      "Current iteration=121, the loss=1.0436511059852858, the grad=0.6466894128673354\n",
      "Current iteration=122, the loss=1.043500645129503, the grad=0.6463200535300079\n",
      "Current iteration=123, the loss=1.0433502464004878, the grad=0.6459510926910353\n",
      "Current iteration=124, the loss=1.0431999097877664, the grad=0.6455825300880149\n",
      "Current iteration=125, the loss=1.0430496352807526, the grad=0.6452143654574366\n",
      "Current iteration=126, the loss=1.042899422868748, the grad=0.6448465985346883\n",
      "Current iteration=127, the loss=1.0427492725409426, the grad=0.6444792290540636\n",
      "Current iteration=128, the loss=1.0425991842864153, the grad=0.6441122567487696\n",
      "Current iteration=129, the loss=1.0424491580941337, the grad=0.6437456813509312\n",
      "Current iteration=130, the loss=1.0422991939529556, the grad=0.6433795025916009\n",
      "Current iteration=131, the loss=1.0421492918516295, the grad=0.643013720200762\n",
      "Current iteration=132, the loss=1.0419994517787943, the grad=0.6426483339073381\n",
      "Current iteration=133, the loss=1.0418496737229803, the grad=0.6422833434391992\n",
      "Current iteration=134, the loss=1.0416999576726103, the grad=0.6419187485231677\n",
      "Current iteration=135, the loss=1.0415503036159985, the grad=0.6415545488850255\n",
      "Current iteration=136, the loss=1.041400711541354, the grad=0.6411907442495202\n",
      "Current iteration=137, the loss=1.0412511814367769, the grad=0.6408273343403729\n",
      "Current iteration=138, the loss=1.0411017132902631, the grad=0.6404643188802833\n",
      "Current iteration=139, the loss=1.0409523070897022, the grad=0.6401016975909379\n",
      "Current iteration=140, the loss=1.0408029628228792, the grad=0.6397394701930158\n",
      "Current iteration=141, the loss=1.0406536804774746, the grad=0.6393776364061946\n",
      "Current iteration=142, the loss=1.0405044600410636, the grad=0.6390161959491585\n",
      "Current iteration=143, the loss=1.04035530150112, the grad=0.6386551485396036\n",
      "Current iteration=144, the loss=1.040206204845013, the grad=0.6382944938942456\n",
      "Current iteration=145, the loss=1.0400571700600103, the grad=0.6379342317288257\n",
      "Current iteration=146, the loss=1.039908197133276, the grad=0.6375743617581165\n",
      "Current iteration=147, the loss=1.0397592860518752, the grad=0.6372148836959298\n",
      "Current iteration=148, the loss=1.039610436802769, the grad=0.6368557972551224\n",
      "Current iteration=149, the loss=1.0394616493728206, the grad=0.6364971021476031\n",
      "Current iteration=150, the loss=1.0393129237487917, the grad=0.6361387980843384\n",
      "Current iteration=151, the loss=1.039164259917345, the grad=0.6357808847753592\n",
      "Current iteration=152, the loss=1.039015657865042, the grad=0.635423361929768\n",
      "Current iteration=153, the loss=1.0388671175783486, the grad=0.6350662292557446\n",
      "Current iteration=154, the loss=1.0387186390436312, the grad=0.6347094864605526\n",
      "Current iteration=155, the loss=1.0385702222471584, the grad=0.6343531332505463\n",
      "Current iteration=156, the loss=1.0384218671751015, the grad=0.6339971693311762\n",
      "Current iteration=157, the loss=1.0382735738135351, the grad=0.6336415944069965\n",
      "Current iteration=158, the loss=1.0381253421484378, the grad=0.6332864081816706\n",
      "Current iteration=159, the loss=1.0379771721656923, the grad=0.6329316103579778\n",
      "Current iteration=160, the loss=1.0378290638510852, the grad=0.6325772006378199\n",
      "Current iteration=161, the loss=1.0376810171903097, the grad=0.6322231787222266\n",
      "Current iteration=162, the loss=1.0375330321689629, the grad=0.6318695443113632\n",
      "Current iteration=163, the loss=1.0373851087725494, the grad=0.6315162971045357\n",
      "Current iteration=164, the loss=1.0372372469864792, the grad=0.6311634368001975\n",
      "Current iteration=165, the loss=1.037089446796069, the grad=0.6308109630959563\n",
      "Current iteration=166, the loss=1.0369417081865449, the grad=0.6304588756885788\n",
      "Current iteration=167, the loss=1.0367940311430377, the grad=0.6301071742739988\n",
      "Current iteration=168, the loss=1.0366464156505895, the grad=0.6297558585473217\n",
      "Current iteration=169, the loss=1.0364988616941493, the grad=0.6294049282028324\n",
      "Current iteration=170, the loss=1.0363513692585755, the grad=0.6290543829339996\n",
      "Current iteration=171, the loss=1.0362039383286368, the grad=0.6287042224334838\n",
      "Current iteration=172, the loss=1.036056568889011, the grad=0.6283544463931423\n",
      "Current iteration=173, the loss=1.0359092609242877, the grad=0.6280050545040354\n",
      "Current iteration=174, the loss=1.0357620144189656, the grad=0.6276560464564334\n",
      "Current iteration=175, the loss=1.0356148293574563, the grad=0.6273074219398217\n",
      "Current iteration=176, the loss=1.0354677057240824, the grad=0.6269591806429072\n",
      "Current iteration=177, the loss=1.0353206435030793, the grad=0.6266113222536248\n",
      "Current iteration=178, the loss=1.0351736426785947, the grad=0.6262638464591427\n",
      "Current iteration=179, the loss=1.0350267032346887, the grad=0.6259167529458695\n",
      "Current iteration=180, the loss=1.034879825155336, the grad=0.6255700413994584\n",
      "Current iteration=181, the loss=1.034733008424425, the grad=0.625223711504816\n",
      "Current iteration=182, the loss=1.0345862530257581, the grad=0.6248777629461053\n",
      "Current iteration=183, the loss=1.0344395589430528, the grad=0.6245321954067541\n",
      "Current iteration=184, the loss=1.0342929261599412, the grad=0.6241870085694592\n",
      "Current iteration=185, the loss=1.0341463546599714, the grad=0.6238422021161931\n",
      "Current iteration=186, the loss=1.0339998444266079, the grad=0.6234977757282099\n",
      "Current iteration=187, the loss=1.033853395443231, the grad=0.6231537290860518\n",
      "Current iteration=188, the loss=1.0337070076931383, the grad=0.6228100618695537\n",
      "Current iteration=189, the loss=1.0335606811595437, the grad=0.6224667737578495\n",
      "Current iteration=190, the loss=1.0334144158255805, the grad=0.6221238644293792\n",
      "Current iteration=191, the loss=1.0332682116742982, the grad=0.6217813335618926\n",
      "Current iteration=192, the loss=1.033122068688666, the grad=0.621439180832457\n",
      "Current iteration=193, the loss=1.0329759868515715, the grad=0.621097405917462\n",
      "Current iteration=194, the loss=1.0328299661458216, the grad=0.6207560084926254\n",
      "Current iteration=195, the loss=1.032684006554143, the grad=0.6204149882329988\n",
      "Current iteration=196, the loss=1.0325381080591822, the grad=0.6200743448129746\n",
      "Current iteration=197, the loss=1.0323922706435062, the grad=0.6197340779062898\n",
      "Current iteration=198, the loss=1.0322464942896032, the grad=0.619394187186033\n",
      "Current iteration=199, the loss=1.0321007789798828, the grad=0.6190546723246497\n",
      "Current iteration=200, the loss=1.0319551246966752, the grad=0.6187155329939481\n",
      "Current iteration=201, the loss=1.0318095314222337, the grad=0.6183767688651047\n",
      "Current iteration=202, the loss=1.0316639991387333, the grad=0.6180383796086698\n",
      "Current iteration=203, the loss=1.0315185278282721, the grad=0.6177003648945734\n",
      "Current iteration=204, the loss=1.0313731174728717, the grad=0.6173627243921304\n",
      "Current iteration=205, the loss=1.031227768054477, the grad=0.6170254577700464\n",
      "Current iteration=206, the loss=1.0310824795549562, the grad=0.6166885646964237\n",
      "Current iteration=207, the loss=1.030937251956103, the grad=0.616352044838766\n",
      "Current iteration=208, the loss=1.030792085239635, the grad=0.6160158978639844\n",
      "Current iteration=209, the loss=1.030646979387195, the grad=0.6156801234384031\n",
      "Current iteration=210, the loss=1.0305019343803512, the grad=0.6153447212277643\n",
      "Current iteration=211, the loss=1.0303569502005983, the grad=0.6150096908972345\n",
      "Current iteration=212, the loss=1.0302120268293562, the grad=0.614675032111409\n",
      "Current iteration=213, the loss=1.0300671642479722, the grad=0.6143407445343181\n",
      "Current iteration=214, the loss=1.0299223624377192, the grad=0.6140068278294317\n",
      "Current iteration=215, the loss=1.0297776213797998, the grad=0.6136732816596662\n",
      "Current iteration=216, the loss=1.0296329410553415, the grad=0.6133401056873877\n",
      "Current iteration=217, the loss=1.029488321445402, the grad=0.6130072995744198\n",
      "Current iteration=218, the loss=1.0293437625309667, the grad=0.6126748629820467\n",
      "Current iteration=219, the loss=1.0291992642929488, the grad=0.6123427955710197\n",
      "Current iteration=220, the loss=1.0290548267121917, the grad=0.6120110970015626\n",
      "Current iteration=221, the loss=1.028910449769468, the grad=0.6116797669333767\n",
      "Current iteration=222, the loss=1.0287661334454805, the grad=0.6113488050256456\n",
      "Current iteration=223, the loss=1.0286218777208613, the grad=0.6110182109370415\n",
      "Current iteration=224, the loss=1.0284776825761734, the grad=0.6106879843257292\n",
      "Current iteration=225, the loss=1.0283335479919105, the grad=0.6103581248493725\n",
      "Current iteration=226, the loss=1.028189473948499, the grad=0.6100286321651386\n",
      "Current iteration=227, the loss=1.028045460426294, the grad=0.609699505929703\n",
      "Current iteration=228, the loss=1.0279015074055855, the grad=0.609370745799256\n",
      "Current iteration=229, the loss=1.0277576148665937, the grad=0.6090423514295065\n",
      "Current iteration=230, the loss=1.0276137827894725, the grad=0.6087143224756874\n",
      "Current iteration=231, the loss=1.0274700111543082, the grad=0.6083866585925614\n",
      "Current iteration=232, the loss=1.0273262999411212, the grad=0.6080593594344248\n",
      "Current iteration=233, the loss=1.0271826491298641, the grad=0.6077324246551139\n",
      "Current iteration=234, the loss=1.0270390587004248, the grad=0.6074058539080093\n",
      "Current iteration=235, the loss=1.026895528632625, the grad=0.6070796468460408\n",
      "Current iteration=236, the loss=1.0267520589062207, the grad=0.6067538031216926\n",
      "Current iteration=237, the loss=1.026608649500904, the grad=0.6064283223870088\n",
      "Current iteration=238, the loss=1.0264653003963007, the grad=0.6061032042935973\n",
      "Current iteration=239, the loss=1.0263220115719738, the grad=0.6057784484926357\n",
      "Current iteration=240, the loss=1.026178783007421, the grad=0.6054540546348756\n",
      "Current iteration=241, the loss=1.026035614682077, the grad=0.6051300223706475\n",
      "Current iteration=242, the loss=1.0258925065753135, the grad=0.6048063513498665\n",
      "Current iteration=243, the loss=1.025749458666438, the grad=0.6044830412220358\n",
      "Current iteration=244, the loss=1.025606470934696, the grad=0.604160091636253\n",
      "Current iteration=245, the loss=1.0254635433592707, the grad=0.6038375022412135\n",
      "Current iteration=246, the loss=1.0253206759192832, the grad=0.603515272685217\n",
      "Current iteration=247, the loss=1.0251778685937925, the grad=0.6031934026161698\n",
      "Current iteration=248, the loss=1.025035121361796, the grad=0.6028718916815929\n",
      "Current iteration=249, the loss=1.024892434202231, the grad=0.6025507395286231\n",
      "Current iteration=250, the loss=1.0247498070939722, the grad=0.602229945804021\n",
      "Current iteration=251, the loss=1.024607240015836, the grad=0.6019095101541733\n",
      "Current iteration=252, the loss=1.024464732946577, the grad=0.6015894322250982\n",
      "Current iteration=253, the loss=1.0243222858648904, the grad=0.6012697116624514\n",
      "Current iteration=254, the loss=1.024179898749412, the grad=0.6009503481115286\n",
      "Current iteration=255, the loss=1.024037571578718, the grad=0.6006313412172716\n",
      "Current iteration=256, the loss=1.0238953043313261, the grad=0.6003126906242725\n",
      "Current iteration=257, the loss=1.0237530969856954, the grad=0.5999943959767782\n",
      "Current iteration=258, the loss=1.0236109495202261, the grad=0.5996764569186941\n",
      "Current iteration=259, the loss=1.0234688619132608, the grad=0.5993588730935913\n",
      "Current iteration=260, the loss=1.0233268341430843, the grad=0.599041644144708\n",
      "Current iteration=261, the loss=1.023184866187924, the grad=0.5987247697149557\n",
      "Current iteration=262, the loss=1.02304295802595, the grad=0.5984082494469238\n",
      "Current iteration=263, the loss=1.0229011096352758, the grad=0.5980920829828833\n",
      "Current iteration=264, the loss=1.0227593209939583, the grad=0.5977762699647917\n",
      "Current iteration=265, the loss=1.022617592079998, the grad=0.5974608100342973\n",
      "Current iteration=266, the loss=1.0224759228713398, the grad=0.5971457028327436\n",
      "Current iteration=267, the loss=1.022334313345873, the grad=0.5968309480011739\n",
      "Current iteration=268, the loss=1.022192763481431, the grad=0.5965165451803356\n",
      "Current iteration=269, the loss=1.022051273255793, the grad=0.5962024940106843\n",
      "Current iteration=270, the loss=1.0219098426466826, the grad=0.5958887941323883\n",
      "Current iteration=271, the loss=1.0217684716317694, the grad=0.5955754451853326\n",
      "Current iteration=272, the loss=1.0216271601886684, the grad=0.5952624468091245\n",
      "Current iteration=273, the loss=1.0214859082949415, the grad=0.594949798643096\n",
      "Current iteration=274, the loss=1.0213447159280964, the grad=0.5946375003263096\n",
      "Current iteration=275, the loss=1.021203583065587, the grad=0.5943255514975615\n",
      "Current iteration=276, the loss=1.021062509684816, the grad=0.5940139517953862\n",
      "Current iteration=277, the loss=1.020921495763131, the grad=0.5937027008580608\n",
      "Current iteration=278, the loss=1.0207805412778284, the grad=0.5933917983236096\n",
      "Current iteration=279, the loss=1.0206396462061522, the grad=0.5930812438298069\n",
      "Current iteration=280, the loss=1.0204988105252948, the grad=0.5927710370141828\n",
      "Current iteration=281, the loss=1.0203580342123966, the grad=0.5924611775140262\n",
      "Current iteration=282, the loss=1.0202173172445466, the grad=0.592151664966389\n",
      "Current iteration=283, the loss=1.020076659598783, the grad=0.5918424990080914\n",
      "Current iteration=284, the loss=1.0199360612520927, the grad=0.5915336792757236\n",
      "Current iteration=285, the loss=1.0197955221814123, the grad=0.5912252054056528\n",
      "Current iteration=286, the loss=1.0196550423636286, the grad=0.5909170770340244\n",
      "Current iteration=287, the loss=1.019514621775578, the grad=0.5906092937967681\n",
      "Current iteration=288, the loss=1.0193742603940465, the grad=0.5903018553296004\n",
      "Current iteration=289, the loss=1.0192339581957721, the grad=0.5899947612680303\n",
      "Current iteration=290, the loss=1.0190937151574428, the grad=0.5896880112473613\n",
      "Current iteration=291, the loss=1.0189535312556972, the grad=0.5893816049026964\n",
      "Current iteration=292, the loss=1.0188134064671257, the grad=0.5890755418689425\n",
      "Current iteration=293, the loss=1.018673340768271, the grad=0.5887698217808129\n",
      "Current iteration=294, the loss=1.0185333341356266, the grad=0.5884644442728327\n",
      "Current iteration=295, the loss=1.0183933865456385, the grad=0.5881594089793413\n",
      "Current iteration=296, the loss=1.018253497974706, the grad=0.5878547155344971\n",
      "Current iteration=297, the loss=1.0181136683991792, the grad=0.5875503635722813\n",
      "Current iteration=298, the loss=1.0179738977953623, the grad=0.5872463527265009\n",
      "Current iteration=299, the loss=1.0178341861395128, the grad=0.586942682630794\n",
      "Current iteration=300, the loss=1.0176945334078416, the grad=0.5866393529186319\n",
      "Current iteration=301, the loss=1.0175549395765122, the grad=0.5863363632233236\n",
      "Current iteration=302, the loss=1.0174154046216437, the grad=0.5860337131780203\n",
      "Current iteration=303, the loss=1.0172759285193083, the grad=0.5857314024157176\n",
      "Current iteration=304, the loss=1.0171365112455326, the grad=0.5854294305692601\n",
      "Current iteration=305, the loss=1.016997152776299, the grad=0.5851277972713451\n",
      "Current iteration=306, the loss=1.0168578530875432, the grad=0.5848265021545257\n",
      "Current iteration=307, the loss=1.0167186121551572, the grad=0.5845255448512154\n",
      "Current iteration=308, the loss=1.0165794299549888, the grad=0.5842249249936909\n",
      "Current iteration=309, the loss=1.0164403064628404, the grad=0.5839246422140957\n",
      "Current iteration=310, the loss=1.0163012416544714, the grad=0.583624696144444\n",
      "Current iteration=311, the loss=1.0161622355055957, the grad=0.5833250864166246\n",
      "Current iteration=312, the loss=1.0160232879918865, the grad=0.5830258126624035\n",
      "Current iteration=313, the loss=1.0158843990889703, the grad=0.5827268745134283\n",
      "Current iteration=314, the loss=1.0157455687724335, the grad=0.5824282716012313\n",
      "Current iteration=315, the loss=1.0156067970178175, the grad=0.5821300035572333\n",
      "Current iteration=316, the loss=1.0154680838006227, the grad=0.5818320700127464\n",
      "Current iteration=317, the loss=1.0153294290963062, the grad=0.5815344705989784\n",
      "Current iteration=318, the loss=1.015190832880283, the grad=0.5812372049470355\n",
      "Current iteration=319, the loss=1.0150522951279266, the grad=0.5809402726879256\n",
      "Current iteration=320, the loss=1.0149138158145687, the grad=0.580643673452563\n",
      "Current iteration=321, the loss=1.0147753949155, the grad=0.5803474068717703\n",
      "Current iteration=322, the loss=1.014637032405969, the grad=0.5800514725762825\n",
      "Current iteration=323, the loss=1.0144987282611846, the grad=0.5797558701967501\n",
      "Current iteration=324, the loss=1.0143604824563142, the grad=0.5794605993637424\n",
      "Current iteration=325, the loss=1.014222294966484, the grad=0.5791656597077517\n",
      "Current iteration=326, the loss=1.014084165766782, the grad=0.5788710508591945\n",
      "Current iteration=327, the loss=1.0139460948322545, the grad=0.5785767724484177\n",
      "Current iteration=328, the loss=1.0138080821379087, the grad=0.5782828241056991\n",
      "Current iteration=329, the loss=1.0136701276587121, the grad=0.577989205461252\n",
      "Current iteration=330, the loss=1.0135322313695927, the grad=0.5776959161452294\n",
      "Current iteration=331, the loss=1.01339439324544, the grad=0.5774029557877245\n",
      "Current iteration=332, the loss=1.013256613261104, the grad=0.5771103240187767\n",
      "Current iteration=333, the loss=1.0131188913913964, the grad=0.5768180204683728\n",
      "Current iteration=334, the loss=1.0129812276110906, the grad=0.5765260447664515\n",
      "Current iteration=335, the loss=1.0128436218949215, the grad=0.5762343965429054\n",
      "Current iteration=336, the loss=1.0127060742175855, the grad=0.5759430754275853\n",
      "Current iteration=337, the loss=1.012568584553743, the grad=0.5756520810503022\n",
      "Current iteration=338, the loss=1.012431152878016, the grad=0.5753614130408313\n",
      "Current iteration=339, the loss=1.0122937791649878, the grad=0.5750710710289143\n",
      "Current iteration=340, the loss=1.012156463389207, the grad=0.574781054644263\n",
      "Current iteration=341, the loss=1.0120192055251833, the grad=0.5744913635165624\n",
      "Current iteration=342, the loss=1.0118820055473912, the grad=0.5742019972754733\n",
      "Current iteration=343, the loss=1.0117448634302681, the grad=0.573912955550635\n",
      "Current iteration=344, the loss=1.011607779148215, the grad=0.57362423797167\n",
      "Current iteration=345, the loss=1.0114707526755977, the grad=0.5733358441681844\n",
      "Current iteration=346, the loss=1.0113337839867453, the grad=0.5730477737697734\n",
      "Current iteration=347, the loss=1.0111968730559515, the grad=0.5727600264060225\n",
      "Current iteration=348, the loss=1.0110600198574757, the grad=0.5724726017065113\n",
      "Current iteration=349, the loss=1.0109232243655406, the grad=0.5721854993008153\n",
      "Current iteration=350, the loss=1.010786486554336, the grad=0.5718987188185107\n",
      "Current iteration=351, the loss=1.0106498063980138, the grad=0.5716122598891757\n",
      "Current iteration=352, the loss=1.0105131838706942, the grad=0.5713261221423935\n",
      "Current iteration=353, the loss=1.0103766189464622, the grad=0.5710403052077562\n",
      "Current iteration=354, the loss=1.010240111599369, the grad=0.5707548087148661\n",
      "Current iteration=355, the loss=1.010103661803431, the grad=0.5704696322933401\n",
      "Current iteration=356, the loss=1.009967269532632, the grad=0.5701847755728112\n",
      "Current iteration=357, the loss=1.0098309347609218, the grad=0.5699002381829321\n",
      "Current iteration=358, the loss=1.009694657462216, the grad=0.5696160197533776\n",
      "Current iteration=359, the loss=1.0095584376103997, the grad=0.5693321199138467\n",
      "Current iteration=360, the loss=1.0094222751793218, the grad=0.5690485382940671\n",
      "Current iteration=361, the loss=1.0092861701428015, the grad=0.5687652745237965\n",
      "Current iteration=362, the loss=1.0091501224746238, the grad=0.5684823282328252\n",
      "Current iteration=363, the loss=1.0090141321485417, the grad=0.5681996990509799\n",
      "Current iteration=364, the loss=1.008878199138277, the grad=0.5679173866081254\n",
      "Current iteration=365, the loss=1.008742323417519, the grad=0.5676353905341671\n",
      "Current iteration=366, the loss=1.0086065049599244, the grad=0.5673537104590549\n",
      "Current iteration=367, the loss=1.00847074373912, the grad=0.5670723460127846\n",
      "Current iteration=368, the loss=1.0083350397287008, the grad=0.5667912968254006\n",
      "Current iteration=369, the loss=1.0081993929022304, the grad=0.5665105625269995\n",
      "Current iteration=370, the loss=1.0080638032332416, the grad=0.5662301427477314\n",
      "Current iteration=371, the loss=1.0079282706952368, the grad=0.5659500371178035\n",
      "Current iteration=372, the loss=1.007792795261688, the grad=0.5656702452674813\n",
      "Current iteration=373, the loss=1.0076573769060362, the grad=0.5653907668270937\n",
      "Current iteration=374, the loss=1.007522015601693, the grad=0.5651116014270318\n",
      "Current iteration=375, the loss=1.0073867113220396, the grad=0.5648327486977543\n",
      "Current iteration=376, the loss=1.0072514640404282, the grad=0.56455420826979\n",
      "Current iteration=377, the loss=1.0071162737301806, the grad=0.564275979773738\n",
      "Current iteration=378, the loss=1.0069811403645899, the grad=0.5639980628402723\n",
      "Current iteration=379, the loss=1.0068460639169194, the grad=0.5637204571001427\n",
      "Current iteration=380, the loss=1.0067110443604037, the grad=0.5634431621841793\n",
      "Current iteration=381, the loss=1.0065760816682494, the grad=0.5631661777232919\n",
      "Current iteration=382, the loss=1.0064411758136336, the grad=0.5628895033484757\n",
      "Current iteration=383, the loss=1.0063063267697048, the grad=0.562613138690811\n",
      "Current iteration=384, the loss=1.0061715345095836, the grad=0.5623370833814667\n",
      "Current iteration=385, the loss=1.006036799006364, the grad=0.5620613370517034\n",
      "Current iteration=386, the loss=1.0059021202331089, the grad=0.5617858993328738\n",
      "Current iteration=387, the loss=1.0057674981628568, the grad=0.5615107698564269\n",
      "Current iteration=388, the loss=1.0056329327686166, the grad=0.5612359482539094\n",
      "Current iteration=389, the loss=1.005498424023372, the grad=0.560961434156968\n",
      "Current iteration=390, the loss=1.0053639719000762, the grad=0.5606872271973513\n",
      "Current iteration=391, the loss=1.005229576371659, the grad=0.5604133270069135\n",
      "Current iteration=392, the loss=1.0050952374110207, the grad=0.5601397332176153\n",
      "Current iteration=393, the loss=1.0049609549910379, the grad=0.5598664454615265\n",
      "Current iteration=394, the loss=1.0048267290845572, the grad=0.559593463370828\n",
      "Current iteration=395, the loss=1.0046925596644025, the grad=0.5593207865778153\n",
      "Current iteration=396, the loss=1.0045584467033688, the grad=0.5590484147148979\n",
      "Current iteration=397, the loss=1.0044243901742271, the grad=0.558776347414605\n",
      "Current iteration=398, the loss=1.0042903900497213, the grad=0.5585045843095848\n",
      "Current iteration=399, the loss=1.0041564463025716, the grad=0.5582331250326082\n",
      "shape of teOpti  (568238, 23)\n",
      "end of the logistic_regression with w= [ 0.04195632 -0.06259875 -0.0025392   0.03139999  0.0045528  -0.00460336\n",
      "  0.02398858 -0.03531562  0.04671585  0.04121729 -0.00021553 -0.0008228\n",
      " -0.00640812  0.00017936  0.00075927  0.00234622  0.00130382  0.02073689\n",
      "  0.02019364  0.02458354  0.02343298  0.02343278  0.02033609]  and loss= 1.0041564463025716\n"
     ]
    }
   ],
   "source": [
    "# test logistic regression\n",
    "\n",
    "txOpti = dataClean(tx_tr)\n",
    "print(\"shape of txOpti \", txOpti.shape)\n",
    "initial_w = np.zeros(txOpti.shape[1])\n",
    "# initial_w = w\n",
    "max_iters = 400\n",
    "gamma = 0.0005\n",
    "\n",
    "w, loss = logistic_regression(y_tr, txOpti, initial_w, max_iters, gamma)\n",
    "\n",
    "teOpti = dataClean(tx_te)\n",
    "print(\"shape of teOpti \", teOpti.shape)\n",
    "\n",
    "y_pred = predict(w, teOpti)\n",
    "y_pred[y_pred==0] = -1\n",
    "\n",
    "OUTPUT_PATH = 'sample-submission LR'\n",
    "create_csv_submission(ids_te, y_pred, OUTPUT_PATH)\n",
    "\n",
    "print(\"end of the logistic_regression with w=\",w,\" and loss=\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0b54129f-2e9e-489f-989e-cffe093f018d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[105.012 46.524 73.752 38.468 -999.000 -999.000 -999.000 2.492 12.316\n",
      " 120.665 1.280 -0.356 -999.000 31.804 -0.023 -0.033 40.516 -0.045 0.086\n",
      " 34.802 -0.024 179.739 1.000 38.960 -1.872 -2.093 -999.000 -999.000\n",
      " -999.000 40.513]\n",
      "[-49.023 49.240 81.182 57.896 -708.421 -601.237 -709.357 2.373 18.917\n",
      " 158.432 1.438 -0.128 -708.985 38.707 -0.011 -0.008 46.660 -0.020 0.044\n",
      " 41.717 -0.010 209.797 0.979 -348.330 -399.254 -399.260 -692.381 -709.122\n",
      " -709.119 73.065]\n",
      "(array([ 4,  5,  6, 12, 26, 27, 28]),)\n",
      "index : 4\n",
      "177457\n",
      "index : 5\n",
      "177457\n",
      "index : 6\n",
      "177457\n",
      "index : 12\n",
      "177457\n",
      "index : 26\n",
      "177457\n",
      "index : 27\n",
      "177457\n",
      "index : 28\n",
      "177457\n",
      "counts of each column  [177457, 177457, 177457, 177457, 177457, 177457, 177457]\n"
     ]
    }
   ],
   "source": [
    "#clean column , remove column that has only -999 elements. \n",
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.3f}\".format(x)})\n",
    "\n",
    "median = np.median(tx_tr, axis=0) \n",
    "mean = np.mean(tx_tr, axis=0) \n",
    "\n",
    "print(median)\n",
    "print(mean)\n",
    "\n",
    "indx = np.where(median == -999)\n",
    "print(indx)\n",
    "\n",
    "counts = []\n",
    "count = 0\n",
    "\n",
    "for i in indx[0]: \n",
    "    count = np.count_nonzero( tx_tr[:,i] == -999)\n",
    "    print(\"index :\", i)\n",
    "    print(count)\n",
    "    counts.append(count)\n",
    "\n",
    "print(\"counts of each column \", counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cd948ae1-5376-46f3-b929-6d5d233f0bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7,)\n",
      "(250000, 30)\n",
      "(250000, 23)\n"
     ]
    }
   ],
   "source": [
    "median = np.median(tx_tr, axis=0) \n",
    "indx = np.where(median == -999)\n",
    "tx1 = np.delete(tx_tr, indx[0], 1)\n",
    "print(indx[0].shape)\n",
    "print(tx_tr.shape)\n",
    "print(tx1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f062a94a-ff3d-4d4d-88b2-66b0ca8bf65c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of txOpti  (250000, 23)\n",
      "end of the logistic_regression with w= [ 4.50300842e-03 -6.62757642e-03 -2.65297946e-04  3.59712971e-03\n",
      "  2.58058701e-04 -3.09882051e-04  2.85152955e-03 -3.69117823e-03\n",
      "  5.10551184e-03  4.42795209e-03 -1.83273093e-05 -8.34744891e-05\n",
      " -6.11568648e-04  2.75644738e-05  7.80817587e-05  4.03966168e-04\n",
      "  1.40670181e-04  2.51657143e-03  2.47748581e-03  2.93239003e-03\n",
      "  2.79926559e-03  2.79924277e-03  2.49137767e-03]  and loss= 1.0562025575989353\n"
     ]
    }
   ],
   "source": [
    "# test regularized logistic regression\n",
    "txOpti = dataClean(tx_tr)\n",
    "print(\"shape of txOpti \", txOpti.shape)\n",
    "initial_w = np.zeros(txOpti.shape[1])\n",
    "# initial_w = w\n",
    "max_iters = 200\n",
    "gamma = 0.0001\n",
    "lambda_ = 0.0000001\n",
    "\n",
    "w, loss = reg_logistic_regression(y_tr, txOpti, lambda_, initial_w, max_iters, gamma)\n",
    "\n",
    "print(\"end of the logistic_regression with w=\",w,\" and loss=\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9b9982-bb31-408a-86a5-c11433e7df5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
